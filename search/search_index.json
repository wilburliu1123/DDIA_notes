{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>DDIA \u8fd9\u672c\u4e66\u628a\u6570\u636e\u7cfb\u7edf\u5206\u4e3a\u4e863\u90e8\u5206\u6765\u8fdb\u884c\u8ba8\u8bba</p> <ol> <li>Part 1 \u8ba8\u8bba\u4e86\u8bbe\u8ba1\u5927\u6570\u636e\u7cfb\u7edf\u7684\u57fa\u672c\u6982\u5ff5\u3002Chapter 1 \u8ba8\u8bba\u4e86\u6211\u4eec\u6700\u7ec8\u7684\u76ee\u6807\u662f\u4ec0\u4e48 reliability (\u53ef\u9760\u6027), scalability (\u53ef\u6269\u5c55\u6027), \u8fd8\u6709 maintainability (\u53ef\u7ef4\u62a4\u6027). \u6211\u4eec\u5e94\u8be5\u5982\u4f55\u601d\u8003\u4ee5\u53ca\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5 Chapter 2 \u8ba8\u8bba\u4e86 data model and query languages (\u6570\u636e\u6a21\u578b\u4ee5\u53ca\u67e5\u8be2\u8bed\u8a00) \u770b\u4ed6\u4eec\u5728\u4ec0\u4e48\u573a\u666f\u4e0b\u4e0b\u6700\u9002\u7528\u3002 Chapter 3 \u8ba8\u8bba\u4e86 storage engine (\u5b58\u50a8\u5f15\u64ce)\u3002\u4e3a\u4e86\u66f4\u6709\u6548\u7684\u67e5\u627e\u51fa\u6211\u4eec\u60f3\u8981\u7684\u6570\u636e\uff0c\u6570\u636e\u5e93\u662f\u5982\u4f55\u5728\u786c\u76d8\u4e0a\u5b58\u50a8\u6570\u636e\u7684\u3002 Chapter 4 \u4e3b\u8981\u8bb2\u8ff0\u6570\u636e\u662f\u5982\u4f55\u7f16\u7801\u7684(serialization)\uff0c\u4ee5\u53ca evolution of schemas over time</li> <li>Part 2 \u4ece\u4e4b\u524d\u4e00\u53f0\u673a\u5668\u8f6c\u6362\u5230\u4e86\u591a\u53f0\u673a\u5668\uff0c\u4e5f\u5c31\u662f distributed system. \u8fd9\u662fscalability\u7684\u524d\u63d0\uff0c\u8fd9\u4e00\u90e8\u5206\u8bb2\u4e86 distributed system \u7684\u5f88\u591a\u6982\u5ff5, replication (Chapter 5), partitioning/sharding (Chapter 6), and transactions (Chapter 7). \u4e4b\u540e\u8bb2\u4e86 distributed system \u7684\u8bf8\u591a\u95ee\u9898 (Chapter 8) \u4ee5\u53ca\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u91cc\u9762\u5b9e\u73b0 consistency and consensus \u7a76\u7adf\u610f\u5473\u7740\u4ec0\u4e48 (Chapter 9)</li> <li>\u7b2c\u4e09\u90e8\u5206\u8ba8\u8bba\u4e86\u884d\u751f\u6570\u636e (datasets derived from other datasets)\u3002\u884d\u751f\u6570\u636e (derived datasets) \u901a\u5e38\u610f\u5473\u7740\u6574\u5408\u4e0d\u540c\u7684DB\uff0ccaches, indexes. Chapter 10 \u4ece batch process \u7684\u89d2\u5ea6\u8ba8\u8bba\u884d\u751f\u6570\u636e\uff0c\u5728\u8fd9\u4e2a\u57fa\u7840\u4e0a\u5f00\u59cb\u8ba8\u8bba stream processing (Chapter 11). \u6700\u540e\u7b2c12\u7ae0\u628a\u6240\u6709\u6982\u5ff5\u6574\u5408\u8d77\u6765\u8ba8\u8bba\u4ee5\u540e\u7684 distributed system</li> </ol> <p>This notes is for discussion only</p>"},{"location":"Chapter%201/","title":"Chapter 1","text":""},{"location":"Chapter%201/#reliable-scalable-and-maintainable-app","title":"Reliable, Scalable, and maintainable app","text":"<p>This chapter introduced definitions of concepts that is used throughout this book What does reliability, scalability, and maintainability mean in this book? </p> <p>\u76ee\u524d\u7684\u5e94\u7528\u5206\u4e3a data intensive vs compute-intensive. compute intensive usually comes in scientific computing application (modeling, prediction etc)</p> <p>Most application has problem with data processing and storage</p> <p>bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing</p> <p>This is where this book is focused on: Data. This book will view an application from data's perspective</p> <p>Any application cannot function without data. This is some summary from the author</p> <ul> <li>Store data (databases)</li> <li>remember expensive operation to speed up reads (caches)</li> <li>Search data by keyword or filter it in various ways (search indexes)</li> <li>Send message between processes (stream processing)</li> <li>Periodically process large amount of data (batch processing)</li> </ul> <p>First time hearing someone use Redis as message queue, but I have heard people use. Kafka as database.</p> <p>There are many factors that may influence the design of a data system, including the skills and experience of the people involved, legacy system dependencies, the time-scale for delivery, your organization's tolerance of different kinds of risk, regulatory constrains </p> <p>Yep, data system has many factors. This book will focus on three of them:</p> <ul> <li>Reliability</li> <li>Scalability</li> <li>Maintainability</li> </ul> <p>This 3 area appears almost in every large data system. Especially for maintainability, whoever worked in \"old\" company might reflect on this. Because software or tools in those company can be 10+ years old. Thus it is crucial for new comers to get hands on those old tech or understand how it works. </p>"},{"location":"Chapter%201/#reliability","title":"Reliability","text":"<p>This section introduced 3 kinds of faults:</p> <ul> <li>Hardware</li> <li>Software</li> <li>Human</li> </ul> <p>faults is when things can go wrong. Author emphasis fault tolerance need to define what types of faults, otherwise if your server was swallowed  by blackhole, and you want to tolerate this type of fault, you need to build your server in space~~good luck to talk to Elon or Bezos about your budget ~~ SpaceX or Blue origin welcomes you!</p> <p>fault is not same as failure. A fault is defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user.</p> <p>failure in service can also be called outage, e.g. you cannot load a website</p>"},{"location":"Chapter%201/#hardware-faults","title":"Hardware faults","text":"<p>\u867d\u7136\u5728\u5e73\u65f6\u751f\u6d3b\u4e2d\u4e0d\u591a\u89c1\uff0c\u4f46\u662f\u786c\u4ef6\u4e00\u65e6\u6570\u91cf\u53d8\u591a\u4e86\u4e4b\u540e\u635f\u574f\u7684\u6982\u7387\u4e5f\u4f1a\u8d8a\u6765\u8d8a\u5927 </p> <p>Hard disks are reported as having a mean time to failure about 10 to 50 years</p> <p>Mean Time to Failure (MTTF) refers to the\u00a0average time it takes for a system to fail.</p> \\[ MTTF = \\frac{Total\\ Operating\\ time}{Num\\ of\\ failures} \\] <p>\u6240\u4ee5\u5982\u679c\u4f60\u6709\u4e00\u4e2a 10,000 \u4e2a disk \u7ec4\u6210\u7684 storage cluster, \u90a3\u4e48\u5c31\u4f1a\u6709 \\(\\frac{1\\ day}{10 \\times 365\\ days} \\times 10000= 2.73972603\\) disk per day to fail</p> <p>\u5982\u4f55\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\uff1f\u4e00\u822c\u901a\u8fc7duplication/redundancy \u6765\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898</p>"},{"location":"Chapter%201/#software-errors","title":"Software Errors","text":"<p>\u8fd9\u7c7b bug \u901a\u5e38\u6f5c\u4f0f\u4e00\u6bb5\u65f6\u95f4\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u624d\u4f1a\u51fa\u73b0\uff0c\u6bd4\u5982 memory leak \u4e00\u822c\u53ea\u6709\u5728\u5927\u91cf\u8bf7\u6c42 (client \u7aef retry) \u5bfc\u81f4\u539f\u672c\u4e0d\u4f1a\u663e\u793a\u7684bug \u6d6e\u51fa\u6c34\u9762\uff0c\u56e0\u4e3a pointer \u6ca1\u6709\u88ab\u91ca\u653e\uff0c\u6240\u4ee5\u5927\u91cf request \u5bfc\u81f4\u4e86 resource deplition  \u5982\u4f55\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u5462\uff1f \u6ca1\u6709\u4e00\u4e2a\u56fa\u5b9a\u516c\u5f0f</p> <p>carefully thinking about assumptions and interactions in the system; thorough testing; process isolation; allowing processes to crash and restart; measuring, monitoring, and analyzing system behavior in production</p> <p>\u8fd9\u4e9b\u90fd\u662f\u597d\u7684\u65b9\u6848</p>"},{"location":"Chapter%201/#human-errors","title":"Human errors","text":"<p>System are operated by human. A wrong config can cause company wide outage 2021\u5e74 Facebook \u6240\u6709\u4ea7\u54c1\u90fd\u8fde\u4e0d\u4e0a\u5c31\u662f\u56e0\u4e3a\u4e00\u4e2a command error</p> <p>This was the source of yesterday\u2019s outage. During one of these routine maintenance jobs, a command was issued with the intention to assess the availability of global backbone capacity, which unintentionally took down all the connections in our backbone network, effectively disconnecting Facebook data centers globally.</p> <p>how to avoid human errors?  - Test thoroughly at all levels, unit test, integration test - Easy to rollback. Rollout new code gradually - Setup detailed and clear monitoring (\u8fd9\u5b9e\u9645\u4e0a\u5df2\u7ecf\u591f\u4e00\u4e2a\u5355\u72ec\u7684\u4e66\u4e86)</p>"},{"location":"Chapter%201/#scalability","title":"Scalability","text":"<p>Scalablility \u5c31\u662f\u4e00\u4e2a\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u589e\u957f\u7684load \u7684\u80fd\u529b</p>"},{"location":"Chapter%201/#describing-load","title":"Describing load","text":"<p>\u89e3\u51b3\u4e00\u4e2a\u95ee\u9898\u9996\u5148\u8981\u628a\u95ee\u9898\u63cf\u8ff0\u6e05\u695a What is current load on the system? What happens if our load doubles?  \u8fd9\u91cc\u4e3a\u4e86\u63cf\u8ff0 load, \u5c31\u9700\u8981\u5b9a\u4e49 load parameters.  The choice of load parameters depends on the architecture of your system. \u5982\u679c\u662f\u4e00\u4e2a web server\uff0crequest per second \u53ef\u80fd\u662f\u4e00\u4e2a\u597d\u7684 load parameter. \u5982\u679c\u662f\u4e00\u4e2a database, ratio of read to write \u6ca1\u51c6\u662f\u4e00\u4e2a\u597d\u7684 load parameter. \u5982\u679c\u662f\u804a\u5929\u5e94\u7528\uff0c number of active users \u53ef\u80fd\u662f\u4e00\u4e2a\u597d\u7684 load parameter </p> <p>\u4e66\u91cc\u9762\u7528\u4e86 twitter \u7684\u4f8b\u5b50 post tweet (12k request / sec at peak) read (300k request / sec) 12k writes is not hard but fan-out is, \u56e0\u4e3a\u4e00\u4e2a\u4eba\u53ef\u80fd\u4f1a\u6709\u5f88\u591a\u4e2afollower\uff0c\u7136\u540e\u8fd9\u4e9bfollower \u53c8\u4f1a\u6709\u5176\u4ed6 follower\uff0c\u6240\u4ee5write \u9700\u8981cope with fanout </p> <p>fanout means output need enough current (power) to drive all the attached input In transaction processing systems, we use it to describe the number of requests to other services that we need to make in order to service one incoming request</p> <p>\u6362\u53e5\u8bdd\u8bf4\uff0cfanout \u662f\u4e00\u4e2a service \u63a5\u6536\u5230\u8bf7\u6c42\u4e4b\u540e\u9700\u8981\u53d1\u591a\u5c11\u4e2a request \u7ed9\u5176\u4ed6\u8bf7\u6c42\u624d\u80fd\u5b8c\u6210\u4efb\u52a1</p> <p>there are 2 ways of implementing these two operations (read, write) 1. \u6bcf\u6b21\u65b0\u7684 tweet \u76f4\u63a5 insert \u8fdb\u4e00\u4e2a global collection of tweets. \u7528\u6237\u68c0\u67e5 homeline \u7684\u65f6\u5019\uff0c\u627e\u5230\u4ed6 follow \u7684\u6240\u6709\u4eba\uff0c\u7136\u540e\u627e\u5230\u8fd9\u4e9b\u4eba\u7684 tweet\uff0c\u6700\u540e\u6309\u7167 \u65f6\u95f4\u7ebf merge <pre><code>SELECT tweets.*, users.* FROM tweets\n    JOIN users ON tweets.sender_id = user.id\n    JOIN follows ON follows.followee_id = user.id\n    WHERE follows.follower_id = current_user\n</code></pre> 2. \u5bf9\u6bcf\u4e00\u4e2a\u7528\u6237\u7684 home timeline \u505a\u4e00\u4e2a cache, \u5c31\u50cf email \u5730\u5740\u4e00\u6837\u3002\u6bcf\u5f53\u4e00\u4e2a\u7528\u6237 post \u4e00\u4e2a tweet \u7684\u65f6\u5019\uff0c\u627e\u5230\u6240\u6709follow\u4ed6\u7684\u4eba\uff0c\u7136\u540e\u5728\u4ed6\u4eec\u7684 home timeline \u4e0a\u9762 insert \u65b0\u7684tweet\u3002\u8fd9\u6837\u8bfb\u53d6 home timeline\u7684\u65f6\u95f4\u5c31\u4f1a\u5927\u5927\u63d0\u9ad8\u56e0\u4e3a\u7ed3\u679c\u5df2\u7ecf\u5b58\u597d\u4e86</p> <p></p> <p>twitter (\u73b0\u5728\u5e94\u8be5\u53ebX\u4e86) \u4e4b\u524d\u662f\u7528\u7b2c\u4e00\u79cd\u65b9\u6cd5\uff0c\u4e4b\u540e\u6362\u5230\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u4e86 (2012)</p> <p>This works better because the average rate of published tweets is almost two order of magnitude lower than the rate of home timeline reads, so in this case it's preferable to do more work at write time and less at read time</p> <p>\u8fd9\u79cdpattern \u5b9e\u9645\u4e0a\u662f\u5927\u591a\u6570 app\u7684\u9700\u6c42\uff0c\u65e0\u8bba\u662f youtube, \u6296\u97f3\uff0c\u8fd8\u662f Facebook\uff0c\u8bfb\u7684\u9700\u6c42\u8981\u6bd4\u5199\u7684\u5927\u5f97\u591a</p> <p>\u4f46\u662f\u7b2c\u4e8c\u79cdapproach \u4e5f\u4e0d\u662f\u6ca1\u6709downside\uff0c\u5c31\u6bd4\u5982\u5982\u679c\u4e00\u4e2a\u4eba\u6709\u5f88\u591a\u7684 follower\uff0c30 million\uff0c\u90a3\u4e48\u4ed6\u53d1\u4e00\u6761 tweet \u5c31\u9700\u8981\u4ea7\u751f 30million writes </p> <p>Doing this in a timely manner\u2014Twitter tries to deliver tweets to followers within five seconds\u2014is a significant challenge.</p> <p>\u5728 Twitter \u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0cdistribution of followers per user (maybe weighted by how often those user tweet) is a key load parameter for discussing scalability, since it determines fanout load Trump \u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u4f8b\u5b50\uff0c\u4ed6\u53d1 tweet \u7684\u9891\u7387\u5e94\u8be5\u6bd4\u5176\u4ed6\u660e\u661f\u8981\u9ad8\u5f88\u591a</p> <p>Your application may have very different characteristics, but you can apply similar principles to reasoning about its load</p> <p>\u8fd9\u91cc\u5c31\u662f\u6211\u4eec\u8bbe\u8ba1\u4e00\u4e2a app \u7684\u65f6\u5019\u9700\u8981\u8003\u8651\u7684\u56e0\u7d20\u4e86 \u6700\u540etwitter\u662f\u7528\u7684 hybrid approach\u3002 \u5177\u4f53\u7ec6\u8282\u5728chapter 12 \u91cc\u9762\u8fd8\u4f1a\u63d0\u5230</p>"},{"location":"Chapter%201/#describing-performance","title":"Describing performance","text":"<p>\u5f53\u6211\u4eec\u8bbe\u8ba1\u597d load parameters \u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5f00\u59cb\u89c2\u5bdf\u7cfb\u7edf\u5728 load increase \u4e4b\u540e\u7684\u8868\u73b0\u4e86  - If CPU, memory, network bandwidth is unchanged, how is the performance of our system affected after load increased? - How much do we need to increase our resources to keep performance unchanged? </p> <p>\u4e0d\u540c\u7cfb\u7edf\u9700\u8981\u89c2\u5bdf\u7684 performance parameter \u4e5f\u4e0d\u4e00\u6837, batch processing system like Hadoop cares about throughput -- the number of records we can process per seconds, or the total time it takes to run a job on a dataset of certain size. \u5176\u5b9e\u751f\u6d3b\u4e2d\u7684\u4f8b\u5b50\u4e0d\u4e5f\u5230\u5904\u90fd\u662f\u4e48\uff0c\u4f60\u5e0c\u671b\u7f51\u9875 load \u7684latency (response time) \u8d8a\u4f4e\u8d8a\u597d\uff0c\u4f60\u5e0c\u671b\u4f60\u4e0b\u8f7d\u4e1c\u897f\u7684\u603b\u65f6\u95f4\u8d8a\u77ed\u8d8a\u597d(throughput)</p> <p>Latency and response time are often used synonymously, but they are not the same. The response time is what the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays. Latency is the duration that a request is waiting to be handled\u2014during which it is latent, awaiting service</p> <p>Latency \u662f server side parameter\uff0c\u4e0d\u8003\u8651 network \u4e4b\u95f4\u7684delay  \u5728measure response time \u7684\u65f6\u5019\uff0c\u6211\u4eec\u901a\u5e38take sample of distribution. There are occasional outliers that can take much longer. \u8fd9\u91cc\u5c31\u9700\u8981\u7528\u5230 percentile \u7684\u6982\u5ff5\u4e86 \u8fd9\u5f20\u56fe\u63cf\u8ff0\u5f97\u633a\u6e05\u6670  percentile \u5c31\u662f\u5728\u8fd9\u4e2a sample\u91cc\u9762\u6709\u591a\u5c11 percent\u7684\u503c\u5c0f\u4e8e\u5f53\u524d\u7684\u6570\u503c \u6bd4\u5982\u4f60\u670910\u4e2a\u6570\uff0c\u91cc\u97628\u4e2a1\uff0c1\u4e2a9\u548c1\u4e2a10\uff0c\u90a3\u4e4890th percentile \u662f9\uff0c\u4f46\u662f50 percentile \u53ea\u67091 </p> <p>percentile \u5728\u8861\u91cfperformance \u66f4\u6709\u6548\u56e0\u4e3a\u5927\u591a\u6570\u7684data \u4e00\u822c\u90fd\u5f88\u4f4e\uff0c\u800c\u6211\u4eec\u9700\u8981\u770b\u7684\u5f80\u5f80\u53ea\u662f outlier \u5728\u54ea\u91cc\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u4fdd\u8bc1\u5373\u4f7f\u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684performance (latency) \u4e5f\u4e0d\u4f1a\u9ad8\u4e8e\u8fd9\u4e2a\u503c</p> <p>\u6211\u4eec\u7ec4\u7684 SLA \u5c31\u662f p99 \u8981\u4f4e\u4e8e 8ms</p> <p>High percentiles become especially important in backend services that are called multiple times as part of serving a single end-user request.</p> <p>\u5bf9\u7684\uff0cclient \u5bf9\u4e0d\u540c\u7684 backend service make request \u7684\u65f6\u5019\uff0c\u5c31\u8981\u7b49\u6700\u540e\u4e00\u4e2a\u5b8c\u6210, \u6240\u4ee5\u8fd9\u65f6\u5019 high percentiles become important </p>"},{"location":"Chapter%201/#approaches-for-coping-with-load","title":"Approaches for coping with load","text":"<p>\u5b9a\u4e49\u4e86 load parameter and metrics for measuring performance \u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u8ba8\u8bba scalability \u4e86</p> <p>How do we maintain good performance even when our load parameters increase by some amount? </p> <p>\u4eba\u4eec\u7ecf\u5e38\u63d0\u5230 scale up(\u589e\u52a0\u786c\u4ef6) vs scale out (\u628a load \u5206\u5e03\u5230\u4e0d\u540c\u7684\u673a\u5668\u4e0a) scale out \u9700\u8981\u4f60\u7684\u7cfb\u7edf\u662f shared nothing architecture\u3002 scale up \u6bd4\u8f83\u5bb9\u6613\u4f46\u662f\u901a\u5e38\u4f1a\u5f88\u8d35\uff0c\u800c\u4e14\u901a\u5e38\u4e00\u4e2a\u7cfb\u7edf\u7684 load \u5927\u5230\u4e00\u5b9a\u7a0b\u5ea6\u7684\u65f6\u5019\u5df2\u7ecf\u4e0d\u53ef\u80fd\u7528\u4e00\u53f0\u673a\u5668 handle\u4e86 \u73b0\u5b9e\u4e2d\u53ef\u80fd\u662f mixture of approaches </p> <p>An elastic system can be useful if load is highly unpredictable, but manually scaled systems are simpler and may have fewer operational surprises (see \u201cRebalancing Partitions\u201d on page 209).</p> <p>\u540e\u9762\u4f1a\u8bb2\u5230 partition \u76f8\u5173\u7684\u5185\u5bb9\uff0c\u56e0\u4e3a\u81ea\u52a8\u589e\u52a0\u8ba1\u7b97/\u5b58\u50a8\u8d44\u6e90\u7684\u7cfb\u7edf\u9700\u8981 handle \u5f88\u591a edge case</p> <p>The architecture of systems that operate at large scale is usually highly specific to the application\u2014there is no such thing as a generic, one-size-fits-all scalable architecture (informally known as magic scaling sauce).</p> <p>\u786e\u5b9e\u662f\u8fd9\u6837</p> <p>For example, a system that is designed to handle 100,000 requests per second, each 1 kB in size, looks very different from a system that is designed for 3 requests per minute, each 2 GB in size\u2014even though the two systems have the same data throughput.</p> <p>\u8fd9\u4e2a\u4f8b\u5b50\u8fd8\u662f\u633a\u597d\u7684\uff0c\u6700\u540e\u8fd8\u662f\u8981\u5177\u4f53\u4f8b\u5b50\u5177\u4f53\u5206\u6790\uff0c\u56e0\u4e3a access parttern \u4e0d\u4e00\u6837</p> <p>An architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare\u2014the load parameters. If those assumptions turn out to be wrong, the engineering effort for scaling is at best wasted, and at worst counterproductive. In an early-stage startup or an unproven product it\u2019s usually more important to be able to iterate quickly on product features than it is to scale to some hypothetical future load.</p> <p>\u6240\u4ee5\u8981\u5148\u5b9a\u4e49\u597d\u4ea7\u54c1\u7684\u9700\u6c42\uff0c\u6216\u8005\u8bf4\u5982\u679c\u662f startup\uff0c\u540e\u7aef scale \u7684\u95ee\u9898\u5e94\u8be5\u6839\u636e\u7528\u6237\u589e\u957f\u7684\u9884\u5224\u6765\u8bbe\u8ba1\uff0c\u4fdd\u8bc1\u4ea7\u54c1\u7684 feature \u6ee1\u8db3\u7528\u6237\u7684\u9700\u6c42\u624d\u662f\u66f4\u91cd\u8981\u7684\u4e8b\u60c5 \u4e0d\u7136\u4f60\u82b1\u5f88\u5927\u529b\u6c14\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591fhandle 1000\u4e07 request / sec \u7684\u7cfb\u7edf\u7ed3\u679c\u53ea\u6709 100/sec \u90a3\u53ef\u80fd\u767d\u8d39\u529b\u6c14\u4e86</p> <p>Even though they are specific to a particular application, scalable architectures are nevertheless usually built from general-purpose building blocks, arranged in familiar patterns. In this book we discuss those building blocks and patterns.</p> <p>\u5373\u4f7f\u6bcf\u4e00\u4e2a app \u9700\u6c42\u4e0d\u4e00\u6837, scalable architectures \u4e5f\u662f\u53ef\u4ee5\u6a21\u5757\u5316\u7684\uff0c\u8fd9\u672c\u4e66\u5c31\u8ba8\u8bba building blocks and patterns</p>"},{"location":"Chapter%201/#maintainability","title":"Maintainability","text":"<p>It is well known that the majority of the cost of software is not in its initial development, but in its ongoing maintenance\u2014fixing bugs, keeping its systems operational, investigating failures, adapting it to new platforms, modifying it for new use cases, repaying technical debt, and adding new features.</p> <p>\u5728\u4e9a\u9ebb\u4e00\u4e2a\u8fd0\u884c\u5f88\u4e45\u7684service \u5bf9\u8fd9\u53e5\u8bdd\u6df1\u6709\u4f53\u4f1a\u2026\u2026</p> <p>Yet, unfortunately, many people working on software systems dislike maintenance of so-called legacy systems\u2014perhaps it involves fixing other people\u2019s mistakes, or working with platforms that are now outdated, or systems that were forced to do things they were never intended for.</p> <p>\u662f\u7684\uff01\uff01\u6211\u786e\u5b9e\u7ecf\u5e38\u611f\u89c9\u5230 working on legacy system \u91cc\u9762\u7684\u75db\u70b9\uff01 \u6240\u4ee5\u5728\u8bbe\u8ba1\u7684\u65f6\u5019\u5c31\u8981\u628a maintainability \u7684\u56e0\u7d20\u8003\u8651\u8fdb\u6765 \u4e66\u91cc\u9762\u5f3a\u8c03\u4e86\u5bf9 maintainability \u76843\u4e2a\u8981\u7d20 - Operability - Simplicity - Evolvability</p> <p>operability \u5f3a\u8c03\u7684\u662f\u4fdd\u8bc1 operation \u5c42\u9762\u8d8a\u7b80\u5355\u8d8a\u597d simplicity \u4fdd\u8bc1\u7684\u662f\u80fd\u8ba9\u65b0\u6765\u7684 engineer \u8ba9\u6613\u4e0a\u624b evolvability \u662f\u8ba9\u4f60\u7684\u7cfb\u7edf\u66f4\u5bb9\u6613\u6839\u636e\u65b0\u7684\u9700\u6c42\u8fdb\u884c\u6539\u52a8</p>"},{"location":"Chapter%201/#operability-making-life-easy-for-operations","title":"Operability: Making life easy for operations","text":"<p>\u4e66\u91cc\u9762\u5c45\u7136\u53c8 reference \u5230\u4e86 Jay Kreps \u7684\u6587\u7ae0\uff0c\u4ed6\u7684 the log \u90a3\u7bc7\u6587\u7ae0\u5199\u7684\u662f\u771f\u7684\u597d\uff0c \u770b\u6765linkedin \u7cfb\u7684engineer \u5bf9\u4e8e distributed system \u7406\u89e3\u8fd8\u662f\u5f88\u6df1\u523b\u7684</p> <p>operation \u4e3b\u8981\u8d1f\u8d23\u4e0b\u9762\u7684\u51e0\u70b9 - Monitoring system health and restore service if it goes bad  - Tracking down the cause of the problems (root cause) - Keeping software and platforms up to date. Including security patches - Anticipating future problems and solving them before they occur (capacity planning) - Establishing good practices and tools for deployment, configuration management (deployment pipeline, version control, rollback mechanisms etc) - Performing complex maintenance tasks, such as moving an application from one platform to another (\u8fd9\u4e00\u6b65\u5b9e\u9645\u4e0a\u73b0\u5728\u5927\u591a\u6570 app \u90fd\u5df2\u7ecf\u662f multi platform\u4e86) - Defining processes that make operations predictable and help keep the production environment stable (amazon has change management (CM) and two person verification (TPV)) - Preserving the organization's knowledge about the system, even as individual people come and go (\u8fd9\u4e00\u70b9\u4e9a\u9ebb\u6709\u7684\u7ec4\u505a\u7684\u5f88\u5dee\u2026\u2026)</p> <p>\u5efa\u8bae\u628adoc \u4e5f\u7528 version control \u7ba1\u7406\u8d77\u6765\uff0c\u8fd9\u6837\u65b0\u6765\u7684\u4eba\u53ef\u4ee5\u67e5\u770b\u5386\u53f2\u8bb0\u5f55 \u505a\u5230\u4e0a\u9762\u7684\u51e0\u70b9\u6211\u4eec\u9700\u8981 - Providing good monitoring - Providing good support for automation and integration with standard tools - Avoid dependency on individual machines - Providing good documentation and an easy to understand operational model (If I do X, Y will happen) - Providing good default behavior  - self-healing where appropriate (auto swap)</p>"},{"location":"Chapter%201/#simplicity-managing-complexity","title":"Simplicity: Managing Complexity","text":"<p>but as projects get larger, they often become very complex and difficult to understand. There are various possible symptoms of complexity: explosion of the state space, tight coupling of modules, tangled dependencies, inconsistent naming and terminology, hacks aimed at solving performance problems, special-casing to work around issues elsewhere, and many more. Much has been said on this topic already</p> <p>\u7b2c\u4e00\u904d\u8bfb\u7684\u65f6\u5019\u8fd8\u6ca1\u63a5\u89e6\u5230complex system\uff0c\u5728\u4e9a\u9ebb\u5de5\u4f5c\u4e00\u6bb5\u65f6\u95f4\u540e\u53d1\u73b0\u8fd9\u4e9b\u95ee\u9898\u57fa\u672c\u90fd\"\u4e2d\u67aa\"\u4e86</p>"},{"location":"Chapter%201/#evolvability-making-change-easy","title":"Evolvability: Making change easy","text":"<p>\u65b0\u7684\u9700\u6c42\u603b\u4f1a\u4e0d\u65ad\u51fa\u73b0\uff0c\u6240\u4ee5\u4e00\u4e2a\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u65b0\u7684\u9700\u6c42\u8fdb\u884c\u8fdb\u5316\u5c31\u5f88\u6709\u5fc5\u8981\u4e86 \u6bd4\u5982\u7528\u6237\u60f3\u8981\u65b0\u7684\u529f\u80fd\uff0c\u6216\u8005\u6211\u4eec\u91c7\u7528\u65b0\u7684\u5de5\u5177\u7b49\u7b49</p> <p>The ease with which you can modify a data system, and adapt it to changing requirements, is closely linked to its simplicity and its abstractions: simple and easy-to- understand systems are usually easier to modify than complex ones. But since this is such an important idea, we will use a different word to refer to agility on a data system level: evolvability</p> <p>\u4f5c\u8005\u7528\u4e86twitter \u91cc\u9762\u5982\u4f55 \"refactor\" approach 1 to approach 2 \u7684\u4f8b\u5b50\u6765\u8868\u793a\u4ed6\u4e3a\u4ec0\u4e48\u7528  evolvability instead of refactoring \u56e0\u4e3a\u5f53\u4e00\u4e2a\u7cfb\u7edf\u8db3\u591f\u590d\u6742\u7684\u65f6\u5019\uff0c\u53ef\u80fd\u7528\u8fdb\u5316\u4f1a\u66f4\u6070\u5f53\u4e00\u70b9\u5427\u2026\u2026 refactor \u901a\u5e38\u4e5f\u53ea\u662f\u51e0\u4e2a source file \u7684code</p>"},{"location":"Chapter%201/#summary","title":"Summary","text":"<p>This chapter talked about Reliability, Scalability and Maintainability</p> <p>Reliability means system can recover from faults (hardware, software, human)</p> <p>Scalability means system can cope with increasing load (before that, you need to define load parameter)</p> <p>Maintainability means making life easier for new comers, people who operate this system. And easier to add/upgrade features</p>"},{"location":"Chapter%2010/","title":"Chapter 10","text":"<p>A system cannot be successful if it is too strongly influenced by a single person. Once the initial design is complete and fairly robust, the real test begins as people with many different viewpoints undertake their own experiments.     \u2014Donald Knuth</p> <p>Web made request/response style of interaction so common where people forget there are other ways to interact with the system. There are 3 types of systems: Services (online systems)     A service waits for a request from a client to arrive. When it receive a request, it tries to handle it as soon as possible and sends a response back.  Batch processing systems (offline systems)     A batch processing system takes a large amount of input data, runs a job to process it, and produces some output data. (few minutes to several days) Stream processing systems (near-real-time systems)     Somewhere between online and offline/batch processing. Like batch processing, stream takes input and produces output, but stream job operates on events. </p> <p>MapReduce provides a clear picture of why and how batch processing is useful. Batch processing is very old form of computing. Punch card tabulating machines implemented a semi-mechanized form of batch processing to compute aggregate statistics from large inputs. </p>"},{"location":"Chapter%2010/#batch-processing-with-unix-tools","title":"Batch Processing with Unix Tools","text":"<p>Main idea of unix tools is that it is able to pipe current program's output into next program's input. This way we can start simple log analysis</p>"},{"location":"Chapter%2010/#simple-log-analysis","title":"Simple log analysis","text":"<p>download sample log from NASA by <pre><code>curl \"ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz\" &gt; NASA.gz\ngunzip NASA.gz\n</code></pre> below is the command made from unix tools to perform count on ip address on NASA's webserver log <pre><code>cat NASA | awk '{print $2}' | sort | uniq -c | sort -r -n | head -n 5\n</code></pre> sample output <pre><code>17572 piweba3y.prodigy.com\n11591 piweba4y.prodigy.com\n9868 piweba1y.prodigy.com\n7852 alyssa.prodigy.com\n7573 siltb10.orl.mmc.com\n</code></pre></p> <p>This case you just finished a data pipeline from unix tool</p> <p>Similar logic can be applied by using a programing language <pre><code>counts = Hash.new(0)\n\nFile.open('./NASA') do |file| \n    file.each do |line|\n        url = line.split[6]\n        counts[url] += 1 \n    end\nend\ntop5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5]\ntop5.each{|count, url| puts \"#{count} #{url}\" }\n</code></pre></p>"},{"location":"Chapter%2010/#sorting-versus-in-memory-aggregation","title":"Sorting versus in-memory aggregation","text":"<p>Although the logic is similar in both way of processing log. Main difference is execution flow. If your working set (size of your data), is small to fit in memory, then programming language way might be sufficient because all data can be stored in memory. However, if your working set is larger than memory available (let's say 1TB log file need to be processed). Then unix tool approach is better. Because sort utility in GNU Coreutils (Linux) automatically handles larger-than- memory datasets by spilling to disk, and automatically parallelizes sorting across multiple CPU cores.</p> <p>Martin spent lot of effort explaining unix philosophy which simply comes down to every program should do one thing well. And Unix use uniform interface, i.e. everything is a file to decouple program and it's input/output. This way program can be composed together in a way their designer could never think of. </p> <p>This philosophy is very similar to Agile/Devops practice today</p>"},{"location":"Chapter%2010/#mapreduce-and-distributed-filesystems","title":"MapReduce and Distributed Filesystems","text":"<p>MapReduce is just like unix tools that build for processing data.  Main two function you need to implement is mapper and reducer. Where mapper is like <code>awk '{print $2}'</code> where it extract from input and map it to a key-value pair. <code>sort</code> or <code>uniq -c</code> will be your reducer where it process the input and output to somewhere. </p> <p>Main difference between MapReduce and unix tool is MapReduce job can be deployed to multiple machines where unix tool usually work on a single machine. </p> <p>Main idea is still partition, where each input is typically a directory in HDFS and each file or file block within the input directory is considered to be a separate partition that can be processed by a separate map task </p> <p>MapReduce scheduler tries run each mapper on one of the machines that stores a replica of the input file thus saving network load. Reducer side is also partitioned, the number of reduce tasks is configured by job author. same as Chapter 6#Partitioning by Hash of Key</p> <p></p> <p>Because MapReduce job's outputs are write into a directory in HDFS, chained MapReduce jobs are therefore less like pipelines of Unix commands (which pass the output of one process as input to another process directly, using only a small in-memory buffer)</p> <p>Chained MapReduce jobs can sometimes hard to read so higher level tools have been developed to help with that  Such as Hadoop, such as Pig [30], Hive [31], Cascading [32], Crunch [33], and FlumeJava [34], also set up workflows of multiple MapReduce stages that are automatically wired together appropriately.</p>"},{"location":"Chapter%2010/#reduce-side-joins-and-grouping","title":"Reduce Side Joins and Grouping","text":"<p>This section goes deeper into how <code>JOIN</code> operator was implemented. In most dataset, data have relation to each other. A foreign key in relational model, a document reference in document model, or edge in a graph model </p> <p>Those can be used as index to look stuff up then performing joins. When MapReduce job given set of files as input, it will scan the entire file and this is called full table scan in database terms because MapReduce has no concept of index</p> <p>When we talk about joins in the context of batch processing, we mean resolving all occurrences of some association within a dataset. For example</p> <p></p> <p>left is things that logged in user did on a website, right is database of users. This is like Chapter 3#Stars and Snowflake schema where left is the fact table, and right is one of the dimensions </p> <p>Since fact table can be very large, in order to achieve good throughput in batch process, it is better to bring user table into same distributed filesystem as log of user activities. </p>"},{"location":"Chapter%2010/#sort-merge-joins","title":"Sort merge joins","text":"<p>Mapper is basically to extract a key and value from each input record. In this example, mapper will extract key (user ID) and particular event (url or dob). Reducer can be partitioned into odd or even number (2 partition in total)  when MapReduce partitions the mapper output by key and then sorts the kv pairs, the effect is that all the activity events and the user record with same user ID become adjacent to each other in reducer input </p> <p>after sorting, join logic become easy: the reducer function is called once for every user ID, and first value will be dob from user db. Reducer iterates over activity events with the same user ID, outputting <code>viewed-url</code> and <code>viewer-age-in-years</code> pair </p> <p>Subsequent MapReduce job can calculate the distribution of viewer ages for each URL</p> <p>This algorithm is called sort merge join because mapper output is sorted by key and reducer can process all of the records for a particular user ID in one go. (output that result and merge it together)</p>"},{"location":"Chapter%2010/#group-by","title":"GROUP BY","text":"<p>Besides joins, another common use of bring related data to same place is grouping records by some key (<code>GROUP BY</code> clause in SQL) and perform some kind of aggregation  - counting - adding up particular field - pick top k record</p> <p>Another common use for grouping is sessionization where such analysis could be used to work out whether users who were shown a new version of your website are more likely to make a purchase than those who used old version (A/B testing)</p> <p>hot keys are those celebrities on social network website. Collecting all activity related to a celebrity can lead to skew (aka hot spots) that is, one reducer are processing significantly more records than the others </p> <p>There are method like sampling to determine which keys are hot. Other approach like sharded join (predetermined hot spot). </p>"},{"location":"Chapter%2010/#map-side-joins","title":"Map Side Joins","text":"<p>Previous section talked about join logic on reducers. You don't need to make any assumption about the input data on reducer side join. However, the downside is that all that sorting, copying to reducers, and merging of reducer inputs can be expensive. </p> <p>On the other hand, if you can determine your input data format, it is possible to make joins faster by using map side join. Each mapper read from distributed file system and output to file system</p>"},{"location":"Chapter%2010/#broadcast-hash-joins","title":"Broadcast hash joins","text":"<p>the word broadcast reflects the fact that each mapper for a partition of the large input reads the entirety of the small input (so small input is \"broadcast\" to all partitions of the large input) and this small dataset is loaded into memory </p>"},{"location":"Chapter%2010/#partitioned-hash-joins","title":"Partitioned hash joins","text":"<p>partition and map to same way. For example, arrange activity events and user database to each be partitioned based on last decimal digit of user ID. (10 partition in total). Mapper. 3 first loads all users with ID ending in 3 into hash table, then scans over all activity events for each user whose ID ends in 3</p>"},{"location":"Chapter%2010/#map-side-merge-joins","title":"Map side merge joins","text":"<p>On top of partitioned in the same way, we can also sort the key. In this case, it does not matter whether the inputs are small enough to fit in memory, because mapper can perform same merging operation that would be done by reducer. </p> <p>This usually mean there is previous map reduce job that partition and sort the key already. </p>"},{"location":"Chapter%2010/#the-output-of-batch-workflows","title":"The Output of Batch Workflows","text":"<p>Compare to OLTP where generally look up small number of records by key, analysis workload scan over a large number of records. Performing groupings and aggregations and output is often some form of report: a graph showing the change in a metric over time, top 10 items according to some ranking. The consumer of such a report is often an analyst or manager who make business decisions. Below are different use cases </p>"},{"location":"Chapter%2010/#building-search-indexes","title":"Building search indexes","text":"<p>MapReduce was used for building index for search engine in Google. If we need to perform full text search over a fixed set of documents, then batch process is a very effective way of building indexes: mappers partition the set of document as needed. Each reducer builds index for its partition, and index file are written to distributed file system</p>"},{"location":"Chapter%2010/#kv-stores-as-batch-process-output","title":"KV stores as batch process output","text":"<p>Another common output from batch processing is to build machine learning systems such as classifiers (e.g. spam filters, anomaly detection, image recognition) and recommendation systems (e.g. people you may know, products you may be interested in, or related searches)</p> <p>The output of those jobs are some kind of database: for example, a database that can be queried by user ID to obtain suggested friends for that user. </p> <p>These database can be files from batch jobs and read only. Since mapper usually sort key and map them together</p>"},{"location":"Chapter%2010/#philosophy-of-batch-process","title":"Philosophy of batch process","text":"<p>Since input is left unchanged, we can experiment with it very easily. (no side effects) - If you introduce a bug into the code and the output is wrong or corrupted, you can simply roll back to a previous version of the code and rerun the job, and the output will be correct again. - As a consequence of this ease of rolling back, feature development can proceed more quickly than in an environment where mistakes could mean irreversible damage. - If a map or reduce task fails, the MapReduce framework automatically re- schedules it and runs it again on the same input. - Like Unix tools, MapReduce jobs separate logic from wiring (configuring the input and output directories), which provides a separation of concerns and enables potential reuse of code</p>"},{"location":"Chapter%2010/#comparing-hadoop-to-distributed-databases","title":"Comparing Hadoop to Distributed Databases","text":"<p>Hadoop is like a distributed version of Unix tools. Massively parallel processing databases have been explored this idea previously. (such as Teradata, NonStop SQL) </p> <p>The main difference is MPP databases focus on parallel execution analytic SQL queries where combination of MapReduce and distributed filesystem provide something more like a general purpose OS that can run arbitrary programs </p> <p>Database requires you to define schema but filesystem does not. they can equally well be text, images, videos, sensor readings, sparse matrices, feature vectors, genome sequences, or any other kind of data.</p> <p>Hadoop enable dumping data into HDFS and figuring out later how to process it. While MPP databases need careful up-front modeling</p> <p>The idea is similar to data warehouse. And this data dumping shifts the burden of interpreting the data from producer to consumer (the interpretation of the data)</p> <p>This can be advantage if producer and consumer of the data are in different teams. </p> <p>Hadoop has been used for ETL process. Data is dumped into distributed filesystem in raw form. MapReduce jobs transform it into a relational form and import it into an MPP data warehouse for analytic purposes. </p>"},{"location":"Chapter%2010/#diversity-of-processing-models","title":"Diversity of processing models","text":"<p>MPP databases are monolithic and tightly integrated software that take care of storage layout on disk, query planning, scheduling, and execution. The system can achieve very good performance on the types of queries that it is designed for. On top of that, SQL allows graphical tools used by business analysts (such as Tableau)</p> <p>But not all kinds of processing can be sensibly expressed as SQL queries. If you are building recommendation systems, or full text search indexes with relevance ranking models, or performing image analysis. You most likely need a more general model of data processing. These kinds of processing are often very specific to particular application (e.g. feature engineering for machine learning, natural language models for machine translation, risk estimation functions for fraud prediction). Those inevitably require writing code, not just queries </p> <p>MapReduce gave engineers the ability to easily run their own code over large data sets. You can build SQL query execution engine on top of HDFS and MapReduce. (Hive)</p> <p>Various processing models can be run on a single cluster machine, all access the same files on the distributed file system. Hadoop ecosystem include OLTP databases such as HBase and MPP style databases such as Impala. Neither of them use MapReduce, but both use HDFS for storage. This way they can be integrated in the same system</p>"},{"location":"Chapter%2010/#fault-tolerance","title":"Fault tolerance","text":"<p>Batch process system tolerate fault better than online system does. If node crash when executing a query, MPP usually abort the entire process. MapReduce can tolerate failure of a map or reduce task without affecting the whole job by retrying work at the granularity of an individual task. </p> <p>The MapReduce approach is more appropriate for larger jobs where so much data and the job would run a long time that they are likely experience at least one task failure along the way. </p>"},{"location":"Chapter%2010/#beyond-mapreduce","title":"Beyond MapReduce","text":"<p>MapReduce is just one of programming models in distributed systems. Depending on the volume of data, the structure of the data and the type of processing being done with it, other tools may be more appropriate for expressing a computation </p> <p>MapReduce is a useful learning tool. Concept are simple to understand (but hard to implement) That is why higher-level programming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top of MapReduce. </p> <p>Rest of this chapter will explore alternatives for batch processing</p>"},{"location":"Chapter%2010/#materialization-of-intermediate-state","title":"Materialization of Intermediate State","text":"<p>In many case you know the output of one job is used as input to another job, which is maintained by the same team. In this case, the files on the distributed system is called intermediate state. In the complex workflows used to build recommendation systems there is a lot of such intermediate state (50 or 100 MapReduce jobs)</p> <p>The process of writing out this intermediate state to files is called materialization. (basically write to some storage)</p> <p>The log analysis example in the beginning used Unix pipes do not fully materialize the intermediate state, but instead stream the output to the input incrementally using a small in memory buffer.</p> <p>MapReduce's approach of fully materializing intermediate state has downsides compare to Unix pipes:  - job can only start when all tasks in the preceding jobs have completed.  - Storing intermediate state in distributed filesystem means those files are replicated across several nodes, which is overkill for such temporary data.</p>"},{"location":"Chapter%2010/#dataflow-engines","title":"Dataflow engines","text":"<p>In order to fix these problems with MapReduce, several new execution engines were developed. Most well known are Spark, Tez, and Flink. They have one thing in common: they handle an entire workflow as one job, rather breaking it up into independent subjobs.</p> <p>They explicitly model the flow of data through several processing stages. So they are called dataflow engines. Like MapReduce, they work by repeatedly calling a user-defined function (UDF) to process one record at a time on a single thread. They parallelize work by partitioning inputs, and copy output of one function over the network to become the input to another function </p> <p>Unlike in MapReduce, these functions need not take the strict roles of alternating map and reduce, but instead can be assembled in more flexible ways. They are called operators. And the dataflow engine provides several different options for connecting one operator's output to another's input  - One option is to repartition and sort records by key, This feature enables sort-merge joins and grouping in the same way as in MapReduce - Another way is to take inputs and partition them in the same way but skip sorting.  - For broadcast hash joins, the same output from one operator can be sent to all partitions of the join operator.</p> <p>These style of processing engine has several advantages compared to MapReduce model:  - Expensive work such as sorting only performed in places where it is actually required. (Rather than default in every map and reduce stage) - No unnecessary map tasks since the work done by a mapper can often be incorporated into the previous reduce operator  - Because data dependencies in workflow are explicitly declared, the scheduler can make locality optimizations. For example, try to place the task that consumes some data on the same machine as the task that produces it. saves network round trip - Existing JVM processes can be reused to run new operators, reducing startup overheads compared to MapReduce (which launches new JVM for each task)</p>"},{"location":"Chapter%2010/#fault-tolerance_1","title":"Fault tolerance","text":"<p>Fully materializing intermediate state to distributed filesystem is that it makes fault tolerance fairly easy in MapReduce: just restart </p> <p>Spark, Flink, and Tez avoid writing intermediate state to HDFS (replication has costs) so they take different approach: if machine fails and intermediate state is lost, it is recomputed from other data that is still available </p> <p>To enable this recomputation, the framework must keep track of how a given piece of data was computed -- which input partitions it used, and which operators were applied to it. Spark uses resilient distributed dataset (RDD) abstraction for tracking the ancestry of data 61 , while Flink checkpoints operator state. </p> <p>Recovering from faults by recomputing data is not always the right answer if the intermediate data is CPU-intensive. It is probably cheaper to materialize the intermediate data to files than recompute it</p> <p>61 is a good reference </p>"},{"location":"Chapter%2010/#materialization","title":"Materialization","text":"<p>MapReduce is like writing the output of each command to a temporary file, whereas dataflow engines look much more like Unix pipes. Flink built around the idea of pipelined execution: i.e. incrementally passing the output of an operator to other operators and not waiting for input to be complete </p> <p>this non-blocking idea was the same for SSI where readers don't wait writers or vise versa</p> <p>A sorting operation inevitably needs to consume its entire input because it is possible last input is the lowest key. But many other parts of a workflow can be executed in a pipelined manner.</p> <p>Basically dataflow engine combines MapReduce job together into on job and output is usually saved to HDFS </p>"},{"location":"Chapter%2010/#graphs-and-iterative-processing","title":"Graphs and Iterative Processing","text":"<p>Chapter 2#Graph like Data models discussed graph model in OLTP context. It is interesting to look at graphs in batch processing as well. (offline processing or analysis on entire graph) </p> <p>Graph processing is often used in recommendation engines or machine learning applications or in ranking systems. Famous graph analysis algorithms is PageRank.</p> <p>Many graph algorithms are expressed by traversing the graph one edge at a time, joining one vertex with adjacent vertex in order to pass some information, and repeating until some condition is met. </p> <p>It is possible to store a graph in a distributed filesystem (files containing lists of vertices and edges), but this idea of \"repeating until done\" cannot be expressed in plain MapReduce, since it performs single pass over the data. This kind of algorithm is often implemented in an iterative style: 1. An scheduler runs a batch process to calculate one step of the algorithm 2. When process complete, scheduler checks completion condition has been met (finished or not)  3. If it has not finished yet, scheduler goes back to step 1 and runs another round of batch process This works but it is very inefficient if implemented in MapReduce</p>"},{"location":"Chapter%2010/#pregel-processing-model","title":"Pregel processing model","text":"<p>As an optimization for batch processing graphs, bulk synchronous parallel model of computation has become popular. It is implemented by Apache Giraph [37], Spark\u2019s GraphX API, and Flink\u2019s Gelly API [71]. This is known as Pregel model popularized by Google's Pregel paper </p> <p>Similar to mapper conceptually \"send a message\" to a particular call of the reducer, vertex can \"send a message\" to another vertex, and those messages are sent along the edges in a graph</p>"},{"location":"Chapter%2010/#fault-tolerance_2","title":"Fault tolerance","text":"<p>Messages can be batched and thus less waiting for communication. This helps the performance of Pregel jobs The only waiting time is between iterations. Even though underlying network may drop, duplicate, or arbitrarily delay messages, Pregel implements guarantee that messages are processed exactly once in the following iteration. The framework's ability to recover from faults simplifies the programming model for algorithms on top of Pregel.</p> <p>This fault tolerance is achieved by periodically checkpointing the state of all vertices at the end of an iteration. Writing full state to durable storage. If a node fails and in-memory state is lost, we can start from previous checkpoint. </p>"},{"location":"Chapter%2010/#parallel-execution","title":"Parallel execution","text":"<p>A vertex does not need to know on which physical machine it is executing. It is up to the framework to partition the graph -- i.e. to decide which vertex runs on which machine and how to route messages over the network so that they end up in the right place.</p> <p>Ideally graph would be partitioned such that vertices are colocated on the same machine if they need to communicate a lot. But finding such optimized partitioning is hard. In practice, cross-machine communication overhead is high. For this reason, if your graph can fit in memory on a single machine, it is likely that a single-machine algorithm will outperform a distributed batch process 73, 74. Efficiently parallelize graph algorithms is ongoing research </p>"},{"location":"Chapter%2010/#high-level-apis-and-languages","title":"High level APIs and Languages","text":"<p>The execution engines for distributed batch processing have matured since MapReduce. By now, the infrastructure has become robust enough to store and process many petabytes of data on clusters of over 10,000 machines. The problem of operating batch processes at such scale is considered solved. Attention has focused on other areas: improving the programming model, improving the efficiency of processing, and broadening the set of problems that these technologies can solve. </p> <p>As previously stated, higher level languages and APIs such as Hive, Pig, Cascading, and Crunch became popular because programming MapReduce jobs by hand is laborious</p> <p>These high level languages had the additional benefit of able to move to the new dataflow execution engine without the need to rewrite job code. Spark and Flink include their own high-level dataflow APIs, taking inspiration from FlumeJava</p> <p>These dataflow APIs generally use relational style building blocks to express a computation: joining datasets on value of some field; grouping by key; filtering by some condition; aggregating by counting, summing, or other functions. </p>"},{"location":"Chapter%2010/#move-toward-declarative-query-languages","title":"Move toward declarative query languages","text":"<p>Advantage of specifying joins as relational operators rather than code that performs the join is that the framework can analyze the properties of the join inputs and automatically decide which of the aforementioned join algorithms would be suitable for task at hand. </p> <p>Hive, Spark, and Flink have cost-based query optimizers that can do this, and even change the order of joins so that the amount of intermediate state is minimized [66, 77, 78, 79].</p> <p>Same thing in single machine relational operator...  But those dataflow operators in many ways are different than fully declarative model like SQL. MapReduce was built around the idea of function callbacks: for each record or group of records, a user-defined function(mapper or reducer) is called, and this function is free to call arbitrary code to decide what to output. </p> <p>The freedom of run arbitrary code is the main different between batch processing systems and MPP databases. </p> <p>Dataflow engines have found that there are advantages to include more declarative features in areas besides joins. For example, if a callback function contains only a simple filtering condition, or it just selects some fields from a record, then there is significant CPU overhead in calling the function on every record. </p> <p>If such filtering and mapping operations are expressed in declarative way, query optimizer can take advantage of column storage layout and iterating over data in a tight inner loop that is friendly to CPU caches and avoiding function calls. </p>"},{"location":"Chapter%2010/#specialization-for-different-domains","title":"Specialization for different domains","text":"<p>Reusable implementations are emerging: For example, Mahout implements various algorithms for machine learning on top of MapReduce, Spark, and Flink. </p> <p>As batch processing systems gain built-in functionality and high-level declarative operators, and as MPP databases become more programmable and flexible, the two are beginning to look more alike: in the end, they are all just systems for storing and processing data.</p>"},{"location":"Chapter%2010/#summary","title":"Summary","text":"<p>This chapter explored the topic of batch processing. Started by looking at Unix tools such as <code>awk</code>, <code>grep</code>, <code>sort</code>. And how similar logic carried forward to MapReduce and more recent dataflow engines. Some of those design principles are that inputs are immutable (same as functional programming) and outputs are intended to become the input to another program, complex problems are solved by composing small tools that \"do one thing well\"</p> <p>In Unix world, the uniform interface is files and pipes. In MapReduce, this interface is distributed filesystem. Dataflow engine add their own pipe-like data transport mechanism to avoid materializing intermediate state, but final output is still usually HDFS</p> <p>The two main problems that distributed batch processing frameworks need to solve are  Partitioning     In MapReduce, mappers are partitioned according to input file blocks. The output of mappers is repartitioned, sorted, and merged into a configurable number of reducer partitions.      Post MapReduce dataflow engine try to avoid sorting unless it is required.  Fault tolerance     MapReduce frequently write to disk which makes it easy to recover from individual task failure. Dataflow engines perform less materialization of intermediate state and keep more in memory. With trade of for recomputation when node fails</p> <p>Several join algorithms used in MapReduce are discussed. These algorithm are implemented in MPP and dataflow engines as well. They provide a good illustration of how partitioned algorithms work:</p> <p>Sort merge joins     Each of the inputs being joined goes through a mapper that extracts the join key. By partitioning, sorting, and merging. All the records with the same key end up going to the same call of the reducer. This function can then output the joined records. Broadcast hash joins     One of the two joins input is small, so it is not partitioned and it can be entirely loaded into a in-memory hash table. Thus load small input into each mapper, and scan over large input one record at a time.  Partitioned hash joins     If the two join inputs are partitioned in the same way, then hash table can be independently used for each partition</p> <p>Distributed batch processing engines hide some of the hard distributed systems problem behind its abstraction (in the face of crashes and network issues, tasks can be retried) </p> <p>Thanks to the framework, our code doesn't need to worry about implementing fault-tolerance mechanisms (framework did it for you) </p> <p>The distinguishing feature of batch processing job is that its input are bounded: known and fixed size. </p> <p>In next chapter, we will turn to stream processing, which input is unbounded. That is, you still have a job to process but the input is never ending stream of data. </p>"},{"location":"Chapter%2011/","title":"Chapter 11","text":"<p>Chapter 10 discussed about taking a set of input files, process it and output it into another set of files. This output is a form of derived data, i.e. a dataset that can be derived when needed </p> <p>One assumption on chapter 10 is that the input are bounded which means finite size. And batch process is often rely on this property because sorting a set of data need to read all data before proceeding to the next job. But in reality a lot of data is unbounded where it arrives gradually over time, such as user activity. Batch process often need to set a end time such as end of a day to process daily user activity. </p> <p>But this approach only reflect the output a day later, which might be slow for some users. We can run the process more frequent to reduce the delay (e.g. a second worth of data). Or just continuously process the data. This approach is called stream processing. This concept is in many places. Stdin and stdout in Unix system, TCP connections, deliver audio or video on internet etc.</p> <p>This chapter will focus on event streams. First discuss about how stream is represented, stored, transmitted over the network. Compare those stream system with databases, and finally discuss about those tools for processing streams</p>"},{"location":"Chapter%2011/#transmitting-event-streams","title":"Transmitting Event Streams","text":"<p>When processing input, batch will parse the input bytes into records, this is event in terms of stream system. They are essentially the same thing, a self contained immutable object that does something at a point of time. An event usually contains a timestamp. </p> <p>For example, user make a purchase at certain time, temperature from a sensor, CPU metrics etc. Event may be encoded as text or JSON or binary form as discussed in Chapter 4. Compare to batch process where file is written once and might be read multiple times, stream system has a producer (publisher or sender) that generate an event and potentially multiple consumers (subscribers or recipients) </p> <p>In a file system, file name group records together. In stream system, a topic or stream group related events together. </p>"},{"location":"Chapter%2011/#messaging-systems","title":"Messaging Systems","text":"<p>A common approach for notifying consumers about new events is to use messaging system: Producer sends a message and push it to consumers. Unix pipe or TCP connection would be a simple way of implementing a message system. Many system expand on this where unix pipe or TCP connection only allow one to one connection whereas messaging system allows multiple  producer to send message into a topic and allows multiple consumer connect to this topic. </p> <p>This is called publish/subscribe model and different system take a different approaches. To distinguish those system, two question might be helpful 1. What happens if producer is faster than consumer? Generally there are 3 options.      1. drop the message.     2. buffer message in a queue     3. apply back pressure (TCP and unix pipe does this. if the small buffer fills up, they slow down the producer)     What happen if the queue grows very large? If we write it to disk, how does it impact performance? 2. What happens if a node crash or temporarily go offline? any message get lost? Writing to disk or replication has cost</p>"},{"location":"Chapter%2011/#direct-messaging-from-producers-to-consumers","title":"Direct messaging from producers to consumers","text":"<p>A number of messaging system use direct network to connect between producers and consumers: - UDP broadcast is widely used in financial industry where latency matters. Although UDP itself is unreliable, application layer can recover the lost messages (producer must remember packet and resend on demand) - Brokerless messaging libraries such as ZeroMQ [9] and nanomsg take a similar approach, implementing publish/subscribe messaging over TCP or IP multicast. - StatsD [10] and Brubeck [7] use unreliable UDP messaging for collecting metrics from all machines on the network and monitoring them. - If consumer expose a service over the network, producer can send a request by using HTTP or RPC call. This is the idea behind webhooks [12] a pattern where a callback url is registered inside another service.  This approach has limited reliability </p>"},{"location":"Chapter%2011/#message-brokers","title":"Message brokers","text":"<p>Another approach that is widely used is message brokers (aka message queue) which is kind of a database that is optimized for handling message streams [13]. It runs as a server and producer and consumer connect to it.</p> <p>This way client can come and go because durability problem goes into broker instead. Some broker only keep message in memory and other write to disk</p> <p>broker/queue are often asynchronous (producer don't wait for consumer to respond, only wait for broker to respond) </p>"},{"location":"Chapter%2011/#message-brokerqueue-compare-to-databases","title":"Message broker/queue compare to databases","text":"<p>There are many differences between message queue and databases despite they both store data</p> <ul> <li>Database usually keep data until it is deleted explicitly. On the other hand, most message broker delete the message when it successfully deliver to consumer. </li> <li>Since they delete the message, message queue often assume their working set if often small. </li> <li>Database often support secondary indexes but not message queue (message queue can let consumer to select subset of data but it is different mechanism)</li> <li>Database support arbitrary queries but not message brokers/queue</li> </ul>"},{"location":"Chapter%2011/#multiple-consumers","title":"Multiple consumers","text":"<p>Two patterns are used when multiple consumers are connect to same topic.  </p> <p>Load balancing     Each message is delivered to 1 of the consumer. This is useful when message are expensive to process  Fanout     Each message is delivered to all consumers </p> <p>two patterns can be combined</p>"},{"location":"Chapter%2011/#ack-and-redelivery","title":"Ack and redelivery","text":"<p>Broker use ack to ensure consumer received its message and successfully processed it. unacknowledged message can be redelivered to consumer but this will cause message out of order even if message queue try to preserve the order </p>"},{"location":"Chapter%2011/#partitioned-logs","title":"Partitioned Logs","text":"<p>Database and files are designed with permanent storage in mind and it is very good for reprocessing the data when needed (as in batch processing system) message queue doesn't have this in mind </p> <p>Why can we not have a hybrid, combining the durable storage approach of databases with the low-latency notification facilities of messaging? This is the idea behind log- based message brokers.</p>"},{"location":"Chapter%2011/#using-logs-for-message-storage","title":"Using logs for message storage","text":"<p>A log is simply an append only sequence of bytes on disk. Same can be used to implement a message broker: producer append the message at the end of the log. Consumer read the log sequentially and waits for the log to be updated if it reaches the end of the log. Unix tool <code>tail -f</code> which watches a file of data to be appended work like this</p> <p>In order to scale to higher throughput, the log can be partitioned. Different partition can then be hosted on different machines.  </p> <p>Within each partition, broker assign a monotonic increasing number or offset for each message. Apache Kafka [17, 18], Amazon Kinesis Streams [19], and Twitter\u2019s DistributedLog [20, 21] are log-based message brokers that work like this.</p> <p>Even though these message brokers write all messages to disk, they are able to achieve throughput of millions of messages per second by partitioning across multiple machines, and fault tolerance by replicating messages [22, 23].</p>"},{"location":"Chapter%2011/#log-vs-traditional-messaging-system","title":"Log vs traditional messaging system","text":"<p>Log natively support fanout because consumer can just read from an offset of a log. </p> <p>To achieve load balancing, broker can assign a partition to a consumer group. Typically consumer will read all messages inside a partition in a straight forward single threaded manner. This approach has some downside - Number of nodes sharing the work can be at most the number of partitions  - If a single message is slow to process, it holds up the processing of subsequent messages in that partition</p> <p>When you want to parallelize message by message base, JMS/AMPQ works well. When you want to increase message throughput where each message is fast to process and ordering is important, log based is better</p>"},{"location":"Chapter%2011/#consumer-offset","title":"Consumer offset","text":"<p>With offset, broker doesn't need to wait consumer's ack but simply periodically remember which offset this consumer was on. With offset, batching and pipelining make increase in throughput possible. </p> <p>This offset is similar to log sequence number from leader-follower replication scheme. In replication context, leader allows follower to reconnect and resume replication based on previous log sequence number. Same thing applies here, consumer act like a follower and message broker act like leader.</p>"},{"location":"Chapter%2011/#disk-space-usage","title":"Disk space usage","text":"<p>If we keep append to a log, we will run out of disk space eventually. To reclaim disk space, older segments are deleted or moved to archive storage. </p> <p>The log implements bounded buffer by using circular buffer or ring buffer where old message get overwrite by newer messages. </p> <p>With 6TB disk and 150MB/s rate of writing, you could keep 11 hours of log history. And usually we don't keep full write speed all the time, so this log can sometime goes to days or weeks worth of messages </p>"},{"location":"Chapter%2011/#when-consumers-cannot-keep-up-with-producers","title":"When consumers cannot keep up with producers","text":"<p>We have talked about 3 options previously  1. disgard the message 2. buffering  3. apply back pressure In this taxonomy, log based approach is under buffering category. </p> <p>You can set alarms for consumer when it falls behind significantly because log buffer is large enough where human intervention can fix the consumer and allow it to catch up before missing messages </p>"},{"location":"Chapter%2011/#replaying-old-messages","title":"Replaying old messages","text":"<p>For JMS and AMQP style message broker, processing and acknowledging message is a destructive operation. Whereas log based mess</p>"},{"location":"Chapter%2011/#databases-and-streams","title":"Databases and Streams","text":"<p>We see log based message system take idea from databases and apply them to messaging. Reverse is also possible: take ideas from messaging and streams, and apply them to databases</p> <p>Event is a record of something that happened at some point in time. This could be user action or sensor reading. Or it could be write to database. The write operation can be seen as an event that can be captured, stored, and processed. </p> <p>In fact, replication log is a stream of database write events. Follower process this stream of write events to their own database and end up with an accurate copy of the same data. </p> <p>state machine replication principle in Chapter 9#Total Order Broadcast states: if every event represents a write to the database, and every replica processes the same events in the same order, then replicas will all end up in the same final state. </p> <p>This is a hint for how we can solve heterogeneous data systems problems by bringing ideas from event streams to databases</p>"},{"location":"Chapter%2011/#keeping-systems-in-sync","title":"Keeping Systems in Sync","text":"<p>Throughout this book, we have seen no single data system can satisfy all storage, querying, processing needs. In practice, applications need to combine several different technologies in order to satisfy their requirements: An OLTP database to serve user requests, a cache to speed up common requests, full text index to handle search queries, and a data warehouse for analytics </p> <p>Same as for computing, GPU and CPU all have memory in common to process different needs</p> <p>As related data appears in several different places, they need to be kept in sync. If item is updated in database, it also needs to be updated in the cache, search indexes, and data warehouse. </p> <p>As we see batch process from previous chapter, we can see it can create search indexes and copy of database during each batch job. But periodic full data dump is too slow. An alternative is dual writes. But this can face race conditions </p> <p></p> <p>Another problem is one of the write may fail. In order to ensure that they either both succeed or fail, we need to solve atomic commit problem which could be expensive (2PC or consensus) </p> <p>Figure 11-4 isn't single leader so conflicts can occur. If we could make the search index a follower of the database, it could be much better.</p>"},{"location":"Chapter%2011/#change-data-capture","title":"Change Data Capture","text":"<p>Replication log has long been consider an internal implementation detail of database. More recently, there has been growing interest in change data capture (CDC), which is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other system</p> <p>You can capture changes in database and continually apply them in search index </p>"},{"location":"Chapter%2011/#implementing-change-data-capture","title":"Implementing change data capture","text":"<p>Log consumers can be called derived data systems as discussed in Part 3: data stored in search index and data warehouse is just another view onto the data in the system of record. </p> <p>Change data capture can be seen as mechanism for ensuring all changes made to the system of record is reflected in derived data systems</p> <p>CDC basically made one database the leader and all into followers. A log based message broker is well suited for transporting source database changes to all followers</p> <p>Change data capture can implemented by using database triggers, but trigger based tend to be fragile and have performance overhead. More robust approach is to parse the replication log. </p> <p>LinkedIn\u2019s Databus [25], Facebook\u2019s Wormhole [26], and Yahoo!\u2019s Sherpa [27] use this idea at large scale. Bottled Water implements CDC for PostgreSQL using an API that decodes the write-ahead log [28], Maxwell and Debezium do something similar for MySQL by parsing the binlog [29, 30, 31], Mongoriver reads the MongoDB oplog [32, 33], and GoldenGate provides similar facilities for Oracle [34, 35].</p>"},{"location":"Chapter%2011/#initial-snapshot","title":"Initial snapshot","text":"<p>If you have all the changes from a log, then you can reconstruct the database by replaying the log. But keep all changes forever is not feasible in reality (too much disk space), replaying it takes too long. so the log needs to be truncated</p> <p>If you don't have entire snapshot history, you need to start with a snapshot. Which is all data that are stored at some point in time in that database. With this snapshot, you know at which point to start applying changes after the snapshot has been processed. Some CDC tools integrate this snapshot facility. </p>"},{"location":"Chapter%2011/#log-compaction","title":"Log compaction","text":"<p>Log compaction is a good alternative if you don't want to go through snapshot process every time you want to add a new derived data system</p> <p>Log compaction is done by periodically throw away duplicate record for same key and only keep the latest value (most recent update) </p> <p>With this compaction, you can scan through the change log and apply them to get the full copy of the database without having to take another snapshot of the CDC source database.</p> <p>This log compaction feature is supported by Apache Kafka. </p>"},{"location":"Chapter%2011/#event-sourcing","title":"Event Sourcing","text":"<p>There are some parallels between the ideas we\u2019ve discussed here and event sourcing, which is storing all changes to the application state as a log of change events. The biggest idea is that event sourcing applies the idea at a different level of abstraction: - In CDC, it is low level ensured by the database where application doesn't have to be aware of this is happening - In event sourcing, event store are designed to reflect the changes at application level  For example, game replay can use event sourcing to store all user activity to replay an entire game</p> <p>Event sourcing helps to debug application by making it easier to understand after the fact why something happened. </p> <p>For example, storing the event \"student cancelled their course enrollment\" is way better than \"one entry was deleted from the enrollments table, and one cancellation reason was added to the student feedback table\"</p>"},{"location":"Chapter%2011/#command-and-events","title":"Command and events","text":"<p>The event sourcing philosophy carefully distinguish between events and commands. </p> <p>When user request arrives, it may still fail so it is initially a command. If the validation is successful and command is accepted, it becomes an event (which is durable and immutable)</p> <p>At the event is generated, it becomes a fact. A consumer of the event stream is not allowed to reject an event. </p>"},{"location":"Chapter%2011/#state-stream-and-immutability","title":"State, Stream, and Immutability","text":"<p>The principle of immutability makes event sourcing and change data capture powerful</p> <p>mutable state and log of immutable events are two sides of the same coin. where current state is the integral of event stream over time and stream at time t will be the differentiate of the state by time $$ state(now) = \\int_{t=0}^{now} stream(t)  dt $$</p>"},{"location":"Chapter%2011/#advantages-of-immutable-events","title":"Advantages of immutable events","text":"<p>Immutability is an old idea in database. For example, accountants have been using immutability for centuries in financial bookkeeping. When transaction occurs, it is recorded in an append-only ledger.</p> <p>If a mistake is made, accountants don't erase or change the incorrect transaction in the ledge but add another transaction to compensate for the mistake instead. </p> <p>Immutable events capture more information than just the current state. For example if a user add an item to their cart and latter deletes it, from order fulfillment's view nothing happens, but for analytics purposes the customer was considering a particular item but then decided against it. This information is captured in an event log but lost in a database</p>"},{"location":"Chapter%2011/#deriving-several-views-from-the-same-event-log","title":"Deriving several views from the same event log","text":"<p>separate logs from mutable state can derive several different read-oriented representation. For example, analytic database Druid ingests from Kafka, Pistachio k-v store uses Kafka as a commit log. It would make sense for many other storage to take their input from distributed log</p> <p>Having a translation step from event log to a database makes it easier to evolve application over time. If you want to introduce new features, you can use the event log to build a separate view for the new feature and run it alongside the existing systems without modifying them. Running old and new systems side by side is often easier than performing complicated schema migration in an existing system. </p> <p>With log based approach, you don't have to assume data must be written in the same form as it will be queried. </p>"},{"location":"Chapter%2011/#concurrency-control","title":"Concurrency control","text":"<p>The biggest downside of event sourcing and change data capture is that consumer are usually asynchronous. So it is possible for a user make a write to the log and find their write has not been reflected in the read view. </p> <p>One solution would be perform the updates of the read view synchronously with appending the event to the log which requires atomic transaction and requires keep the event log and read view in the same storage system. Or we can use distributed transaction to achieve this</p> <p>Deriving current state from event log simplifies concurrency control. With event sourcing, you can design an event such that it is self-contained where user action only requires a single write in one place -- namely appending the events to the log</p>"},{"location":"Chapter%2011/#processing-streams","title":"Processing Streams","text":"<p>There are 3 options for processing streams 1. Take the data in events and write it to a database, cache, search index, or similar storage system 2. Push events to user in some way, for example by sending email notifications, or streaming events by real-time dashboard. In this case, human is the consumer of the stream 3. Process 1 or more input streams and produce 1 or more output streams and output either option 1 or 2 (machine or human)</p> <p>The rest will discuss about option 3: processing streams to produce other derived streams. A piece of code that process streams is known as an operator or a job. It is closely related to Unix processes and MapReduce jobs</p> <p>The patterns of partitioning and parallelization in stream processors are very similar to those in MapReduce in Chapter 10 so we won't repeat those topics here. </p> <p>Since stream data is unbounded, sort-merge joins cannot be used. Fault tolerance mechanisms must change: batch job can restart a failed task, but with streaming job that has been running for a long time is not a viable option</p>"},{"location":"Chapter%2011/#uses-of-stream-processing","title":"Uses of Stream Processing","text":"<p>Stream has long been used for monitoring purposes. For example - Fraud detection systems need to determine if the usage patterns of a credit card have unexpectedly changed, and block the card if it is likely to have been stolen - Trading systems need to examine price changes in a financial market and execute trades according to specified rules. - Manufacturing systems need to monitor the status of machines in a factory and quickly identify the problem if there is a malfunction - Military and intelligence systems need to track potential sign of attack</p>"},{"location":"Chapter%2011/#complex-event-processing","title":"Complex event processing","text":"<p>Complex event processing(CEP) is an approach developed in the 1990s for analyzing event streams, especially when application need to search for certain event patterns. Just like regular expression allows you to search for certain patterns of characters in a string, CEP allows you to specify rules to search for certain patterns of events in a stream</p> <p>CEP systems often use a declarative query language like SQL or a graphical user interface. These queries are submitted to a processing engine that consumes the input streams and maintains a state machine that performs required matching. When a match is found, it emits a complex event with details of the event pattern that was detected</p> <p>The relationship between queries and data is reversed in these system compare to normal databases. </p> <p>Normally, databases store data persistently and queries are consider transient. CEP engines reverse these roles: queries are stored long-term, and events from the input streams continuously flow past them.</p>"},{"location":"Chapter%2011/#stream-analytics","title":"Stream analytics","text":"<p>Another area for stream processing is to add analytics on streams. The boundary is blurry between stream analytics and CEP. As a general rule, analytics is less interested in specific event and is more interested toward aggregations and statistical metrics over large number of events. For example,</p> <ul> <li>Measuring the rate of some type of event</li> <li>Calculating rolling average of a value over some time period</li> <li>Comparing current statistics to previous time intervals </li> </ul> <p>Such statistics are usually computed over fixed time intervals. For example, you want to know average QPS for a service over last 5 mins. Or average TPS in 5 mins period over last day, and 99th percentile response time during that period. </p> <p>Many open source distributed stream processing frameworks are designed with analytics in mind: for example, Apache Storm, Spark Streaming, Flink, Concord, Samza, and Kafka Streams [74]. Hosted services include Google Cloud Dataflow and Azure Stream Analytics.</p> <p>~~#### Maintaining materialized views  We have discussed about materialized view previously where it is an alternative view on some dataset so we could query it more efficiently  In event sourcing, the application state is kind of a materialized view. ~~</p>"},{"location":"Chapter%2011/#search-on-streams","title":"Search on streams","text":"<p>Sometimes we need to search for individual events based on complex criteria, such as full text search queries.</p> <p>For example, media monitoring services subscribe to feeds of news articles and broadcasts from media outlets and search for any news mentioning companies, products or topics of interests. This is done by creating a search query in advance and then continually matching the stream of news items against the query. Similar feature existing on website, user get notification when a house that matches their criteria shows up on the market. </p> <p>The percolator feature of Elasticsearch is one option of implementing this kind of stream search</p>"},{"location":"Chapter%2011/#reasoning-about-time","title":"Reasoning About Time","text":"<p>Stream processor often need to deal with time, especially when used for analytics such as \"average over last 5 minutes\" </p> <p>Many stream processing framework use local system clock to determine windowing. This approach has advantage of being simple, but it breaks down if there is significant processing lag, i.e. if the processing may happen way much later than the time at which the event actually occurred. </p>"},{"location":"Chapter%2011/#event-time-vs-processing-time","title":"Event time vs processing time","text":"<p>There are many reason why processing might get delayed. Network fault, queueing, a restart of consumer process, or reprocessing past events. </p> <p>On top of that, message delay can lead to unpredictable ordering of messages. If user first make web request handled by web server A and then second request handled by web server B. A and B emits event to message broker and B's event reaches message broker first. Now stream processor will see B's event first even though they actually occurred in the other way around. </p> <p>If it helps to have an analogy, consider the Star Wars movies: Episode IV was released in 1977, Episode V in 1980, and Episode VI in 1983, followed by Episodes I, II, and III in 1999, 2002, and 2005, respectively, and Episode VII in 2015 [80].ii If you watched the movies in the order they came out, the order in which you processed the movies is inconsistent with the order of their narrative.</p> <p>(The episode number is like the event timestamp, and the date when you watched the movie is the processing time.)</p> <p></p> <p>You can declare when to timeout for a window to process all the events.  For example, when we grouping events into 1 min windows and after 1 min passed, it could still happen that some events were buffered on another machine somewhere, delayed due to a network interruption. These straggler events need to be handled. We have 2 options:  1. Ignore it. Alert if we start dropping a significant amount of data 2. Publish a correction, an updated value for the window with stragglers included. </p>"},{"location":"Chapter%2011/#whose-clock-are-you-using-anyway","title":"Whose clock are you using, anyway?","text":"<p>Event may be delayed for hours or days if mobile device doesn't have internet connection. (events are buffered locally) </p> <p>In this context, the timestamp on the events should really be the time at which user interaction occurred, according to the mobile's local clock. However it cannot be trusted, because user might accidentally or deliberately set to the wrong time. The time at which event was received by the server is more accurate since you have control over your server. </p> <p>To adjust for incorrect device clocks, one approach is to log 3 timestamps: 1. At time at which the event occurred (device clock) 2. At time event was sent to server (device clock) 3. Time at which event was received by the server (server clock)</p> <p>subtracting 3rd and 2nd, you can estimate the offset between device clock and server clock. You can then apply that offset to the event timestamp thus estimate true time at which event occurred</p>"},{"location":"Chapter%2011/#types-of-windows","title":"Types of windows","text":"<p>Tumbling window     A tumbling window has fixed length, and every event belongs to exactly one window. For example, if you have 1min tumbling window, all timestamp between 10:03:00 and 10:03:59 are grouped into one, and 10:04:00 to 10:04:59 grouped into another.  Hopping window     A hopping window also has a fixed length, but allows windows to overlap in order to provide some smoothing. For example, 5 mins window with a hop size of 1 min would contain events from 10:03:00 and 10:07:59 and next window would cover between 10:04:00 and 10:08:59 Sliding window     A sliding window contains all the events that occur within some interval of each other. For example, a 5 mins sliding window would cover events at 10:03:39 to 10:08:12 Session window     Session window has no fixed length. It is defined by grouping together all events for the same user that occur closely together in time. </p>"},{"location":"Chapter%2011/#stream-joins","title":"Stream Joins","text":"<p>Joins on streams are more challenging than batch jobs. We have 3 types of joins  stream-stream joins, stream-table joins and table-table joins </p>"},{"location":"Chapter%2011/#stream-stream-join-window-join","title":"Stream-stream join (window join)","text":"<p>Let's say you want to detect recent trends in searched-for URLs. Every time someone types a search query, you log an event containing the query and results returned. Every search result clicked by user is recorded as an event. In order to calculate the click-through rate for each URL in the search results, you need to join the events for the search action and the click action </p> <p>The click may never come if user abandons their search. Or they may be hours or days apart (user searched for something and left the tab open and only click after few days later) In this case, we can choose suitable window for the join -- for example, only join a click with a search if they occur at most one hour apart</p> <p>To implement this type of join, a stream processor needs to maintain state: for example, all the events that occurred in the last hour, indexed by session id. Whenever a search event or click event occurs, it is added to the appropriate session.</p>"},{"location":"Chapter%2011/#stream-table-join-stream-enrichment","title":"Stream-table join (stream enrichment)","text":"<p>In previous chapter, we see example of joining user activities with user profile. Naturally user activity can be think of a stream and processed continuously by stream processor. To perform this join, the stream process needs to look at one activity at a time and look up user ID from the database (remotely), and add profile information to the activity event. </p> <p>Another approach is to load a copy of the database into the stream processor so that it saves network trip. </p>"},{"location":"Chapter%2011/#table-table-join-materialized-view-maintenance","title":"Table-table join (materialized view maintenance)","text":"<p>Consider Twitter timeline example and that when a user wants to view their home timeline, it is too expensive to iterate over all the people the user is following</p> <p>We want the timeline cached instead, per user \"inbox\" so that reading the timeline is a single lookup. Materializing and maintaining this cache requires the following: - When user u sends a new tweet, it is added to the timeline of every user who is following u - when a user deletes a tweet, it is removed from all user's timelines. - when user \\(u_{1}\\) starts following user \\(u_{2}\\) recent tweets by \\(u_{2}\\) are added to \\(u_{1}\\)'s timeline. - When user \\(u_{1}\\) unfollows \\(u_{2}\\), tweets by \\(u_{2}\\) are removed from \\(u_{1}\\)'s timeline. </p> <p>To implement this in a cache by stream processor, you need streams of events for tweets and follow relationships. </p> <p>Basically joining two tables (tweets and follows)  <pre><code>SELECT follows.follower_id AS timeline_id,\n    array_agg(tweets.* ORDER BY tweets.timestamp DESC)\nFROM tweets\nJOIN follows ON follows.followee_id = tweets.sender_id\nGROUP BY follows.follower_id\n</code></pre></p>"},{"location":"Chapter%2011/#fault-tolerance","title":"Fault Tolerance","text":"<p>Batch process can tolerate fault pretty easily because all outputs are written to file.  Fault tolerance is less straightforward for streaming </p>"},{"location":"Chapter%2011/#microbatching-checkpointing","title":"Microbatching checkpointing","text":"<p>One solution is to break stream into small blocks and treat it as mini batch process called microbatching and it is used by Spark Streaming. </p> <p>Another approach is used in Apache Flink where it periodically generate rolling check points and write them to durable storage. </p>"},{"location":"Chapter%2011/#rebuilding-state-after-failure","title":"Rebuilding state after failure","text":"<p>Any stream that requires state -- any tables or indexes used for joins-- must ensure this state can be recovered after a failure. </p> <p>One option is to keep state in remote datastore and replicate it. Or keep state local to the stream processor and replicate it periodically. so when stream processor fails, it can recover by reading replicated state and resume processing</p> <p>For example, Flink periodically captures snapshots of operator state and writes them to durable storage such as HDFS </p>"},{"location":"Chapter%2011/#summary","title":"Summary","text":"<p>In this chapter we have discussed event streams, why we need them and how to process them. It is similar to batch process but with unbounded data. </p> <p>We spent time comparing two types of message brokers AMQP/JMS-style message broker     The broker assign individual messages to consumers and consumers ack individual messages when they successfully processed them. This approach is appropriate as an asynchronous form of RPC  Log based message broker     The broker assigns all messages in a partition to the same consumer node. consumer get message by reading from an offset. </p> <p>Stream can come from user activities, sensors, data feeds (market data in finance) </p> <p>Representing database as streams opens up the possibility of integrating different data systems. You can keep derived data such as search indexes, caches up to date by consuming the log of changes and applying them to the derived system. </p> <p>We discussed several purpose of stream processing, such as searching for event patterns (complex event processing), aggregations (stream analytics), and keep derived data system up to date. </p> <p>We also talked about 3 types of joins stream-stream joins: related events within some window of time stream-table joins: one side is input from stream, another is data from database (user profile) table-table joins: both input are database changelogs. </p>"},{"location":"Chapter%2012/","title":"Chapter 12","text":"<p>This chapter will look into the future of data systems</p>"},{"location":"Chapter%2012/#data-integration","title":"Data Integration","text":"<p>For complex applications, data often used in several different ways. We often have to use multiple kind of database together to accomplish one goal</p>"},{"location":"Chapter%2012/#combining-specialized-tools-by-deriving-data","title":"Combining Specialized Tools by Deriving Data","text":"<p>It is common to combine OLTP database with full-text search index. This can be done by introducing log based message broker that has CDC feature implemented. We might arrange data to write to a system of record database and use CDC to forward those change to search index in the same order. </p> <p>Allowing application directly write to both database and search index will introduce race condition. </p>"},{"location":"Chapter%2012/#derived-data-vs-distributed-transactions","title":"Derived data vs distributed transactions","text":"<p>Classic approach for keep different data system is to use distributed transactions such as 2PC. At an abstract level, 2PC using lock for mutual exclusion while CDC and event sourcing use a log for ordering. Distributed transactions use atomic commit to ensure that changes only effect exactly once, while log-based systems are based on deterministic retry and idempotence. </p> <p>Biggest difference is transaction system provide linearizability where derived data system often updated asynchronously, so they do not offer same timing guarantees by default</p> <p>Because of lack support of good distributed transaction protocol, Martin believe that log based derived data is the most promising approach for integrating different data systems. </p>"},{"location":"Chapter%2012/#the-limits-of-total-ordering","title":"The limits of total ordering","text":"<p>When system scaled bigger enough, limitations of total ordering begins to emerge: - In most cases, constructing a totally ordered log requires all events to pass through a single leader that decides on the ordering. If the throughput of events is greater than single machine can handle, we need to partition it across multiple machines. The order across different partitions is ambiguous - If servers are spread across multiple geographically distributed datacenters, where you would like to tolerate entire data center go offline. This typically requires a separate leader in each datacenter. This implies undefined ordering of events that originate in two different datacenters. - For microservices, a common design is to deploy storage unit as an independent unit. So no two services share same storage system. When two events originate in different services, there is no defined order for those events.  - Some application maintain client side state where user input get immediately updated and continue to work offline. With such application, clients and servers are very likely to see events in different orders.</p> <p>In formal terms, deciding on a total order of events is known as total order broadcast, which is equivalent to consensus. </p> <p>Most consensus algorithms are designed with single node throughput. It is still an open research problem to design consensus algorithms that can scale beyond the throughput of a single node</p>"},{"location":"Chapter%2012/#ordering-events-to-capture-causality","title":"Ordering events to capture causality","text":"<p>In social network app, after a couple break up, they should not see each other's rude message. This has a unfriend event followed by message event. If this causal dependency is not captured, message will send notification to the user. Starting point to solve this problem could be </p> <ul> <li>Logical timestamp can provide total ordering so they can help and require recipient to handle events that is out of order.</li> <li>Conflict resolution algorithm </li> </ul>"},{"location":"Chapter%2012/#batch-and-stream-processing","title":"Batch and Stream Processing","text":"<p>There are also many detailed differences in the ways the processing engines are implemented, but these distinctions are beginning to blur.</p> <p>Spark performs stream processing on top of a batch processing engine by breaking the stream into microbatches, whereas Apache Flink performs batch processing on top of a stream processing engine [5].</p>"},{"location":"Chapter%2012/#maintaining-derived-state","title":"Maintaining derived state","text":"<p>Batch process has a strong functional flavor where outputs are deterministic and can be retried many times. No matter what derived data is (cache, search index, or statistical model), it can be think of as a data pipeline. In principle derived data should be maintain synchronously but asynchronous processing make the system more robust (log based message system)</p>"},{"location":"Chapter%2012/#reprocessing-data-for-application-evolution","title":"Reprocessing data for application evolution","text":"<p>Schema migration on railway is a very interesting example Similarily, batch and stream processing can be used for schema evolution. </p> <p>You can then start shifting a small number of users to the new view in order to test its performance and find any bugs, while most users continue to be routed to the old view. Gradually, you can increase the proportion of users accessing the new view, and eventually you can drop the old view [10].</p>"},{"location":"Chapter%2012/#lambda-architecture","title":"Lambda architecture","text":"<p>lambda architecture is the proposal to combine batch and stream processing. </p> <p>The lambda architecture proposes running two different systems in parallel: a batch processing system such as Hadoop MapReduce, and a separate stream- processing system such as Storm.</p> <p>Although lambda architecture was an influential idea, Martin think it has couple of practical problem:</p> <ul> <li>Maintaining batch and stream process at the same time is an additional effort. </li> <li>Stream pipeline and batch pipeline produce separate output and it can be hard to join them together </li> <li>for extreme large dataset, batch pipeline need to setup incremental batch which can have timing issues</li> </ul>"},{"location":"Chapter%2012/#unifying-batch-and-stream-process","title":"Unifying batch and stream process","text":"<p>Unifying batch and stream processing in one system requires the following features, which are becoming increasingly widely available: - Replay historical events from same stream engine - Exact once processing semantics  - Tools for windowing by event time</p>"},{"location":"Chapter%2012/#unbundling-databases","title":"Unbundling Databases","text":"<p>At most abstract level, databases, Hadoop, operating systems are all store some data and allow you to process and query those data. </p> <p>Database store data in row and column where OS store it in file, but at its core both system are information management system</p> <p>Unix and relational databases have approached information management problem with different philosophies. Unix would like to provide programmer an abstract view of low level hardware. Whereas relational databases give application programmer a high level abstraction that hides complexities of data structures on disk, concurrency and crash recovery. </p> <p>Unix is basically thin wrapper around hardware resources and relational databases can draw a lot of powerful infrastructure (query optimization, indexes, join methods, concurrency control, replication etc) </p> <p>Both Unix and relational model emerged in 1970s and those different approach still effect today's world. Martin would interpret NoSQL movement as apply a Unix approach of low level abstraction in distributed OLTP domain </p> <p>Martin will try to reconcile the two philosophies in this section and hope to combine both worlds </p>"},{"location":"Chapter%2012/#composing-a-data-storage-technologies","title":"Composing a Data Storage Technologies","text":"<p>Over the course of the book, we have seen different feature provided by databases - Secondary indexes  - materialized views (precomputed cache) - Replication logs  - Full text search indexes </p> <p>Those features are very similar that batch and stream process are trying to perform</p>"},{"location":"Chapter%2012/#creating-an-index","title":"Creating an index","text":"<p>When <code>CREATE INDEX</code> run on relational database, it will scan over a consistent snapshot of the table, sort them, and write it out. This process is very similar setting up a new follower or bootstrapping change data capture in stream system</p>"},{"location":"Chapter%2012/#the-meta-database-of-everything","title":"The meta database of everything","text":"<p>The dataflow across entire organization starts looking like one huge database. Whenever, batch, stream, or ETL process transport data from one place to another, it is like keeping this huge database subsystem up to date. </p> <p>Martin think there are two ways to compose different databases into a cohesive system</p> <p>Federated Databases: Unifying reads It is possible to provide unified query interface to different storage engine and processing methods. This approach is called federated database or polystore </p> <p>Applications that need a specialized data model or query interface can still access the underlying storage engines directly, while users who want to combine data from disparate places can do so easily through the federated interface.</p> <p>JDB can use specialized data structure to speed up read?</p> <p>Unbundled databases: Unifying writes  Through change data capture or event logs, we could synchronized write across different technologies. This approach is like Unix tradition of small tools that do one thing well and communicate through uniform API (pipes) and composed using a higher level language (shell)</p>"},{"location":"Chapter%2012/#making-unbundling-work","title":"Making unbundling work","text":"<p>Martin believe that an asynchronous event log with idempotent writes is a much more robust and practical approach.</p> <p>The big advantage of log-based integration is loose coupling between the various components  1. Asynchronous event stream make system more robust against individual components failure.  2. Unbundling data systems allows each component developed, maintained and independently from different teams. Thus allow each team focus on doing one thing well. </p>"},{"location":"Chapter%2012/#unbundled-vs-integrated-system","title":"Unbundled vs integrated system","text":"<p>The goal of unbundling is not to compete with individual databases on performance for particular workloads; the goal is to allow you to combine several different data\u2010 bases in order to achieve good performance for a much wider range of workloads than is possible with a single piece of software. It\u2019s about breadth, not depth</p>"},{"location":"Chapter%2012/#designing-applications-around-dataflow","title":"Designing Applications Around Dataflow","text":"<p>Composing specialized storage and process systems with application code is called database inside out approach (Martin's talk on this) </p> <p>The term unbundling in this context was proposed by Jay Kreps [7].</p> <p>In this section Martin will expand on these ideas and explore some ways of building applications around the ideas of unbundled databases and dataflow.</p>"},{"location":"Chapter%2012/#separation-of-application-code-and-state","title":"Separation of application code and state","text":"<p>~~Deployment and cluster management tools such as Mesos, YARN, Docker, Kubernetes, and others are designed specifically for the purpose of running application code.  ~~ In a typical web application, database is like a shared mutable variable that can be accessed synchronously over the network</p> <p>However, in most programming languages you cannot subscribe to changes in a mutable variable\u2014you can only read it periodically. Unlike in a spreadsheet, readers of the variable don\u2019t get notified if the value of the variable changes. (You can implement such notifications in your own code\u2014this is known as the observer pattern\u2014 but most languages do not have this pattern as a built-in feature.)</p>"},{"location":"Chapter%2012/#dataflow-interplay-between-state-changes-and-application-code","title":"Dataflow: Interplay between state changes and application code","text":"<p>Thinking about applications in terms of dataflow allows us to rethink relationship between application code and state management.</p>"},{"location":"Chapter%2012/#stream-processors-and-services","title":"Stream processors and services","text":"<p>SOA (service oriented architecture) allow organizational scalability by decoupling. </p> <p>Composing stream operators into dataflow systems has a lot of similar characteristics to the microservices approach [40].</p> <p>However, underlying mechanisms are different. microservices require synchronous request/response interaction where dataflow system is one direction and asynchronous </p> <p>Dataflow system sometimes can achieve better performance. For example, when a customer purchase a product at one currency but pay it in another currency, there are two approaches to this problem:  1. In microservices approach, purchase could query an exchange rate service in order to obtain the current rate for particular currency 2. In dataflow approach, code that processes purchases would subscribe to stream of exchange rate and recorded it in local database. So when it processing purchase request, it only need to query local database.</p> <p>Second approach will not only be faster but more robust to the failure of another service. </p>"},{"location":"Chapter%2012/#observing-derived-state","title":"Observing Derived State","text":"<p> write path is where data is stored into the system and read path is when user request for a value.  Derived data is where write path meet read path</p>"},{"location":"Chapter%2012/#materialized-views-and-caching","title":"Materialized views and caching","text":"<p>Full text search is a good example: write path updates the index and read path searches the index for keywords. </p> <p>If you didn\u2019t have an index, a search query would have to scan over all documents (like grep), which would get very expensive if you had a large number of documents. No index means less work on the write path (no index to update), but a lot more work on the read path.</p> <p>On the other hand, you could imagine precomputing the search results for all possible queries. In that case, you would have less work to do on the read path: no Boolean logic, just find the results for your query and return them. However, the write path would be a lot more expensive</p> <p>Another option would be to precompute the search results for only a fixed set of the most common queries. This would generally be called a cache of common queries, although we could also call it a materialized view, as it would need to be updated when new documents appear that should be included in the results of one of the common queries.</p>"},{"location":"Chapter%2012/#stateful-offline-capable-clients","title":"Stateful offline capable clients","text":"<p>The client/server model in which clients are largely stateless and servers have the authority of the data is so common where we forget the other possibility</p> <p>Recent single page application allow client side user interface and local storage in the web browser. Mobile apps can also store a lot of state on device and don't need network round-trip </p> <p>These capabilities led to renewed interest in offline-first applications that process as much as request locally as possible and sync with server in the background.</p> <p>When we move away the assumption of stateless client, a world of new opportunities opens up. we can think of on-device is a cache state of the server. The model objects are a local replica of state in a remote datacenter.</p>"},{"location":"Chapter%2012/#pushing-state-changes-to-clients","title":"Pushing state changes to clients","text":"<p>In a typical web page, server don't do anything until you submit a request or reload the page. </p> <p>More recent protocol such as server-sent events and WebSocket provide communication channel where server maintain an open TCP connection with browser. This allows server actively push changes to client to prevent stale cache</p> <p>In terms of our model of write path and read path, actively pushing state changes all the way to client devices means extending the write path all the way to the end user. When a client is first initialized, it would still need to use a read path to get its initial state, but thereafter it could rely on a stream of state changes sent by the server.</p>"},{"location":"Chapter%2012/#aiming-for-correctness","title":"Aiming for Correctness","text":"<p>It is not a big deal if something goes wrong in stateless services. But for stateful systems it is not the case because they design to remember things forever which an error might last forever. </p> <p>Transactions properties of atomicity, isolation, and durability have been the tools of building correct applications. </p> <p>In some areas, transactions are being abandoned entirely and replaced with models that offer better performance and scalability, but much messier semantics </p> <p>Our engineering methods in this important topic (consistency) are surprisingly flaky. </p> <p>For example, it is very difficult to determine whether it is safe to run a particular application at a particular transaction isolation level or replication configuration [51, 52].</p> <p>Kyle Kingsbury\u2019s Jepsen experiments [53] have demoed many discrepancies where product claims safety guarantees and their actual behavior </p> <p>Life is a lot of simpler if our application can tolerate occasionally corrupting or losing data. On the other side, stronger guarantee like serializability and atomic commit came at a cost. They limit the scale and fault-tolerance properties you can achieve</p> <p>This section Martin will suggest some ways of thinking about correctness in the context of dataflow architectures</p>"},{"location":"Chapter%2012/#the-end-to-end-argument-for-databases","title":"The End to End Argument for Databases","text":"<p>Application that uses strong safety properties such as serializable transactions doesn't guarantee this application is free from data loss or corruption. If application has a bug that causes it to write incorrect data or delete data from database, serializable transactions can't save you</p> <p>Application bugs occur and people make mistakes. So immutable and append-only data is in favor because it is easier to recover from such mistakes </p> <p>But append only is not cure-all by itself. let's see some example </p>"},{"location":"Chapter%2012/#exactly-once-execution-of-an-operation","title":"Exactly once execution of an operation","text":"<p>When processing message, if previous process failed and you try again, there is a chance this message get processed twice. </p> <p>Processing twice is a form of data corruption (you don't want to be charged twice for the same service). </p> <p>One most effective approach to solve this problem is to make operation idempotent; that is, to ensure it has same effect no matter this message is processed how many times. In order to achieve this, we need additional metadata and fencing when failing over from one node to another </p>"},{"location":"Chapter%2012/#duplicate-suppression","title":"Duplicate suppression","text":"<p>idempotent which avoid duplicates occurs in many other areas as well. For example, TCP uses sequence number on packets to put them in correct order and determine whether any packets were lost or duplicated on the network. Any duplicates are removed by TCP before it sends the data to an application</p> <p>However, this benefit only exists in single TCP connection. Imagine TCP connection is client connection to a database and client suffers a network interruption and connection timeout after sending the COMMIT before it hears back from the database server. It does not know whether the transaction has been committed or aborted</p> <p>Let's say we have a transaction like this <pre><code>BEGIN TRANSACTION;\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts set balance = balance + 11.00 WHERE account_id = 4321;\nCOMMIT;\n</code></pre></p> <p>The client can reconnect to database and retry this transaction, but now this is outside of the scope of TCP duplication suppression. </p> <p>Even though this is a standard example for transaction atomicity, it is actually not correct and real banks do not work like this</p> <p>2PC protocols break the 1:1 mapping from TCP connection and a transaction since coordinator must ensure whether to commit or abort an in-doubt transaction. Is this sufficient to ensure that the transaction will only executed once? Unfortunately not. </p> <p>Even if transactions between database client and server is solved, we still need to worry about network between end-user device and the application server. If end user client is a web browser, it probably uses an HTTP POST request to submit an instruction to the server. and perhaps user successfully send POST request but failed to receive server's response due to poor cellular connection</p> <p>In this case user probably will be shown an error message, and they may retry manually. From web server's point of view the retry is a separate request, and from the database's point of view it is a separate transaction. The usual deduplication mechanisms don't help</p>"},{"location":"Chapter%2012/#operation-identifiers","title":"Operation identifiers","text":"<p>To make operation idempotent through several network hops, you need to consider end to end data flow. Not just at transaction level or TCP level</p> <p>For example, you could generate a unique identifier for an operation and included it as a hidden field in client application, or calculate a hash of all the relevant form fields to derive the operation ID. If the web browser submits the POST request twice, they will have same ID. You can then pass this ID all the way through database and check if we ever execute one operation with given ID </p> <pre><code>ALTER TABLE requests ADD UNIQUE (request_id)\nBEGIN TRANSACTION;\nINSERT INTO request\n    (request_id, from_account, to_account, amount)\n    VALUES('0286FDB8-D7E1-423F-B40B-792B3608036C', 4321, 1234, 11.00);\n\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 4321;\n\nCOMMIT;\n</code></pre> <p>above relies on <code>request_id</code> column and if transaction attempts to insert an ID that already exists, the INSERT fails and the transaction is aborted. </p>"},{"location":"Chapter%2012/#the-end-to-end-argument","title":"The end to end argument","text":"<p>suppressing duplicate transactions is just one example of a more general principle called end-to-end argument which was articulated by Saltzer, Reed, and Clark in 1984 55</p> <p>function can only be implemented correctly with the help of knowledge and application standing at the endpoints of the communication system. </p> <p>In our example, function was duplicate suppression. By themselves, TCP, database transactions, and stream processors cannot entirely rule out these duplicates. Solving the problem requires an end-to-end solution: a transaction identifier that is passed all the way from end user client to the database.</p> <p>End to end argument also checks integrity of data: checksums are built into Ethernet, TCP, and TLS to detect corruption of packets in the network, but they cannot detect corruption due to bugs in the software at sending and receiving ends of the network connection. </p>"},{"location":"Chapter%2012/#applying-end-to-end-thinking-in-data-systems","title":"Applying end to end thinking in data systems","text":"<p>This brings back Martin's original thesis: application that provides strong safety properties such as serializable transactions does not mean the application is guaranteed to be free from data loss or corruption </p> <p>TCP works well for abstracting low level fault tolerance but we have not yet found the right abstraction for application level</p> <p>Transaction have long been seen as a good abstraction and Martin believe that they are useful. But he fear this is not enough</p> <p>Transactions are expensive especially in heterogeneous storage technologies. When we refuse to use distributed transactions because they are too expensive, we end up reimplementing fault tolerance mechanisms in application code. And it is hard to implement those mechanisms ourselves</p> <p>For those reason, it is worth to explore fault tolerance abstractions that could provide application specific end to end correctness properties. </p>"},{"location":"Chapter%2012/#enforcing-constraints","title":"Enforcing Constraints","text":"<p>We saw end to end duplicate suppression can be achieved with request ID that is passed all the way from client to the database that records the write.</p>"},{"location":"Chapter%2012/#uniqueness-constraints-require-consensus","title":"Uniqueness constraints require consensus","text":"<p>We saw in Chapter 9 where unique username or email address require consensus</p> <p>The most common way of achieving this consensus is to make a single node the leader and put it in charge of making all the decisions. This works fine until you need to tolerate leader failing, and we are back to consensus problem</p> <p>Uniqueness checking can be scaled out by partitioning based on the value that needs to be unique. For example, you can partition by hash of username and you can ensure all requests with the same request ID are routed to the same partition </p> <p>Asynchronous multi-leader master replication is ruled out because it could take conflicting writes and values are no longer unique. If you want to be able to immediately reject any writes that would violate the constraint, synchronous coordination is unavoidable.</p>"},{"location":"Chapter%2012/#uniqueness-in-log-based-messaging","title":"Uniqueness in log-based messaging","text":"<p>The log ensure all consumer see messages in the same order. Formally known as total order broadcast and is equivalent to consensus. </p> <p>A stream processor consumes all the messages in a log partition sequentially on a single thread. If the log is partitioned based on the value that needs to be unique, a stream processor can unambiguously and deterministically decide which one of several conflicting operations came first. For example: 1. Every request for a username is encoded as a message, and appended to a partition determined by the hash of the username.  2. A stream processor sequentially reads the request in the log, using a local database to keep track of which usernames are taken. For every request that a username that is available, it records the name as taken and emits a success message to an output stream. For user name that is already taken, it emits a rejection message to output stream 3. The client watches output stream and waits for a success or rejection message corresponding to its request.</p> <p>This algorithm scales easily to a large request throughput by increasing the number of partitions, as each partition can be processed independently </p> <p>This approach not only works for uniqueness constraints, but also for many other kinds of constraints. The fundamental principle is that any writes that may conflict are routed to the same partition and processed sequentially. </p>"},{"location":"Chapter%2012/#multi-partition-request-processing","title":"Multi partition request processing","text":"<p>Things becomes more interesting when multiple partitions are involved. In this example <pre><code>ALTER TABLE requests ADD UNIQUE (request_id)\nBEGIN TRANSACTION;\nINSERT INTO request\n    (request_id, from_account, to_account, amount)\n    VALUES('0286FDB8-D7E1-423F-B40B-792B3608036C', 4321, 1234, 11.00);\n\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 4321;\n\nCOMMIT;\n</code></pre> there are potentially 3 partitions: 1 containing request ID, 1 containing payee account, 1 containing payer account. Since they are independent from each other, they could be on different partitions.</p> <p>In traditional approach, executing this transaction would require an atomic commit across all partitions. Throughput is likely to suffer because it require all partition come into a total order together</p> <p>However, it turns out that equivalent correctness can be achieved with partitioned logs, and without an atomic commit: 1. The request transfer money from A to B is given a unique request ID and appended to a log partition based on this request ID 2. A stream processor reads the log of requests. For each request message it emits 2 messages to output streams: a debit instruction payer account A and a credit instruction to payee account B. The original request ID is included in those emitted messages. 3. Further processors consume the streams of credit and debit instructions, deduplicate by request ID, and apply the changes to the account balances.</p> <p>If the stream processor in step 2 crashes, it resumes processing from its last check point. In doing so, it does not skip any request messages, but it may process requests twice or more and produce duplicate credit and debit instructions. Since we have unique request ID, step 3 can easily deduplicate them using this ID </p> <p>By breaking down the multi-partition transaction into two different partitioned stages and using end to end request ID, we have achieved same correctness and without using an atomic commit protocol. </p>"},{"location":"Chapter%2012/#timeliness-and-integrity","title":"Timeliness and Integrity","text":"<p>A convenient property of transaction is that writers wait until a transaction is committed and after its writes all readers can immediately see the results. </p> <p>This is not the case when using multiple stages in stream processors: consumers of a log are asynchronous by design. Sender doesn't wait for consumer to process its messages. However, it is possible for a client to wait for message to appear on an output stream. Just as when we mentioned previously </p> <ol> <li>Every request for a username is encoded as a message, and appended to a partition determined by the hash of the username. </li> <li>A stream processor sequentially reads the request in the log, using a local database to keep track of which usernames are taken. For every request that a username that is available, it records the name as taken and emits a success message to an output stream. For user name that is already taken, it emits a rejection message to output stream</li> <li>The client watches output stream and waits for a success or rejection message corresponding to its request.</li> </ol> <p>in this example, the uniqueness doesn't depend on whether sender wait or not. waiting only has the purpose of informing the sender whether or not the uniqueness check succeeded. so this notification can be decoupled from effects of processing the message.</p> <p>More generally, Martin think term consistency conflates 2 different requirements that are worth considering separately: Timeliness     Timeliness means ensuring that users observe the system in an up-to-date state. We have seen stale read can cause inconsistent state. CAP theorem uses consistency in the sense of linearizability (which is a strong way of achieving timeliness) Weaker timeliness properties like read-after-write consistency can also be useful  Integrity     Integrity means absence of corruption (no data loss, no contradictory data or false data)</p> <p>If integrity is violated, the inconsistency is permanent. Waiting and retry doesn't fix this issue. Atomicity and durability are important tools for preserving integrity </p> <p>In slogan form: violations of timeliness are \u201ceventual consistency,\u201d whereas violations of integrity are \u201cperpetual inconsistency.\u201d</p> <p>Martin assert integrity is much more important than timeliness. For example, It is not surprising if transaction for your credit card does not appear in last 24 hours -- It is normal for these system have a certain lag. Banks reconcile and settle transactions asynchronously, and timeliness is not very important here (3). And integrity violation such as disappearing money would be catastrophic </p>"},{"location":"Chapter%2012/#correctness-of-dataflow-systems","title":"Correctness of dataflow systems","text":"<p>ACID transactions usually provide both timeliness (linearizability) and integrity (atomic commit) guarantees. On the other hand, event based dataflow systems decoupled timeliness and integrity. When processing event streams asynchronously, there is no guarantee of timeliness unless you build consumers that wait for message to arrive before returning. integrity is in fact central to streaming systems.</p> <p>Exactly-once or effectively-once semantics is a mechanism for preserving integrity. Fault-tolerant message delivery and duplicate suppression are important for maintaining the integrity of a data system in the face of faults. </p> <p>As we saw in the last section, reliable stream processing systems can preserve integrity without distributed transaction and atomic commit protocol. which means same correctness but better performance and operational robustness. We achieved this integrity through a combination of mechanisms: - Representing the content of write operation as a single message. (which can be easily written atomically) - Deriving all other state from this single message using deterministic functions  - Passing client generated request ID through all these levels of processing, enabling end to end duplicate suppression and idempotence - Making message immutable and allow derived data to reprocess from time to time</p>"},{"location":"Chapter%2012/#loosely-interpreted-constraints","title":"Loosely interpreted constraints","text":"<p>Traditional uniqueness require all events are routed to particular partition and stream processing cannot avoid it.  However, many real application can get away with much weaker notions of uniqueness:</p> <ul> <li>If two people concurrently register the same username or book the same seat, you can send one of them a message to apologize, and ask them to choose a different one. This change to correct a mistake is called compensation transaction [59, 60]</li> <li>If customers order more items than you have in stock, you can order more and apologize to customer for the delay and offer them a discount. </li> <li> <p>Similarly, airlines overbook airplanes with expectation that some passengers will miss their flight and many hotels overbook rooms with expectation some guests will cancel. </p> <ul> <li>n these cases, the constraint of \u201cone person per seat\u201d is deliberately violated for business reasons, and compensation processes (refunds, upgrades, providing a complimentary room at a neighboring hotel) are put in place to handle situations in which demand exceeds supply. Even if there was no overbooking, apology and compensation processes would be needed in order to deal with flights being cancelled due to bad weather or staff on strike\u2014recover\u2010 ing from such issues is just a normal part of business [3].</li> </ul> </li> <li> <p>If someone withdraws more money than they have in their account, the bank can charge them an overdraft fee and ask them to pay back what they owe. By limit\u2010 ing the total withdrawals per day, the risk to the bank is bounded.</p> </li> </ul> <p>In many business contexts, it's actually acceptable to temporarily violate a constraint and fix it up later by apologizing. The cost of the apology varies but it is often quite low: you can't unsend an email but send a follow up email with a correction. </p> <p>Wether the cost of the apology is acceptable is a business decision. If it is acceptable, the traditional model of checking all constraint is not needed. </p>"},{"location":"Chapter%2012/#coordination-avoiding-data-systems","title":"Coordination-avoiding data systems","text":"<p>Now we have 2 interesting observations:  1. Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizability or synchronous cross-partition coordination  2. Although strict uniqueness require timeliness and coordination, many applications are actually fine with loose constraints that may be temporarily violated and fixed up later.</p> <p>These observations mean that dataflow system can provide the data management services without requiring coordination, while still giving strong integrity guarantees. </p> <p>For example, such a system could operate distributed across multiple datacenters in a multi-leader configuration, asynchronously replicating between regions. Such a system would have weak timeliness guarantees -- it would not be linearizable without introducing coordination -- but it can still have strong integrity guarantees.</p> <p>In this context, synchronous coordination can still be introduced in places where it is needed, but there is no need for everything to pay the cost of coordination if only a small part of an application needs it. </p>"},{"location":"Chapter%2012/#trust-but-verify","title":"Trust, but Verify","text":"<p>All of our discussion of correctness, integrity, and fault-tolerance has been under the assumption that certain things might go wrong, but other things won't. We call these assumptions our system model </p> <p>For example, we assume process might crash, machines can lose power, and network can arbitrarily delay or drop messages. But we also assume disk will not lose data after <code>fsync</code> and data in memory is not corrupted, CPU instruction always returns the correct result. </p> <p>Traditionally, system model take a binary approach toward faults: we assume some things can happen and other things can never happen. </p> <p>In reality, it is more a question of probabilities: something are more likely, somethings less likely. </p> <p>Data can become corrupted while it is sitting untouched on disks and data corruption on network can evade TCP checksums</p> <p>One application Martin worked on collected crash reports from clients and some of the reports we received could only be explained by random bit-flips in the memory of those devices. </p> <p>Besides random memory corrup\u2010 tion due to hardware faults or radiation, certain pathological memory access patterns can flip bits even in memory that has no faults [62]\u2014an effect that can be used to break security mechanisms in operating systems [63] (this technique is known as rowhammer). Once you look closely, hardware isn\u2019t quite the perfect abstraction that it may seem.</p>"},{"location":"Chapter%2012/#in-the-face-of-software-bugs","title":"In the face of software bugs","text":"<p>Besides hardware bugs, we also face software bugs. Even widely used database has bugs: Martin has personally seen cases of MySQL failing to correctly maintain a uniqueness constraint and PostgreSQL's serializable isolation level exhibiting write skew anomalies. even though MySQL and PostgreSQL are robust and well-regarded databases that have been battle-tested by many people for many years. In less mature software, the situation is likely to be much worse.</p>"},{"location":"Chapter%2012/#dont-just-blindly-trust-what-they-promise","title":"Don't just blindly trust what they promise","text":"<p>With both hardware and software problems, it seems that data corruption is inevitable sooner or later. Thus we should have at least a way of finding out if data has been corrupted so that we can fix it. Checking the integrity of data is known as auditing</p> <p>Auditing is not just for financial applications. It is highly important in finance because we know mistakes happen and we recognize the need to be able to detect and fix problems.</p> <p>Mature systems tend to consider the possibility of unlikely things going wrong and manage that risk. For example, large scale storage such as HDFS and S3 do not fully trust disks. they run background processes to continually read back files and move files from one disk to another in order to mitigate risk of silent corruption</p>"},{"location":"Chapter%2012/#a-culture-of-verification","title":"A culture of verification","text":"<p>HDFS and S3 assume disk work correctly most of the time. But not many system have this \"trust but verify\" approach of continually auditing themselves. We should think about designing auditability</p>"},{"location":"Chapter%2012/#designing-for-auditability","title":"Designing for auditability","text":"<p>Database with log of multiple insertions, updates, and deletions in various tables doesn't give full picture of why those mutations were performed. </p> <p>By contrast, event-based systems can provide better auditability. In event sourcing approach, user input to the system is represented as a single immutable event, any resulting state is derived from that event. </p> <p>Being explicit about dataflow makes the provenance of data much clearer, resulting integrity checking to be more feasible. For event log, we can check the hashes which event storage has not been corrupted. For derived state, we can rerun the batch or stream processors to check whether we get the same result. </p> <p>A deterministic and well-defined dataflow also makes it easier to debug and trace the execution of a system in order to determine why it did something [4, 69].</p>"},{"location":"Chapter%2012/#tools-for-auditable-data-systems","title":"Tools for auditable data systems","text":"<p>It would be interesting to use cryptographic tools to prove the integrity of a system in a way that is robust to a wide range of hardware and software issues, and even potentially malicious actions. </p> <p>Cryptocurrencies, blockchains, and distributed ledger technologies such as Bitcoin, Ethereum, Ripple, Stellar, and various others [71, 72, 73] have sprung up to explore this area.</p> <p>From a data systems point of view they contain some interesting ideas. Essentially they are distributed databases, with a data model and transaction mechanism, in which different replicas can be hosted by mutually untrusting organizations. The replicas continually check each other's integrity and use a consensus protocol to agree on the transactions that should be executed.</p> <p>I am somewhat skeptical about the Byzantine fault tolerance aspects of these technol\u2010 ogies (see \u201cByzantine Faults\u201d on page 304), and I find the technique of proof of work (e.g., Bitcoin mining) extraordinarily wasteful. The transaction throughput of Bitcoin is rather low, albeit for political and economic reasons more than for technical ones. However, the integrity checking aspects are interesting.</p> <p>Cryptographic auditing and integrity checking often relies on Merkle trees and certificate transparency relies on Merkle trees to check validity of TLS/SSL certificate. </p> <p>It could be an area where auditing algorithms become more widely used in data systems </p>"},{"location":"Chapter%2012/#doing-the-right-thing","title":"Doing the Right Thing","text":"<p>Every system is built for a purpose; every action we take has both intended and unintended consequences. The purpose might be making money, but the consequences for the world may reach far beyond that original purpose. As engineer we should consider what kind of world we want to live in when building systems. </p> <p>A technology is not good or bad in itself. What matters is how it is used and how it effects people. Software system like search engine is much the same way as it is for weapon like a gun. </p>"},{"location":"Chapter%2012/#predictive-analytics","title":"Predictive Analytics","text":"<p>Predictive analytics is major part of \"Big Data\" hype. Using data to predict weather or spread of diseases is one thing but a different matter when predict a convict is likely to reoffend, or applicant of a loan is likely to default, or whether a customer of insurance is likely to make expensive claims. Latter has direct impact on people's lives.</p>"},{"location":"Chapter%2012/#bias-and-discrimination","title":"Bias and discrimination","text":"<p>Decisions made by an algorithm are not necessarily any better or any worse than those made by a human. Every person is likely to have bias even if they actively try to counteract them. There is hope that basing decisions on data, rather than subjective and instinctive assessments by people could be more fair and give a better chance for people who are often overlooked in the traditional system</p> <p>Current predictive analytics system not only automating a human's decision by specify the rules to say yes or no; but also leaving the rules themselves to be inferred from data. But those patterns are opaque: even if there is correlation, we may not know why. </p> <p>In many countries, anti-discrimination laws prohibit treating people differently on protected traits such as ethnicity, age, gender, sexuality, disability or beliefs. but person's data contain those traits, for example in racially segregated neighborhoods, a person's postal code or even IP address is strong predictor of race. Predictive analytics systems merely extrapolate from the past; if we want future to be better than the past, moral imagination is required. And this is something only human can provide. </p>"},{"location":"Chapter%2012/#responsibility-and-accountability","title":"Responsibility and accountability","text":"<p>Credit rating agencies are an old example of collecting data to make decisions about people. Bad credit score can make life difficult, but at least credit score is based on actual borrowing history. However, scoring algorithms based on machine learning typically use a much wider range of inputs and more opaque. </p> <p>Blind belief in the supremacy of data for making decisions is not only delusional but dangerous. </p>"},{"location":"Chapter%2012/#feedback-loops","title":"Feedback loops","text":"<p>When services become good at predicting what content users want to see, they may end up showing people only opinions they already agree with, leading to echo chambers in which stereotypes, misinformation, and polarization can breed. As we already seeing in social media on election campaigns</p> <p>We can't always predict when such feedback loops happen. But many consequences can be predicted by thinking about the entire system. </p>"},{"location":"Chapter%2012/#privacy-and-tracking","title":"Privacy and Tracking","text":"<p>There are ethical problems with data collection itself. When user's activity is tracked and logged as a side effect of other things they are doing, what is the relationship between the organizations collecting the data and the user? The service no longer just does what the user tells it to do, but takes on interest of its own which may conflict with the user's interest.</p> <p>Tracking behavioral data can be helpful where search results are improved by ranking of search results click through rate; recommending \"people who liked X also liked Y\" helps users discover interesting and useful things. However, depending on company's business model, tracking doesn't stop there. If service is funded through advertising and advertisers are customers, then user's interests take second place. Tracking become more detailed and analysis become further reaching. </p> <p>Matin think this relationship can be appropriately described with a word that has more sinister connotations: surveillance</p>"},{"location":"Chapter%2012/#surveillance","title":"Surveillance","text":"<p>As a thought experiment, try replacing the word data with surveillance, and observe if common phrases still sound so good [93]. How about this: \u201cIn our surveillance- driven organization we collect real-time surveillance streams and store them in our surveillance warehouse. Our surveillance scientists use advanced analytics and surveillance processing in order to derive new insights.\u201d</p> <p>As we rush toward internet of things, we are rapidly approaching a world in which every inhabited space contains at least one internet connected microphone, in the form of smartphones, smart TVs, voice controlled assistant device, baby monitors. Many of those devices have terrible security record. </p> <p>Even the most totalitarian and repressive regimes could only dream of putting a microphone in every room and forcing every person to constantly carry a device capable of tracking their location and movements. Yet we apparently voluntarily, even enthusiastically, throw ourselves into this world of total surveillance. The difference is just that the data is being collected by corporations rather than government agencies</p>"},{"location":"Chapter%2012/#consent-and-freedom-of-choice","title":"Consent and freedom of choice","text":"<p>the relationship between the service and the user is very asymmetric and one- sided. The terms are set by the service, not by the user [99].</p>"},{"location":"Chapter%2012/#summary","title":"Summary","text":"<p>This chapter discussed new approaches to design data systems, Martin included personal opinions and speculations about the future. We discussed how to solve data integration problem by using batch processing and event streams to let data changes flow between different systems.</p> <p>In this approach, certain system are acting as systems of record, and other data is derived from them through transformations. in this way we can maintain indexes, cache, machine learning models, statistical summaries and more. By making these derivation and transformations asynchronous and loosely coupled, it increased robustness and fault tolerance of the system as a whole. </p> <p>These process are quite similar to what databases already do internally, so we recast the idea of unbundling the components of a database. And building an application by composing these loosely coupled components</p> <p>Derived state can be updated by observing changes in underlying data. We can extend this idea all the way through to the end user device and build user interfaces that dynamically update to reflect data changes and continue to work offline. </p> <p>Then we discussed end to end operation ID to make operations idempotent and by checking constraints asynchronously, client can either wait until the check has passed, or go ahead without waiting but risk having to apologize about a constraint violation. This approach is much more scalable and robust than distributed transactions. </p> <p>By structuring applications around dataflow and checking constraints asynchronously, we can avoid most coordination and create systems that maintain integrity that still performs well. </p> <p>Finally we took step back and discussed about ethical aspects of building data intensive applications. As software and data are having such a large impact on the world, we engineers must remember that we carry a responsibility to work toward the kind of world that we want to live in: a world that treats people with humanity and respect. </p> <p>reference that are mentioned multiple times 3, 59, 60</p>"},{"location":"Chapter%202/","title":"Chapter 2","text":"<p>The limits of my language mean the limits of my world.</p> <p>\u6211\u53d1\u73b0\u7ef4\u7279\u7518\u65af\u5766\u5728 software \u91cc\u9762\u4e5f\u7279\u522b\u51fa\u540d\u2026\u2026 \u5434\u519b\u8001\u5e08\u4e5f\u4e13\u95e8\u5199\u8fc7\u4e00\u7bc7\u6587\u7ae0\u6765\u4ecb\u7ecd\u4ed6 \u53ef\u80fd\u8fd9\u4e9b\u4eba\u90fd\u559c\u6b22\u54f2\u5b66\u5427\u2026\u2026</p> <p>data model \u53ef\u80fd\u662f software \u91cc\u9762\u6700\u91cd\u8981\u7684\u5143\u7d20\u4e86\uff0c\u56e0\u4e3a\u5b83\u4e0d\u4ec5\u51b3\u5b9a\u6211\u4eec\u5982\u4f55\u5199\u8f6f\u4ef6\uff0c\u8fd8\u51b3\u5b9a\u4e86\u6211\u4eec\u5982\u4f55 \u601d\u8003 \u6211\u4eec\u6709\u89e3\u51b3\u7684\u4e00\u4e2a\u95ee\u9898 \u6bd4\u5982\u6700\u77ed\u8def\u95ee\u9898\u5c31\u9700\u8981\u7528 \u56fe \u8fd9\u79cd data model \u5176\u4ed6\u7684 model \u53ef\u80fd\u4e5f\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u662f\u4e0d\u5982\u56fe\u6765\u7684\u65b9\u4fbf</p> <p>Most app are built by layering one data model on top of another</p> <p>\u5176\u5b9e\u8fd9\u4e5f\u662f abstraction \u7684\u4e00\u79cd\u4f53\u73b0\uff0c\u5728\u6bcf\u4e00\u4e2a layer \u4e0a\u9762\uff0c\u5173\u952e\u7684\u95ee\u9898\u5c31\u662f</p> <p>how is it represented in terms of next-lower layer </p> <p>\u6bd4\u5982 1. \u5728 app \u5c42\uff0c\u6211\u4eec\u628a\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u7269\u54c1/\u5bf9\u8c61\u62bd\u8c61\u6210\u4e00\u4e2a\u80fd\u591f\u8868\u8fbe\u4ed6\u4eec\u7684objects \u6216\u8005\u6570\u636e\u7ed3\u6784\u5c31\u53ef\u4ee5\u4e86\u3002\u6bd4\u5982 social graph, \u4e2a\u4eba\u8d44\u6599(age, sex, job, salary, etc) \u4ee5\u53ca\u80fd\u591f CRUD \u8fd9\u4e9b\u6570\u636e\u7684API 2. When store those data structures, you express them in terms of a general-purpose data model, such as JSON or XML documents, tables in a relational database, or a graph model. 3. DB \u5de5\u7a0b\u5e08\u5219\u4f1a\u8bbe\u8ba1\u5982\u4f55\u628a\u8fd9\u4e9b JSON/XML/relational/graph \u7528 bytes \u5b58\u5728 memory, disk \u4e0a\u9762\uff0c\u4ee5\u53ca\u5982\u4f55\u67e5\u8be2\u8fd9\u4e9b\u6570\u636e 4. \u518d\u5f80\u4e0b\u4e00\u5c42\uff0c\u786c\u4ef6\u5de5\u7a0b\u5e08\u5219\u8003\u8651 how to represent bytes in terms of electrical currents, pulses of light, magnetic fields, and more.</p> <p>In a complex application there may be more intermediary levels, such as APIs built upon APIs, but the basic idea is still the same: each layer hides the complexity of the layers below it by providing a clean data model.</p> <p>network \u4e5f\u662f\u8fd9\u6837\u7684\u3002Link layer \u5916\u9762\u5305\u4e00\u5c42network layer\uff0c \u518d\u662ftransport layer\uff0c \u6700\u540eapplication layer</p> <p>data model \u5c31\u662f\u73b0\u5b9e\u7684\u6a21\u578b\uff0c\u7136\u540e\u4ece\u8fd9\u91cc\u51fa\u53d1\uff0cjson in data base, representing data in terms of bytes in memory, on disk or on a network. This representation may allow the data to be queried, searched, manipulated, and processed in various ways.</p> <p>\u6bcf\u4e00\u4e2a data model \u90fd\u6709\u81ea\u5df1\u7684\u4f18\u52bf\u6216\u8005\u8bf4\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u597d\u5904\uff0c\u800c\u4e14\u57fa\u4e8e data model \u4e0a\u9762\u5f00\u53d1\u7684\u5de5\u5177\u6709\u4e9b\u4e8b\u60c5\u5c31\u53d8\u7684\u53ef\u80fd\u4e86(\u6216\u8005\u8bf4\u7528\u9519\u4e86 data model \u6709\u4e9b\u4e8b\u60c5\u6839\u672c\u6ca1\u6cd5\u5b8c\u6210). \u6240\u4ee5\u9009\u62e9\u4e00\u4e2a\u5408\u9002\u7684 data model \u5c31\u53d8\u5f97\u5f88\u91cd\u8981\u4e86</p> <p>\u8fd9\u4e00\u7ae0\u8ba8\u8bba\u4e86\u4e00\u7cfb\u5217\u901a\u7528\u7684 data model \u4ee5\u53ca\u67e5\u8be2\u4ed6\u4eec\u7684\u8bed\u8a00 (query language)  - relational model - document model - graph model</p> <p>chapter 3 \u4f1a\u8ba8\u8bba\u5982\u4f55\u5b58\u50a8\u8fd9\u4e9b\u6570\u636e\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u7b2c3\u70b9</p>"},{"location":"Chapter%202/#relational-model-vs-document-model","title":"Relational Model vs Document Model","text":"<p>\u8bb2\u5230 data model \u4e0d\u5f97\u4e0d\u63d0 relational model\uff0c \u662f Edgar Codd \u5728 1970 \u63d0\u51fa\u7684\u6982\u5ff5 </p> <p>data is organized into relations (called tables in SQL), where each relation is an unordered collection of tuples (rows in SQL).</p> <p>\u5f53\u65f6\u53ea\u662f\u4e00\u4e2a\u7406\u8bba\u4e0a\u7684\u6982\u5ff5\uff0c\u4e0d\u8fc7\u540e\u67651980\u5e74\u7684\u65f6\u5019 relational database management system (RDBMS) \u4ee5\u53ca SQL \u5df2\u7ecf\u662f\u5f00\u53d1\u8005\u4eec\u7684\u9ed8\u8ba4\u9009\u9879\u4e86</p> <p>relational database \u7684\u6839\u5728 business data processing \u4e5f\u5c31\u662f 1960-70\u5e74\u5927\u578b\u673a\u5728\u505a\u7684\u4e8b\u60c5 (\u94f6\u884c\u4ea4\u6613\uff0c\u822a\u7a7a\u516c\u53f8\u5b9a\u673a\u7968\uff0c\u4ed3\u50a8\u7ba1\u7406\u8fd9\u7c7b\u7684), \u4ee5\u53ca batch processing (customer invoicing, payroll, reporting).</p> <p>The goal of the relational model was to hide that implementation detail behind a cleaner interface.</p> <p>SQL \u5c31\u662f\u4e00\u5c42 abstraction\uff0c\u4f7f\u7528\u8005\u4e0d\u9700\u8981\u8003\u8651\u8f93\u5165 query \u4e4b\u540e database \u662f\u600e\u4e48\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u7684\uff0c\u5f53\u65f6\u5176\u4ed6\u7684database \u8fd8\u9700\u8981 developer \u6765\u8003\u8651\u5982\u4f55\u5b58/\u67e5\u8be2\u6570\u636e</p> <p>Object databases came and went again in the late 1980s and early 1990s. XML databases appeared in the early 2000s, but have only seen niche adoption. Each competitor to the relational model generated a lot of hype in its time, but it never lasted </p> <p>\u8fd9\u91cc\u7684reference \u8bb2\u4e86 data model \u7684\u5386\u53f2, \u5c31\u662f\u5728 noSQL \u4e4b\u524d\u5df2\u7ecf\u6709\u5f88\u591a\u4e0d\u540c\u79cd\u7c7b\u7684 data model \u51fa\u73b0\u8fc7\u4e86\uff0c\u4f46\u662f\u6700\u540e relational \u6d3b\u4e86\u4e0b\u6765\uff0c\u800c\u4e14\u73b0\u5728\u5927\u90e8\u5206\u7684\u4e92\u8054\u7f51app \u8fd8\u662f\u5728\u7528 relational model (online publishing, discussion, social networking, ecommerce, games, software-as-a-service productivity applications, or much more)</p>"},{"location":"Chapter%202/#birth-of-nosql","title":"Birth of NoSQL","text":"<p>\u8fd9\u4e00\u6bb5\u8bb2\u4e86 NoSQL \u7684\u5386\u53f2\uff0c\u4ed6\u5e76\u4e0d\u4ee3\u8868\u4e00\u4e2a\u5355\u72ec\u7684\u6280\u672f</p> <p>it was originally intended simply as a catchy Twitter hashtag for a meetup on open source, distributed, nonrelational databases in 2009</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u7406\u89e3\u6210 Not Only SQL </p> <p>Different applications have different requirements, and the best choice of technology for one use case may well be different from the best choice for another use case. It therefore seems likely that in the foreseeable future, relational databases will continue to be used alongside a broad variety of nonrelational datastores\u2014an idea that is sometimes called polyglot persistence [3]</p> <p>\u8fd9\u6982\u5ff5\u5728\u5f88\u591a\u5730\u65b9\u4e5f\u63d0\u5230\u8fc7\uff0c\u5c31\u662f\u4e00\u4e2a app \u5728\u7528\u4e0d\u540c\u7c7b\u578b\u7684 DB \u4e00\u8d77serve \u4e00\u4e2aservice\uff0c\u6bd4\u5982 redis \u6765\u505a cache\uff0c MySQL \u6765\u505a transaction, redshift/snowflake \u6765\u505a datalake  \u4e66\u91cc\u9762\u8fd9\u4e2a reference (Pramod J. Sadalage and Martin Fowler: NoSQL Distilled. Addison-Wesley) Martin Fowler \u5728\u5565\u90fd\u4f1a\uff1f\uff1fOOD \u4e5f\u662f\u4ed6</p>"},{"location":"Chapter%202/#object-relational-mismatch","title":"Object-Relational mismatch","text":"<p>\u56e0\u4e3a\u5927\u591a\u6570\u7684 language \u90fd\u7528 object oriented \u7684\u6a21\u5f0f\u6765\u5199\u7a0b\u5e8f\uff0c\u4f46\u60f3\u8981\u8f6c\u6362\u6210 relational model \u5c31\u9700\u8981 ORM \u7684\u5e2e\u52a9\u4e86 (\u56e0\u4e3a\u4f60\u9700\u8981\u628a object.update \u8f6c\u6362\u6210 SQL \u8bed\u53e5\uff0cORM \u5219\u907f\u514d\u4e86\u8fd9\u7c7b\u7684 boiler plate code)</p> <p>ActiveRecord \u6ca1\u542c\u8bf4\u8fc7\uff0cHibernate \u5e94\u8be5\u662f spring framework \u91cc\u9762\u7528\u5230\u7684 library </p> <p>\u4e66\u91cc\u9762\u7528\u5230\u4e86 LinkedIn profile \u6765\u4e3e\u4f8b\u5b50\uff0c\u6bcf\u4e2a\u4eba\u7684 profile \u5230\u53ef\u4ee5\u7528\u4e00\u4e2a unique identifier \u6765locate\uff0cuser_id. <code>first_name</code> and <code>last_name</code> \u5c31\u53ef\u4ee5\u5f53\u4f5c column \u653e\u8fdb user table \u91cc\u9762\u4e86 \u4f46\u662f\u6709\u4e9b\u5c5e\u6027\u4e0d\u50cf\u540d\u5b57\u4e00\u6837\u4e00\u822c\u53ea\u51fa\u73b0\u4e00\u6b21\uff0c\u6bd4\u5982\u4f60\u53c2\u52a0\u8fc7\u7684\u5de5\u4f5c\u5c97\u4f4d\uff0c\u8fd9\u79cd\u5c31\u4f1a\u6709\u591a\u4e2a instance \u4e86\u3002\u4ee5\u53ca\u4e0d\u540c\u5b66\u6821\u7684\u8bb0\u5f55\u3002\u8fd9\u7c7b one-to-many \u7684 relation \u901a\u5e38\u6709\u4ee5\u4e0b\u51e0\u79cd\u8868\u8fbe\u65b9\u5f0f - Put career positions, education, contact information into separate tables (\u4f20\u7edf\u65b9\u5f0f) - Add structured data type (XML, JSON) \u5728\u4e00\u4e2a column/attribute \u4e0a\u9762 \u8fd9\u6837 query \u7684\u65f6\u5019\u5c31\u53ef\u4ee5\u4e00\u6b21\u67e5\u8be2\u591a\u4e2a\u503c\u4e86 (MySQL, Oracle, DB2, SQL Server \u652f\u6301\u8fd9\u7c7b\u7684\u64cd\u4f5c) - Encode jobs, education, contact info as JSON or XML document and store it on a text column. \u7136\u540e\u5728 application \u5c42 parse </p> <p>relation row (userid, first name + last name, job)</p> <p>\u50cf resume \u8fd9\u79cd self-contained document\uff0cJSON \u8fd9\u79cd\u683c\u5f0f\u53ef\u80fd\u66f4\u5408\u9002\u3002MongoDB, RethinkDB, CouchDB, Espresso \u8fd9\u7c7b\u7684\u6570\u636e\u5e93\u5c31\u662f\u57fa\u4e8e JSON \u8fd9\u79cd\u683c\u5f0f\u6765\u5b58\u50a8\u6570\u636e\u7684 <pre><code>{  \n    \"user_id\": 251,  \n    \"first_name\": \"Bill\",  \n    \"last_name\": \"Gates\",  \n    \"summary\": \"Co-chair of the Bill &amp; Melinda Gates... Active blogger.\",  \n    \"region_id\": \"us:91\",  \n    \"industry_id\": 131,  \n    \"photo_url\": \"/p/7/000/253/05b/308dd6e.jpg\",  \n    \"positions\": [  \n        {  \n            \"job_title\": \"Co-chair\",  \n            \"organization\": \"Bill &amp; Melinda Gates Foundation\"  \n        },  \n        {  \n            \"job_title\": \"Co-founder, Chairman\",  \n            \"organization\": \"Microsoft\"  \n        }  \n    ],  \n    \"education\": [  \n        {  \n            \"school_name\": \"Harvard University\",  \n            \"start\": 1973,  \n            \"end\": 1975  \n        },  \n        {  \n            \"school_name\": \"Lakeside School, Seattle\",  \n            \"start\": null,  \n            \"end\": null  \n        }  \n    ],  \n    \"contact_info\": {  \n    \"blog\": \"http://thegatesnotes.com\",  \n    \"twitter\": \"http://twitter.com/BillGates\"  \n    }  \n}\n</code></pre></p> <p>JSON \u770b\u8d77\u6765\u51cf\u5c11\u4e86 application code \u8ddf storage layer \u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u4f46\u4e5f\u4e0d\u662f\u7edd\u5bf9\u7684(\u7b2c\u56db\u7ae0\u4f1a\u8bb2\u5230)</p> <p>JSON \u8fd9\u79cd\u683c\u5f0f\u76f8\u6bd4\u4e8e relational model \u6709\u66f4\u597d\u7684 locality\uff0c\u4ed6\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a tree structure\uff0c\u5c31\u597d\u50cf HTML \u6587\u4ef6\u4e00\u6837   \u76f8\u6bd4relational model \u7684 multiway join\uff0cJSON \u53ea\u9700\u8981 tree traversal \u5c31\u53ef\u4ee5\u4e86</p>"},{"location":"Chapter%202/#many-to-one-and-many-to-many-relationships","title":"Many-To-one and Many-to-many relationships","text":"<p>\u5728\u524d\u9762\u7684\u4f8b\u5b50\u91cc\u9762\uff0c<code>region_id</code> \u4ee5\u53ca <code>industry_id</code> \u90fd\u662f\u7528 Id \u800c\u4e0d\u662f\u6587\u5b57(\"\u5927\u897f\u96c5\u56fe\"\uff0c \u201c\u54f2\u5b66\u201d) \u6765\u8bb0\u5f55\u7684\u3002 \u4e3a\u4ec0\u4e48\uff1f</p> <p>\u5982\u679c\u7528\u6237\u754c\u9762\u91cc\u9762\u53ef\u4ee5\u81ea\u7531\u8f93\u5165\u6587\u5b57\uff0c\u90a3\u4e48\u76f4\u63a5\u7528\u6587\u5b57\u5b58\u50a8\u633a\u5408\u7406\u3002\u4f46\u7528 ID \u6709\u5176\u4ed6\u7684\u597d\u5904\uff0c - Consistent style and spelling across profiles - Avoiding ambiguity - Ease of updating (\u8fd9\u4e2a\u5730\u57df\u7684\u540d\u5b57\u53ea\u5b58\u4e00\u4e2a\u5730\u65b9\u5c31\u597d\u4e86\uff0c\u800c\u4e14\u672a\u6765\u66f4\u6539\u8d77\u6765\u5f88\u65b9\u4fbf) - Localization support \uff08\u7ffb\u8bd1\u7684\u65f6\u5019\u4e5f\u65b9\u4fbf\uff09 - Better search (Greater Seattle \u53ef\u4ee5\u5b58\u8fdb Washington state \u91cc\u9762)</p> <p>\u7528ID \u5b58\u50a8\u672c\u8d28\u4e0a\u4e5f\u662f\u907f\u514d\u91cd\u590d, \u5f53\u6211\u4eec\u5b58\u50a8\u7684\u65f6\u5019\u53bb\u6389\u91cd\u590d\u5185\u5bb9\u7684\u8fc7\u7a0b\u4e5f\u53eb\u505a normalization \u53bb\u91cd\u590d\u6709\u5f88\u591a\u597d\u5904, avoid write overheads, avoid inconsistencies etc</p> <p>Literature on the relational model distinguishes several different normal forms, but the distinctions are of little practical interest. As a rule of thumb, if you\u2019re duplicating values that could be stored in just one place, the schema is not normalized.</p> <p>\u4e0d\u5e78\u7684\u662f\uff0c normalization \u5c31\u9700\u8981 many to one relationships (many people live in one particular region, many people work in one particular industry)  document model \u5728 many to one \u8fd9\u79cdrelationship \u5c31\u4e0d\u662f\u5f88\u7075\u6d3b\u4e86</p> <p>In relational database, it's normal to refer to rows in other tables by IDs, because joins are easy In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak.iii</p> <p>\u4e0d\u8fc7\u73b0\u5728\u597d\u591a documentDB \u652f\u6301 join \u4e86 (MongoDB) \u5982\u679c\u4f60\u7528\u7684\u6570\u636e\u5e93\u81ea\u5df1\u4e0d\u652f\u6301 join\uff0c\u90a3\u4f60\u5c31\u8981\u5728 application \u5c42\u81ea\u5df1\u5b9e\u73b0join\u7684\u903b\u8f91\u3002\u5e76\u4e14\u968f\u7740 application\u7684\u8fdb\u5316\uff0c\u6570\u636e\u4e4b\u95f4\u7684 interconnection \u4e5f\u4f1a\u8d8a\u6765\u8d8a\u591a</p> <p>\u4f5c\u8005\u8fd9\u91cc\u4e3e\u4e86 resume \u7684\u4f8b\u5b50\uff0c\u5982\u679c organization \u4ee5\u53ca school \u4e5f\u8981\u5f53\u4f5c\u4e00\u4e2a entities\uff0c \u5f53\u5b83\u4eec\u81ea\u5df1\u6210\u4e86 entities\u7684\u65f6\u5019\uff0c\u5c31\u4e0d\u80fd\u50cf\u4e4b\u524d\u4e00\u6837\u53ea\u5b58 string \u4e86\uff0c organization, school could have its own web page (logo, news feed etc), \u540c\u6837\u7684\uff0crecommandation \u4e5f\u662f\u9700\u8981\u4e00\u4e2a\u7528\u6237\u7684 recommender \u80fd\u591f\u901a\u8fc7 ID \u627e\u5230 recommend \u4ed6\u7684\u4eba \u8fd9\u91cc\u5f3a\u8c03\u7684\u8fd8\u662f document \u4e00\u5f00\u59cb\u5728 resume \u8fd9\u91cc\u5f88\u9002\u7528\uff0c\u4f46\u662f\u4ea7\u54c1\u7684\u53d8\u5316\u53ef\u80fd\u4e5f\u4f1a\u6539\u53d8\u4f60 \u5bf9data model\u7684\u9009\u62e9 </p>"},{"location":"Chapter%202/#are-document-databases-repeating-history","title":"Are Document Databases Repeating history?","text":"<p>\u8fd9\u4e00\u5c0f\u8282\u7684\u5185\u5bb9\u4ecb\u7ecd\u4e86 database \u7684\u5386\u53f2\uff0c\u8fd8\u662f\u633a\u6709\u610f\u601d\u7684\uff0c\u53ef\u4ee5\u770b\u51fa\u5386\u53f2\u662f\u6709\u4e00\u5b9a\u7684\u91cd\u590d\uff0c\u4f46\u53c8\u4e0d\u662f\u7b80\u5355\u7684\u91cd\u590d</p> <p>\u8fd9\u91cc\u9700\u8981\u4ecb\u7ecd\u4e00\u4e0b relational model \u5f53\u521d\u5c31\u662f\u4e3a\u4e86\u89e3\u51b3 network model \u6ca1\u6cd5 abstract query\u7684\u95ee\u9898</p>"},{"location":"Chapter%202/#relational-vs-document-databases-today","title":"Relational vs Document Databases Today","text":"<p>\u5982\u679c\u8981\u5bf9 relational \u548c document \u6570\u636e\u5e93\u8fdb\u884c\u5bf9\u6bd4\u7684\u8bdd\u8fd8\u662f\u6709\u5f88\u591a\u4e0d\u540c\u7684\uff0c\u5305\u62ec fault-tolerance properties (chapter 5) \u4ee5\u53ca handling concurrency (chapter 7)  \u8fd9\u4e00\u7ae0\u96c6\u4e2d\u8ba8\u8bba difference in data model</p> <p>Main arguments in document data model are schema flexibility, better performance due to locality and closer to data structures used by some app (resume example) </p> <p>Relational model counters by providing better support for joins, many to one and many to many relationships</p>"},{"location":"Chapter%202/#which-data-model-leads-to-simpler-application-code","title":"Which data model leads to simpler application code?","text":"<p>If data in your app has tree structure (tree of one to many relationships, where typically entire tree is loaded at once), then it's probably a good idea to use a document model. </p> <p>the relational technique of shredding </p> <p>\u8fd9\u5176\u5b9e\u4e5f\u662f\u4e4b\u524d\u63d0\u5230\u7684 normalization\uff0c\u8fd9\u4e0d\u8fc7\u4f1a\u9020\u6210app \u5c42\u9762\u7684\u4ee3\u7801\u8981\u66f4\u590d\u6742\u4e00\u4e9b \u4f46 document model \u4e5f\u6709\u5c40\u9650\uff0c\u6bd4\u5982\u6ca1\u6cd5\u76f4\u63a5 reference \u5230 nested structure\uff0c\u800c\u9700\u8981 tree traversal </p> <p>The poor support for joins in document databases may or may not be a problem, depending on the application. For example, many-to-many relationships may never be needed in an analytics application that uses a document database to record which events occurred at which time</p> <p>\u4f46\u5982\u679c\u4f60\u7684app \u6709 many to many relationship, document \u5c31\u6ca1\u90a3\u4e48\u9999\u4e86</p> <p>Joins can be emulated in application code by making multiple requests to database, but that also moves complexity into the application and is usually slower than a join performed by specialized code inside the database.</p> <p>\u6709\u6839\u636eapp \u5c42\u9762\u7684\u9700\u6c42\u6765\u51b3\u5b9a\uff0c\u4e0d\u8fc7\u5927\u591a\u6570\u65f6\u5019 relational \u53ef\u80fd\u66f4\u5408\u9002\u2026\u2026 \u53ef\u80fd\u662f\u6211\u6ca1\u89c1\u8fc7 graph \u7684\u5e94\u7528\u573a\u666f\u5427</p> <p>For highly interconnected data, the document model is awkward, the relational model is acceptable, and graph models (see \u201cGraph-Like Data Models\u201d on page 49) are the most natural.</p>"},{"location":"Chapter%202/#schema-flexibility-in-the-document-model","title":"Schema flexibility in the document model","text":"<p>\u8fd9\u91cc\u5f3a\u8c03\u4e86 document databases \u5176\u5b9e\u662f schema on read instead of schemaless schema on read means data is only interpreted when it is read. </p> <p>In contrast, schema-on-write (relational databases) ensures all written data conforms to the schema <pre><code>INSERT user_id \"123\"\nSELECT * from user\nWHERE id = 123\n</code></pre></p> <p>schema on read \u5c31\u50cf dynamic type language (python, javascript) \u4e00\u6837\uff0c schema on write \u5219\u50cf\u662f static (compile time) checking language (c, c++, java)  \u5404\u6709\u4f18\u52bf\uff0c\u6ca1\u6709\u5bf9\u9519</p> <p>\u8fd9\u91cc\u4e3e\u4e86\u4e00\u4e2a\u9700\u8981\u6539 format \u7684\u4f8b\u5b50\uff0c\u6bd4\u5982\u4e4b\u524d\u4f60\u662f\u5b58fullname (last + first name), \u73b0\u5728\u5219\u662f\u8981\u5206\u5f00\u5b58\u3002\u90a3\u4e48 document database\u53ef\u4ee5\u76f4\u63a5\u5f00\u59cb\u5199\u4e00\u4e2a\u65b0\u7684 field \u7136\u540e\u5728 application \u5c42\u505a\u4e00\u4e2a\u5224\u65ad\u5c31\u597d\u4e86 <pre><code>if (user &amp;&amp; user.name &amp;&amp; !user.first_name) {\n    // old schema doesn't have `first_name` field\n    user.first_name = user.name.split(\" \")[0];\n}\n</code></pre></p> <p>\u800c \"statically typed\" schema \u5219\u9700\u8981\u505a\u4e00\u6b21 migration  <pre><code>ALTER TABLE users ADD COLUMN first_name text;  \nUPDATE users SET first_name = split_part(name, ' ', 1); PostgreSQL\nUPDATE users SET first_name = substring_index(name, ' ', 1); -- MySQL\n</code></pre> schema changes \u770b\u8d77\u6765\u53ef\u80fd\u9700\u8981 downtime \u4f46\u73b0\u5b9e\u4e2d\u53ef\u4ee5\u8981\u5f88\u591a\u65b9\u5f0f\u907f\u514d downtime\uff0c\u6bd4\u5982\u7528\u4e00\u4e2a replica continue to serve traffic\uff0c\u7136\u540e\u540e\u53f0\u505amigration\uff0c\u5b8c\u6210\u4e4b\u540e\u628a\u65b0\u7684 application \u4ee5\u53ca database \u4e00\u8d77 deploy \u5c31\u53ef\u4ee5\u4e86</p> <p>schema on read \u4e00\u822c\u662f\u7528\u6237\u53ef\u4ee5\u968f\u610f\u6dfb\u52a0\u65b0\u7684 field \u7684\u65f6\u5019\u6bd4\u8f83\u597d\u7528\uff0c\u800c schema on write \u5219\u662f\u5728\u4f60\u5df2\u7ecf\u77e5\u9053\u4e00\u4e9b field \u80af\u5b9a\u4f1a\u7528\u5230\u4ee5\u53ca\u4ed6\u4eec\u7684\u683c\u5f0f\u7684\u65f6\u5019\u66f4\u597d\u7528\u3002</p> <p>in cases where all records are expected to have the same structure, schemas are a useful mechanism for documenting and enforcing that structure. We will discuss schemas and schema evolution in more detail in Chapter 4.</p>"},{"location":"Chapter%202/#data-locality","title":"Data locality","text":"<p>document usually stored as a single string (JSON, XML). If your app require access to entire document (render it on web page), there is performance advantage. Because if data is split across multiple tables, additional lookups are required to retrieve it all, which may require more disk seeks and take more time</p> <p>\u4f46\u5982\u679c\u4f60\u53ea\u9700\u8981 document \u91cc\u9762\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u90a3\u4e48\u6bcf\u6b21\u5230 load \u6574\u4e2a document \u5c31\u4f1a\u9020\u6210\u6d6a\u8d39, \u800c\u4e14\u5728 update \u64cd\u4f5c\u7684\u65f6\u5019\uff0cdatabase \u4e00\u822c\u662f\u628a\u6574\u4e2a string \u91cd\u65b0\u5199\u5165\uff0c\u53ea\u6709\u5728\u4f60 document encoded size \u4e0d\u53d8\u7684\u65f6\u5019\u624d\u80fd\u505a\u5230 inplace modification \u8fd9\u4e9b\u60c5\u51b5\u4e5f\u9650\u5236\u4e86 document model \u7684\u4f7f\u7528\u573a\u666f</p> <p>data locality \u4e0d\u53ea\u662f document model \u8981\u8003\u8651\u7684\uff0crelational model \u4e5f\u540c\u6837\u9700\u8981\u3002\u751a\u81f3\u4f60\u5199\u4ee3\u7801\u7684\u65f6\u5019\u4e5f\u9700\u8981\u8003\u8651\u2026\u2026 \u7ecf\u5178 <code>for (i) { for (j) }</code> \u800c\u4e0d\u662f <code>for (j) {for (i)}</code></p> <p>It\u2019s worth pointing out that the idea of grouping related data together for locality is not limited to the document model. For example, Google\u2019s Spanner database offers the same locality properties in a relational data model, by allowing the schema to declare that a table\u2019s rows should be interleaved (nested) within a parent table [27]. Oracle allows the same, using a feature called multi-table index cluster tables [28]. The column-family concept in the Bigtable data model (used in Cassandra and HBase) has a similar purpose of managing locality [29].</p> <p>29 \u8fd9\u91cc\u7684 paper \u53ef\u4ee5\u770b\u4e00\u4e0b We will also see more on locality in Chapter 3.</p>"},{"location":"Chapter%202/#convergence-of-document-and-relational-databases","title":"Convergence of document and relational databases","text":"<p>\u505a database\u7684\u80af\u5b9a\u5e0c\u671b\u81ea\u5bb6\u7684\u4ea7\u54c1\u80fd\u591f\u63d0\u4f9b\u66f4\u591a\u7684\u529f\u80fd\uff0c\u6240\u4ee5 convergence \u5e94\u8be5\u662f\u5fc5\u7136\u4f1a\u53d1\u751f\u7684\u8d8b\u52bf</p> <p>Most relational database systems (other than MySQL) have supported XML since the mid-2000s. This includes functions to make local modifications to XML documents and the ability to index and query inside XML documents, which allows applications to use data models very similar to what they would do when using a document data\u2010 base.</p> <p>PostgreSQL, MySQL, IBM DB2 \u9875\u90fd\u652f\u6301 json. Document DB \u90a3\u8fb9\u4e5f\u662f\u9010\u6e10\u5f00\u59cb\u652f\u6301 join \u64cd\u4f5c\u4e86</p> <p>It seems that relational and document databases are becoming more similar over time, and that is a good thing: the data models complement each other. [^1]</p> <p>[^1] Codd \u5728\u4e00\u5f00\u59cb\u63d0\u51fa relational model \u7684\u65f6\u5019\u5176\u5b9e\u4e5f\u5df2\u7ecf\u63d0\u5230\u4e86 nested structure\u4e86</p> <p>Codd\u2019s original description of the relational model [1] actually allowed something quite similar to JSON documents within a relational schema. He called it nonsimple domains. The idea was that a value in a row doesn\u2019t have to just be a primitive datatype like a number or a string, but could also be a nested relation (table)\u2014so you can have an arbitrarily nested tree structure as a value, much like the JSON or XML support that was added to SQL over 30 years later.</p>"},{"location":"Chapter%202/#query-languages-for-data","title":"Query Languages for Data","text":"<p>\u5f53 relational model \u88ab\u63d0\u51fa\u7684\u65f6\u5019\uff0cSQL \u4e5f\u8ddf\u7740\u88ab\u8bbe\u8ba1\u51fa\u6765\u4e86\u3002 SQL \u5c31\u662f\u67e5\u8be2\u6570\u636e\u7528\u7684\u4e00\u79cd\u8bed\u8a00\uff0cand this language is a declarative query language. \u76f8\u5bf9\u5e94 IMS and CODASYL queried the database using imperative code. What does that mean?</p> <p>\u6211\u4eec\u5e73\u65f6\u7528\u5230\u7684 programming language \u5927\u90e8\u5206\u90fd\u662f imperative language. For example <pre><code>function getSharks() {\n    var sharks = [];\n    for (var i = 0; i &lt; animals.length; i++) { \n        if (animals[i].family === \"Sharks\") {\n            sharks.push(animals[i]);\n        }\n    }\n    return sharks;\n}\n</code></pre></p> <p>\u800c declarative language (functional programming) \u5219\u662f  <pre><code>sharks = animals.filter( animal =&gt; animal.family === \"Sharks\");\n</code></pre></p> <p>relational model \u662f\u57fa\u4e8e relational algebra \u5b9a\u4e49\u7684\uff0c $$ sharks = \\sigma_{family = Sharks} (animals) $$ \\(\\sigma\\) (sigma) \u662f select operator \u6839\u636e\u8fd9\u4e2a\u5b9a\u4e49\uff0c SQL \u5c31\u53ef\u4ee5\u7528 <pre><code>SELECT * FROM animals WHERE family = 'Sharks';\n</code></pre></p> <p>imperative \u5fc5\u987b\u6309\u7167\u7ed9\u5b9a\u7684\u6b65\u9aa4\u6267\u884c\uff0cdeclarative \u5219\u662f\u4f60\u7ed9\u51fa\u4f60\u60f3\u8981\u7684\u6761\u4ef6\uff0c\u4f46\u4e0d\u9700\u8981\u7ed9\u51fa \u5982\u4f55 \u5b9e\u73b0\u8fd9\u4e2a\u64cd\u4f5c\uff0c query \u4ea4\u7ed9 database \u53bb optimize\uff0cfunction \u4ea4\u7ed9 compiler \u53bb optimize </p> <p>A declarative query language is attractive because it is typically more concise and easier to work with than an imperative API. But more importantly, it also hides implementation details of the database engine, which makes it possible for the database system to introduce performance improvements without requiring any changes to queries.</p> <p>\u5176\u5b9e programming language \u4e5f\u662f\u4e00\u6837\u7684\uff0c\u8fd9\u4e9b for loop \u672c\u8eab\u5df2\u7ecf\u662f\u4e00\u5c42 abstraction\u4e86\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u8003\u8651 assembly code, \u4ea4\u7ed9 compiler \u5c31\u884c\u4e86\uff0cquery language \u4e5f\u662f\u4e00\u6837\u7684\uff0c\u4ea4\u7ed9 database query optimizer \u5c31\u53ef\u4ee5\u4e86</p> <p>The SQL example doesn\u2019t guarantee any particular ordering, and so it doesn\u2019t mind if the order changes. But if the query is written as imperative code, the database can never be sure whether the code is relying on the ordering or not. The fact that SQL is more limited in functionality gives the database much more room for automatic optimizations.</p> <p>\u8fd8\u6709\u4e00\u70b9\u597d\u5904\u5c31\u662f parallel execution </p> <p>Finally, declarative languages often lend themselves to parallel execution. Today, CPUs are getting faster by adding more cores, not by running at significantly higher clock speeds than before</p> <p>Declarative languages have a better chance of getting faster in parallel execution because they specify only the pattern of the results, not the algorithm that is used to determine the results. The database is free to use a parallel implementation of the query language, if appropriate 32 \u6709\u5174\u8da3\u7684\u8bdd\u8fd9\u4e2areference \u53ef\u4ee5\u9605\u8bfb</p>"},{"location":"Chapter%202/#declarative-queries-on-the-web","title":"Declarative queries on the web","text":"<p>Declarative queries \u4e0d\u6b62\u5b58\u5728\u4e8e database \u91cc\u9762\uff0c\u4e00\u4e2a\u6211\u4eec\u5929\u5929\u7528\u5230\u7684\u8f6f\u4ef6\u4e5f\u4e00\u6837\u6709 declarative query language: Browser </p> <p>CSS is a declarative language </p> <p>Imagine what life would be like if you had to use an imperative approach. In JavaScript, using the core Document Object Model (DOM) API, the result might look something like this: <pre><code>var liElements = document.getElementsByTagName(\"li\"); \nfor (var i = 0; i &lt; liElements.length; i++) {\n    if (liElements[i].className === \"selected\") { \n        var children = liElements[i].childNodes; \n        for (var j = 0; j &lt; children.length; j++) {\n            var child = children[j];  \n            if (child.nodeType === Node.ELEMENT_NODE &amp;&amp; child.tagName === \"P\") {\n                child.setAttribute(\"style\", \"background-color: blue\");\n            }\n\n        } \n    }\n}\n</code></pre> \u770b\u5f97\u51fa\u6765 declarative language \u7075\u6d3b\u5f88\u591a\uff0c\u800c\u4e14 imperative language \u8fd8\u6709\u4ee5\u4e0b\u51e0\u70b9\u95ee\u9898 - \u4e0a\u9762\u7684\u4ee3\u7801\u4f1a\u5bfc\u81f4\u7528\u6237\u5373\u4f7f\u9009\u4e86\u522b\u7684\u9875\u9762\u6216\u8005reload\u4e5f\u4e0d\u4f1a\u6e05\u7a7a selected (\u9700\u8981\u52a0\u5165 cursor \u7684\u7eaa\u5f55) CSS \u5219\u662f\u53ef\u4ee5\u8ba9 browser \u51b3\u5b9a - If you want to take advantage of a new API, such as <code>document.getElementsBy ClassName(\"selected\")</code> or even <code>document.evaluate()</code>\u2014which may improve performance\u2014you have to rewrite the code. On the other hand, browser vendors can improve the performance of CSS and XPath without breaking compatibility.</p> <p>database \u4e5f\u662f\u4e00\u6837\u7684\uff0c\u4f60\u4e0d\u7528\u8003\u8651 SQL \u80cc\u540e\u662f\u5982\u4f55\u4f18\u5316\u7684\uff0c\u4f60\u53ea\u8981\u77e5\u9053\u6bcf\u6b21\u8fd9\u4e2a SQL \u7684\u7ed3\u679c\u662f\u4e00\u6837\u7684\u5c31\u53ef\u4ee5\uff0c\u800c database vendor \u53ef\u4ee5\u968f\u4fbf\u8fdb\u884c\u540e\u53f0\u7684\u4f18\u5316</p>"},{"location":"Chapter%202/#mapreduce-querying","title":"MapReduce Querying","text":"<p>\u8fd9\u91cc\u7b80\u77ed\u4ecb\u7ecd\u4e86\u4e00\u4e0b MapReduce. MapReduce is a programming model for processing large amount of data from Google  \u5176\u5b9e\u53ea\u8981\u6d89\u53ca\u5230 large data \u603b\u4f1a\u63d0\u5230 MapReduce Chapter 10 \u4f1a\u8be6\u7ec6\u4ecb\u7ecd MapReduce\uff0c\u56e0\u4e3a derive data \u57fa\u672c\u90fd\u591a\u591a\u5c11\u5c11\u6536\u5230\u5b83\u7684\u5f71\u54cd</p> <p>MapReduce is neither a declarative query language nor a fully imperative query API, but somewhere in between: the logic of the query is expressed with snippets of code, which are called repeatedly by the processing framework.</p> <p>It is based on map(aka collect) and reduce(aka fold or inject) functions in functional programming language</p> <p>\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5047\u5982\u4f60\u662f\u4e00\u4e2a\u6d77\u6d0b\u751f\u7269\u5b66\u5bb6\uff0c\u4f60\u6bcf\u6b21\u89c2\u5bdf\u4e00\u4e2a\u6d77\u6d0b\u751f\u7269\u7684\u65f6\u5019\u90fd\u4f1a\u628a\u8fd9\u4e2a\u89c2\u5bdf\u8bb0\u5f55\u52a0\u5165\u5230\u4f60\u7684 database \u91cc\u9762\u3002\u73b0\u5728\u4f60\u60f3\u8981\u751f\u6210\u4e00\u4e2a\u6bcf\u6708\u89c2\u5bdf\u5230\u9ca8\u9c7c\u7684report <pre><code>SELECT date_trunc('month', observation_timestamp) AS observation_month, sum(num_animals) AS total_animals\n\nFROM observations  \nWHERE family = 'Sharks' GROUP BY observation_month;\n</code></pre> MongoDB's MapReduce \u4e5f\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u4e2arequest  \u5047\u5982\u6570\u636e\u662f\u8fd9\u6837\u7684 <pre><code>{        \n    observationTimestamp: Date.parse(\"Mon, 25 Dec 1995 12:34:56 GMT\"),\n    family:     \"Sharks\",\n    species:    \"Carcharodon carcharias\",\n    numAnimals: 3\n},\n{\n    observationTimestamp: Date.parse(\"Tue, 12 Dec 1995 16:17:18 GMT\"),\n    family:     \"Sharks\",\n    species:    \"Carcharias taurus\",\n    numAnimals: 4\n}\n</code></pre></p> <p>then <code>map</code> will be called once for each document resulting <pre><code>emit(\"1995-12\", 3)\nemit(\"1995-12\", 4)\n</code></pre> then <code>reduce</code> function will be called with  <code>reduce(\"1995-12\", [3,4])</code></p> <p><code>map</code> and <code>reduce</code> function must be pure function, meaning no side effects. This allow these function to be run anywhere and in any order and rerun them on failure </p> <p>MapReduce is just a way of generating derived data (part 3). Higher level query languages like SQL can be implemented as a pipeline of MapReduce operations (Chapter 10) </p> <p>There are many implementations of SQL that don't use MapReduce (in fact MapReduce is probably outdated now, Spark and other framework are being used) </p>"},{"location":"Chapter%202/#graph-like-data-models","title":"Graph like Data models","text":"<p>\u5176\u5b9e relational \u4e5f\u53ef\u4ee5 represent graph\uff0c\u53ea\u4e0d\u8fc7\u4e0d\u662f\u7279\u522b\u65b9\u4fbf\u8ba9\u4eba\u5728\u8111\u5b50\u91cc\u5f62\u6210\u4e00\u4e2a\u753b\u9762 \u6216\u8005\u8bf4\u5f53\u4f60\u7684 application many to many relationships \u53d8\u5f97\u5f88\u590d\u6742\u7684\u65f6\u5019\uff0cgraph model is easier to reason about </p> <p>graph consists of 2 kinds of object: vertices (aka nodes or entities) and edges (aka relationships or arcs) Many type of data can be model as a graph - Social graph - Web graph - Road rail networks (or just networks)</p> <p>Well-known algorithms can operate on these graphs: for example, car navigation sys\u2010 tems search for the shortest path between two points in a road network, and PageRank can be used on the web graph to determine the popularity of a web page and thus its ranking in search results.</p> <p>bfs,dfs, disjktra, min span tree etc</p> <p>graphs are not limited to such homogeneous data: an equally powerful use of graphs is to provide a consistent way of storing completely different types of objects in a single datastore.</p> <p>Facebook maintains a single graph with many different types of vertices and edges: vertices represent people, locations, events, checkins, and comments made by users; edges indicate which people are friends with each other, which checkin happened in which location, who commented on which post, who attended which event, and so on 35</p> <p></p> <p>This section talked about property graph model (Neo4j, Titan, InfiniteGraph) and triple store model (Datomic, AllegroGraph, etc) and 3 declarative query languages for graphs: Cypher, SPARQL, and Datalog.</p>"},{"location":"Chapter%202/#property-graphs","title":"Property Graphs","text":"<p>vertex (nodes) consists of - Unique ID - A set of outgoing edges - A set of incoming edges - A collection of properties (kv pairs) Each edge consists of - Unique ID - Vertex(node) at which the edge starts - vertex at which the edge ends - A label describe the relationship between two nodes - A collection of properties (kv pair) relational model representation of graph <pre><code>CREATE TABLE vertices (  \n    vertex_id integer PRIMARYKEY, \n    properties json\n);\n\nCREATE TABLE edges (  \n    edge_id integer PRIMARY KEY,  \n    tail_vertex integer REFERENCES vertices (vertex_id), \n    head_vertex integer REFERENCES vertices (vertex_id), \n    label text,  \n    properties json    \n);  \n\nCREATE INDEX edges_tails ON edges (tail_vertex);\n\nCREATE INDEX edges_heads ON edges (head_vertex);\n</code></pre> or just a map in programming language <pre><code>Map&lt;ID, (Node, properties)&gt; nodes;\nMap&lt;ID, (startNodeID, endNodeID, label, properties)&gt; edges;\n</code></pre> Aspect of graph model  1. No schema that restricts which kinds of things can or cannot be associated. (person can be connected to city or event or whatever) 2. Given any vertex, you can efficiently find both its incoming and its outgoing edges, and thus traverse the graph\u2014i.e., follow a path through a chain of vertices \u2014both forward and backward. 3. By using different labels for different kinds of relationships, you can store several different kinds of information in a single graph, while still maintaining a clean data model.</p> <p>Those features give graphs a great deal of flexibility for data modeling,</p> <p>just more flexible. \u56e0\u4e3a\u5982\u679c\u7528 relational model\uff0c\u4e0d\u540c\u56fd\u5bb6\u6709\u4e0d\u540c\u7684\u89c4\u5b9a\uff0c\u6bd4\u5982\u7f8e\u56fd\u662f\u5dde\u548c\u53bf\uff0c\u4e2d\u56fd\u662f\u7701\u548c\u5e02 \u7528 graph  \u7684\u8bdd\u66f4\u65b9\u4fbf\u6269\u5c55\uff0c</p> <p>For instance, you could use it to indicate any food allergies they have (by introducing a vertex for each allergen, and an edge between a person and an allergen to indicate an allergy), and link the allergens with a set of vertices that show which foods contain which substances.</p> <p></p>"},{"location":"Chapter%202/#the-cypher-query-language","title":"The Cypher query language","text":"<p>Cypher \u662f\u4e00\u79cd graph query language (created for Neo4j) graph database syntax: <code>(Idaho) -[:WITHIN]-&gt; (USA)</code> creates an edge labeled <code>WITHIN</code> with <code>Idaho</code> as start node and <code>USA</code> as end node</p> <p>example query  <pre><code>CREATE\n  (NAmerica:Location {name:'North America', type:'continent'}),\n  (USA:Location      {name:'United States', type:'country'  }),\n  (Idaho:Location    {name:'Idaho',         type:'state'    }),\n  (Lucy:Person       {name:'Lucy' }),\n  (Idaho) -[:WITHIN]-&gt;  (USA)  -[:WITHIN]-&gt; (NAmerica),\n  (Lucy)  -[:BORN_IN]-&gt; (Idaho)\n</code></pre></p> <p>with data created, we can start asking interesting questions  - find the names of all the people who emigrated from the United States to Europe to be more precise, find all vertices that have a <code>BORN_IN</code> edge to <code>US</code> and also <code>LIVING_IN</code> edge to <code>Europe</code> and return the <code>name</code> property of each vertices  <pre><code>MATCH\n    (person) -[:BORN_IN]-&gt;  () -[:WITHIN*0..]-&gt; (us:Location {name:'United States'}),\n    (person) -[:LIVES_IN]-&gt; () -[:WITHIN*0..]-&gt; (eu:Location {name:'Europe'}) \nRETURN person.name\n</code></pre></p> <p>Find any vertex (call it person) that meets both of the following conditions:</p> <ol> <li> <p>person has an outgoing BORN_IN edge to some vertex. From that vertex, you can follow a chain of outgoing WITHIN edges until eventually you reach a vertex of type Location, whose name property is equal to \"United States\".</p> </li> <li> <p>That same person vertex also has an outgoing LIVES_IN edge. Following that edge, and then a chain of outgoing WITHIN edges, you eventually reach a vertex of type Location, whose name property is equal to \"Europe\". \u53cd\u8fc7\u6765\u4e5f\u53ef\u4ee5</p> <p>But equivalently, you could start with the two Location vertices and work backward. If there is an index on the name property, you can probably efficiently find the two vertices representing the US and Europe. Then you can proceed to find all locations (states, regions, cities, etc.) in the US and Europe respectively by following all incom\u2010 ing WITHIN edges. Finally, you can look for people who can be found through an incoming BORN_IN or LIVES_IN edge at one of the location vertices.</p> </li> </ol> <p>\u8fd9\u4e9b\u7ec6\u8282\u5c31\u7531 query optimizer \u6765\u5904\u7406\u4e86</p>"},{"location":"Chapter%202/#graph-queries-in-sql","title":"Graph queries in SQL","text":"<p>\u76f8\u5bf9\u4e8e<code>() -[:WITHIN*0..]-&gt; ()</code> \u8fd9\u79cdexpression, SQL \u5c31\u4f1a\u590d\u6742\u4e00\u4e9b <pre><code>WITH RECURSIVE\n  -- in_usa is the set of vertex IDs of all locations within the United States\nin_usa(vertex_id) AS (  \n    SELECT vertex_id FROM vertices WHERE properties-&gt;&gt;'name' = 'United States'\n\n    UNION  \n    SELECT edges.tail_vertex FROM edges\n\n    JOIN in_usa ON edges.head_vertex = in_usa.vertex_id\n\n    WHERE edges.label = 'within' \n),\n  -- in_europe is the set of vertex IDs of all locations within Europe\nin_europe(vertex_id) AS (  \n    SELECT vertex_id FROM vertices WHERE properties-&gt;&gt;'name' = 'Europe'\n\n    UNION  \n    SELECT edges.tail_vertex FROM edges\n\n    JOIN in_europe ON edges.head_vertex = in_europe.vertex_id\n\n    WHERE edges.label = 'within' \n),\n...\n</code></pre></p>"},{"location":"Chapter%202/#triple-store-and-sparql","title":"Triple-Store and SPARQL","text":"<p>triple store model is mostly equivalent to property graph model </p> <p>It is nevertheless worth discussing, because there are various tools and languages for triple-stores that can be valuable additions to your toolbox for building applications.</p> <p>\u5728 triple-store, data is stored in forms of 3 parts statement <code>(subject, predicate, object)</code> for example, <code>(Jim, like, bananas)</code></p> <p>The subject is equivalent to vertex in a graph. object can be 1 of 2 things 1. A value in primitive datatype (string or number). In this case, the predicate and object are equivalent to key and value of a property. For example, <code>(lucy, age, 33)</code> is like a node <code>lucy</code> with properties <code>{\"age\": 33}</code>  2. Another node in the graph. In this case, the predicate is an edge in the graph. subject will be the starting node, object will be the end node. For example, <code>(lucy, marriedTo, alain)</code> subject (lucy) and object (alain) are both vertices, and the predicate <code>marriedTo</code> is the label of the edge in this graph</p> <p></p> <p>\u5177\u4f53\u4f8b\u5b50 <pre><code>@prefix : &lt;urn:example:&gt;.\n_:lucy     a       :Person.\n_:lucy     :name   \"Lucy\".\n_:lucy     :bornIn _:idaho.\n</code></pre></p> <p><pre><code>@prefix : &lt;urn:example:&gt;.\n\n_:lucy a :Person;   :name \"Lucy\";          :bornIn _:idaho.\n</code></pre> When the predicate is a property, the object is a string literal, as in <code>_:usa :name \"United States\".</code> otherwise its an edge</p>"},{"location":"Chapter%202/#the-semantic-web","title":"The semantic web","text":"<p>\u8fd9\u91cc\u4e5f\u662f\u4e00\u6bb5\u5386\u53f2\uff0c Resource Description Framework \u66fe\u88ab\u63d0\u51fa\u7528\u6765 publish data \u867d\u7136\u57282000\u521d overhyped \u4f46\u540e\u6765\u6ca1\u6709\u5b9e\u9645\u5e94\u7528 <pre><code>&lt;rdf:RDF xmlns=\"urn:example:\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"&gt;\n    &lt;Location rdf:nodeID=\"idaho\"&gt; \n        &lt;name&gt;Idaho&lt;/name&gt; \n        &lt;type&gt;state&lt;/type&gt; \n        &lt;within&gt;\n            &lt;Location rdf:nodeID=\"usa\"&gt; \n                &lt;name&gt;United States&lt;/name&gt; \n                &lt;type&gt;country&lt;/type&gt; \n                    &lt;within&gt;\n                        &lt;Location rdf:nodeID=\"namerica\"&gt; \n                            &lt;name&gt;North America&lt;/name&gt; \n                            &lt;type&gt;continent&lt;/type&gt;\n                        &lt;/Location&gt;\n                &lt;/within&gt;\n            &lt;/Location&gt;\n        &lt;/within&gt;\n    &lt;/Location&gt;\n    &lt;Person rdf:nodeID=\"lucy\"&gt; \n        &lt;name&gt;Lucy&lt;/name&gt;  \n        &lt;bornIn rdf:nodeID=\"idaho\"/&gt;\n    &lt;/Person&gt;\n&lt;/rdf:RDF&gt;\n</code></pre></p> <p>The URL http://my-company.com/namespace doesn\u2019t necessarily need to resolve to anything\u2014from RDF\u2019s point of view, it is simply a namespace. To avoid potential confusion with http:// URLs, the examples in this section use non-resolvable URIs such as urn:example:within. Fortunately, you can just specify this prefix once at the top of the file, and then forget about it.</p> <p>ant build tool \u91cc\u9762\u7684 xml \u5e94\u8be5\u4e5f\u662f\u4e00\u4e2a\u539f\u7406\uff0c\u5c31\u662f\u901a\u8fc7 namespace \u6765avoid name collision</p>"},{"location":"Chapter%202/#sparql-query-language","title":"SPARQL query language","text":"<p>SPARQL is a query language for triple-stores using the RDF data model <pre><code>PREFIX : &lt;urn:example:&gt;\n\nSELECT ?personName WHERE {\n  ?person :name ?personName.\n  ?person :bornIn / :within* / :name \"United States\".\n  ?person :livesIn / :within* / :name \"Europe\".\n\n}\n</code></pre> \u611f\u89c9\u8fd9\u4e2a\u592a\u504f\u95e8\u4e86\uff0c\u6ca1\u4eba\u7528\uff0c\u8fd8\u4e0d\u5982\u8bb2\u8bb2 graphql... <pre><code>(person) -[:BORN_IN]-&gt; () -[:WITHIN*0..]-&gt; (location)   # Cypher\n</code></pre> <pre><code>?person :bornIn / :within* ?location.                   # SPARQL\n</code></pre></p>"},{"location":"Chapter%202/#the-foundation-datalog","title":"The Foundation: Datalog","text":"<p>Datalog \u662f\u66f4\u8001\u7684query language and it is being studies extensively in 1980s </p> <p>In practice, Datalog is used in a few data systems: for example, it is the query lan\u2010 guage of Datomic [40], and Cascalog [47] is a Datalog implementation for querying large datasets in Hadoop.</p> <p>Datalog\u2019s data model is similar to the triple-store model, generalized a bit. Instead of writing a triple as (subject, predicate, object), we write it as predicate(subject, object). <pre><code>name(usa, 'United States').\ntype(usa, country).\nwithin(usa, namerica).\n</code></pre> \u4e66\u91cc\u7ed9\u51fa\u4e86\u4e00\u4e9b query \u4f8b\u5b50\u7684\u5177\u4f53\u7ec6\u8282, \u4f46\u6211\u611f\u89c9\u5e73\u65f6\u7528\u4e0d\u5230\uff0c\u53ef\u4ee5\u8df3\u8fc7 <pre><code>within_recursive(Location, Name) :- name(Location, Name). /* Rule 1 */\n\nwithin_recursive(Location, Name) :- within(Location, Via), /* Rule 2 */ within_recursive(Via, Name).\n...\n</code></pre></p>"},{"location":"Chapter%202/#summary","title":"Summary","text":"<p>this chapter talked about different data model, although is an overview\uff0cit is enough for us to understand the history of data model and where they fits the best</p> <p>Historically, data start out as one big tree (hierarchical model) but not good for many to many relationships</p> <p>Relational model was invented to solve this problem. More recently (2010 ish), non relational \"NoSQL\" datastore have gone to two direction  1. Document databases self contained and relationship with other document are rare (another tree structure) 2. Graph databases is opposite where anything could be related to everything</p> <p>All three models (document, relational, and graph) are widely used today, and each is good in its respective domain.</p> <p>One model can be emulated in terms of another model \u2014for example, graph data can be represented in a relational database\u2014but the result is often awkward. That\u2019s why we have different systems for different purposes, not a single one-size-fits-all solution.</p> <p>there are other data models left unmentioned. few examples: - Genome data (similarity search in large string) - Particle data (Large Hadron Collider) hundres of petabytes of data  - Full text search (Information retrieval)</p>"},{"location":"Chapter%203/","title":"Chapter 3","text":"<p>Chapter 2 is written in programmers perspective about data model This chapter will discuss the same from database's point of view: how we can store the data that we're given, and how we can find it again when we're asked for it</p> <p>First, why we care how the database handles storage and retrieval internally? </p> <p>Although we don't need to write our own storage engine, we do need to choose a storage engine that is appropriate for our application </p> <p>Two families of storage engines are discussed: log-structured storage engines, and page-oriented storage engines such as B-trees</p>"},{"location":"Chapter%203/#data-structures-that-power-your-database","title":"Data structures that power your database","text":"<p>simplest kv pair database <pre><code>#!/bin/bash\ndb_set () {\n    echo \"$1,$2\" &gt;&gt; database\n}\ndb_get () {\n    grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1\n}\n</code></pre></p> <p><code>db_set</code> has actually really good performance \\(O(1)\\) because it is append only at the end of the file. But <code>db_get</code> has \\(O(n)\\) because it needs to look up the entire database for a key </p> <p>\u7edd\u5927\u591a\u6570 database \u4ee5\u53ca OS \u90fd\u6709\u4e00\u4e2a append only log (\u4e5f\u53eb write ahead log), \u5c31\u662f\u7c7b\u4f3c\u4e8e\u8fd9\u4e2a append only file. \u53ea\u4e0d\u8fc7 DBMS \u6216\u8005 OS \u8981\u4fdd\u8bc1\u5b9a\u671f\u6e05\u7406\u8fd9\u4e2alog \u4ee5\u907f\u514d\u4e0d\u65ad\u589e\u957f\u3002distributed system \u5176\u5b9e\u4e5f\u662f\u57fa\u4e8e\u4e00\u4e2alog \u6765\u7684\uff0c\u611f\u5174\u8da3\u8bf7\u770bthe log \u8fd9\u7bc7\u6587\u7ae0</p> <p>In order to efficiently find the value for a particular key in the database, we need a different data structure: an index.</p> <p>\u8fd9\u91cc\u5c31\u8981\u5f15\u51fa index \u7684\u6982\u5ff5\u4e86\u3002This chapter will look at a range of indexing structures and how they compare </p> <p>The general idea behind them is to keep some additional metadata on the side</p> <p>\u5c31\u662f\u901a\u8fc7 meta data (pointer etc) \u6765\u5e2e\u52a9 locate the data </p> <p>An index is an additional structure that is derived from the primary data</p> <p>\u7d22\u5f15\u662f\u7531\u4e3b\u8981\u6570\u636e\u884d\u751f\u51fa\u6765\u7684\u6570\u636e, \u5c31\u50cf\u6bcf\u672c\u4e66\u540e\u9762\u7684\u7d22\u5f15\u4e00\u6837\uff0c\u7528\u6765\u5e2e\u52a9 locate data  \u7ef4\u62a4\u4e00\u4e2aadditional structure \u5c31\u4f1a\u6709 overhead\uff0cespecially on writes because index needs to be updated every time data is written</p> <p>\u6240\u4ee5 database default \u662f\u4e0d\u4f1a\u7ed9\u6240\u6709\u6570\u636e\u505a index \u7684\uff0c\u800c\u662f application developer \u6765\u9009\u62e9\u54ea\u4e9b\u6570\u636e\u9700\u8981 index </p>"},{"location":"Chapter%203/#hash-indexes","title":"Hash Indexes","text":"<p>Hash index \u4f30\u8ba1\u662f\u6700\u5e38\u89c1\u7684 index \u5f62\u5f0f\u4e86\uff0cjava \u7684 <code>Map</code>, python \u7684 <code>dict</code> \u4ee5\u53ca json \u90fd\u662fkey-value pair </p> <p>\u65e2\u7136\u5df2\u7ecf\u6709 in memory data structure \u4e86\uff0con disk \u4e5f\u53ef\u4ee5\u7528 hash \u6765 index  \u52a0\u5165\u6211\u4eec\u6709\u4e00\u4e2a append only file, \u6700\u7b80\u5355\u7684\u4e00\u4e2a indexing strategy \u5c31\u662f\u5728 in memory hash map \u91cc\u9762\u5b58\u4e00\u4e2a\u4f60\u8981\u67e5\u627e\u7684 key\uff0c value \u5219\u662f byte offset for the append only file, \u60f3\u8981\u67e5\u54ea\u4e2a\u503c\uff0c\u76f4\u63a5\u53bb\u5bf9\u5e94\u7684file \u7684 offset\u91cc\u9762\u8bfb\u53d6\u5c31\u53ef\u4ee5\u4e86</p> <p></p> <p>This may sound simplistic, but it is a viable approach. In fact, this is essentially what Bitcask (the default storage engine in Riak) does</p> <p>\u60f3\u7b97\u4e00\u4e2aurl \u7684\u70b9\u51fb\u6570\u91cf\u8fd9\u79cd workload \u5c31\u5f88\u9002\u5408 Bitcask \u8fd9\u79cd storage engine (a lot of writes, not too many distinct keys (only urls)) in other words, large number of writes per key, and keys are small enough to fit in memory</p>"},{"location":"Chapter%203/#how-to-avoid-running-out-of-disk-space","title":"How to avoid running out of disk space?","text":"<p>break the log into segments and when a segments reach to a certain size (lets say 5mb) making subsequent write to a new segment file and perform compaction on these segments.   \u8fd9\u91cc\u7684compaction \u5c31\u76f4\u63a5\u628a\u6700\u65b0\u7684\u503c\u8bb0\u5f55\u4e0b\u6765\uff0c\u800c\u4e14\u53ef\u4ee5\u5728 background thread \u8fdb\u884c</p> <p>while it is going on, we can still continue to serve read and write requests as normal, using the old segment files. After the merging process is complete, we switch read requests to using the new merged segment instead of the old segments\u2014and then the old segment files can simply be deleted.</p> <p> \u6bcf\u4e00\u4e2a segment \u5728 memory\u91cc\u9762\u90fd\u6709\u81ea\u5df1\u7684 hash table, mapping keys to file offset. In order to find value for a key, we first check the most recent segment hash map; if the key is not present we check the second most and so on <pre><code>Map&lt;key, data&gt; seg1;\nMap&lt;key, data&gt; seg2; # &lt;- check this first \n...\n</code></pre> The merging process keeps the number of segments small </p> <p>There is a lot of details in implementation:</p> <p>File format CSV is not best format. binary is faster and simpler Deleting records Append a special deletion record to the data file (sometime called a tombstone) when log segments are merged, tombstone tells the merging process to ignore any previous value for deleted key Crash recovery If database is restarted, in memory hash maps are lost. In principle you can restore it by reading the entire segment file. but it takes too long if segment file gets large. Bitcask speeds up recovery by storing snapshot of each segment's hashmap on disk  Partially written records database may crash at any time, including halfway through appending to the log. Bitcask files include checksum, allowing such corrupted parts to be detected and ignored Concurrency control Since it is append only, a common implementation is to have 1 writer thread. And many reader threads.</p> <p>Compare to update file in place, append only design has following advantages: - Sequential writes are generally much faster than random writes. Especially on spinning disk. To some extent sequential writes are preferable in SSD 4 - Concurrency and crash recovery are much simpler if segment files are appendonly or immutable. For example, you don\u2019t have to worry about the case where a crash happened while a value was being overwritten, leaving you with a file containing part of the old and part of the new value spliced together. - Merging old segments avoids the problem of data files getting fragmented over time.</p> <p>But it also has limitations: - must fit in memory - range queries are not efficient</p>"},{"location":"Chapter%203/#sstables-and-lsm-trees-log-structure","title":"SSTables and LSM-Trees (log structure)","text":"<p>SSTable \u5c31\u662f\u5728\u4e4b\u524d append only segments\u7684\u57fa\u7840\u4e0a\u8ba9 key sorted and unique\u3002\u6240\u4ee5\u53eb\u505a Sorted String Table (SSTable for short)  SSTables \u76f8\u6bd4\u4e8e hash indexed log segment \u6709\u51e0\u4e2a\u597d\u5904 1. Merging segments is simple and efficient (similar to mergesort)  if one key appears more than 1 segment, keep the most recent one 2. No longer need to keep index of all the keys in memory.  Just like binary search tree, you could get the floor key of current key your are looking for. Then get into the segment you need.  \\(handb &lt; handi &lt; hands\\)  This way the index is sparse which allow us to store more keys (each segment can be few kilobytes so that it can be scanned quickly). Just like page fault in memory, it will also go to disk to bring relevant file content in memory 3. Compress a segment into a block before write to disk and save I/O</p>"},{"location":"Chapter%203/#constructing-and-maintaining-sstables","title":"Constructing and maintaining SSTables","text":"<p>How do you get your data to be sorted by key? In memory we have well known data structure  - BST - red black tree  - AVL trees We can now make our storage engine work as follows: - When write comes in, add it to our in memory balanced tree data structure (red black tree). Those in memory tree is sometimes called memtable - when the in memory tree gets bigger than some threshold (few megabytes), write it out to disk as an SSTable file. This file will become the most recent segment of the database. While SSTable is being written out to disk, writes can continue to a new memtable instance - In order to serve a read request, first try in memory tree. If key is not found, then most recent on-disk segment, then older etc. - From time to time, run merge and compaction process in the background to combine segment files and discard the overwritten and deleted value</p> <p>This technique works well except when database crashes. The most recent writes are lost (in memory). We can keep a separate log on disk to avoid this problem. This log is append only and does not require key to be sorted because it is for recovery only. We could discard the log once the in memory tree is written to disk</p>"},{"location":"Chapter%203/#making-an-lsm-tree-out-of-sstables","title":"Making an LSM-tree out of SSTables","text":"<p>LevelDB and RocksDB essentially use the technique above to build their storage engine.  key-value engine libraries that are design to be embedded into other applications.  LevelDB can be used in Riak as an alternative to Bitcask. Similar storage engine are used in Cassandra and HBase 8, which are inspired by BigTable </p> <p>Log-Structured merge tree (LSM tree) originated from this paper. Storage engines based on this principle of merging and compacting sorted files are called LSM storage engines.</p> <p>Lucene(indexing engine for Elaticsearch and Solr) uses a similar method for storing its term dictionary, where key is the term (word) and the value is the list of IDs of all the documents (posting list) that contain the word </p> <p>In Lucene, this this mapping from term to posting list is kept in SSTable-like sorted files</p>"},{"location":"Chapter%203/#performance-optimizations","title":"Performance optimizations","text":"<p>A lot of details when implementation happens. LSM-tree algorithm can be slow when looking up keys that do not exist. (check in memory tree, check all segment files etc) </p> <p>In order to optimize this, storage engine often use Bloom filters, which tells you if a key does not appear in the database, and thus saves many unnecessary disk reads for nonexistent keys.</p> <p>There are also different strategies of how SSTables are compact and merged.</p> <p>The most common options are size-tiered and leveled compaction.</p> <p>In sized-tiered compaction, newer and smaller SSTables are successively merged into older and larger SSTables. </p> <p>In leveled compaction, the key range is split up into smaller SSTables and older data is moved into separate \"levels\", which allows the compaction to proceed more incrementally and use less disk space.</p> <p></p> <p>the basic idea of LSM-trees\u2014keeping a cascade of SSTables that are merged in the background\u2014is simple and effective. Even when the dataset is much bigger than the available memory it continues to work well. Since data is stored in sorted order, you can efficiently perform range queries (scanning all keys above some minimum and up to some maximum), and because the disk writes are sequential the LSM-tree can support remarkably high write throughput.</p>"},{"location":"Chapter%203/#b-trees-page-structure","title":"B-Trees (page structure)","text":"<p>Most common type of index structure. </p> <p>Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key- value lookups and range queries. But that\u2019s where the similarity ends: B-trees have a very different design philosophy.</p> <p>Segment \u901a\u5e38\u662f\u7531\u4e00\u4e2a\u4e2a variable size segment \u7ec4\u6210\u7684\u3002 \u4e0e\u4e4b\u5bf9\u5e94\u7684 btree \u5219\u662f\u7528 fixed-size blocks or pages (\u4e3a\u4e86\u66f4\u597d\u7684\u8ddf disk \u4e0a\u9762\u7684 blocks/page \u5bf9\u5e94\u8d77\u6765) normally in 4 kb  and read or write one page at a time</p> <p>memory \u4e5f\u662f\u7531 page \u7ec4\u6210\u7684\uff0c\u6240\u4ee5 btree \u53ef\u4ee5\u628a memory page \u5199\u5165 disk\uff0c\u6216\u8005\u628a disk page bring \u8fdb memory \u91cc\u9762</p> <p>\u6bcf\u4e00\u4e2a page \u53ef\u4ee5\u7531 id \u6216\u8005 address \u6765\u8bfb\u53d6\uff0c\u8fd9\u5c31\u53ef\u4ee5\u8ba9 btree \u5728 disk \u4e0a\u9762\u7528\u6307\u9488\u7684\u65b9\u5f0f\u4ece\u4e00\u4e2a page \u6307\u5411\u4e00\u4e2a \u53e6\u4e00\u4e2a page, \u5e76\u7528 tree \u7684\u5f62\u5f0f\u8868\u8fbe\u51fa\u6765 </p> <p>\u53ea\u8981\u4e0d\u662f leaf node, \u6bcf\u4e00\u4e2a page \u90fd\u8d1f\u8d23 continuous range of keys, \u4ece root \u5f00\u59cb\uff0c\u627e\u5230\u4f60\u5f53\u524d key \u6240\u5728\u7684\u533a\u95f4\uff0c\u7136\u540e\u8fdb\u5230\u4e0b\u4e00\u4e2a child page\uff0c\u76f4\u5230 leaf node \u4e3a\u6b62</p> <p>b-tree \u91cc\u9762\u6bcf\u4e00\u4e2a page \u80fd\u5305\u542b\u591a\u5c11\u4e2a child reference \u5c31\u662f branching factor\uff0c\u6bd4\u5982\u4e0a\u9762\u8fd9\u5f20\u56fe\u7684 branching factor \u662f6</p> <p>In practice, the branching factor depends on the amount of space required to store the page references and the range boundaries, but typically it is several hundred.</p> <p>for updating value in b-tree, you search for leaf node/page that contains current key and change the value in that page, and write the page back to disk. If you want to add new key, you perform same search for the leaf page. Add new key and write to disk</p> <p>If there isn't enough free space, it split into half-full pages, and parent page is updated to account for the new subdivision of key ranges </p> <p></p> <p>This algorithm guarantees tree remains balanced, that is, b-tree with n keys always has depth of \\(O(log\\ n)\\)</p> <p>Most databases can fit into a B-tree that is three or four levels deep, so you don\u2019t need to follow many page references to find the page you are look\u2010 ing for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.)</p>"},{"location":"Chapter%203/#making-btree-reliable","title":"Making Btree reliable","text":"<p>Basic write operation is to overwrite page on disk with new data (assumed overwrite does not change the location of the page) </p> <p>\u5c31\u662f\u5728\u5199\u5165\u7684\u65f6\u5019 reference \u4e0d\u4f1a\u53d8 \u76f8\u6bd4 log structure\uff0cb-tree \u4f1a overwrite instead of append only, \u8fd9\u610f\u5473\u7740\u5728 hard disk \u4e0a\u9762\u7684\u8bfb\u5199\u6307\u9488\u4f1a\u79fb\u52a8\u5230\u76f8\u5173page \u7136\u540e overwrite \u66f4\u8fdb\u4e00\u6b65\u6765\u8bf4\uff0c\u5982\u679c\u4e00\u4e2a\u64cd\u4f5c\u9700\u8981\u5199\u5165\u591a\u4e2a page\uff0c\u6bd4\u5982 split a page, \u90a3\u4e48\u4f60\u9700\u8981\u628a\u8fd9\u4e24\u4e2a page \u5148\u5199\u5165\u786c\u76d8\u7684\u76f8\u5173\u4f4d\u7f6e\uff0c\u7136\u540e\u66f4\u65b0 parent page \u7684\u6307\u9488 </p> <p>\u4e3a\u4e86\u9632\u6b62\u5199\u4e00\u534a\u7684\u65f6\u5019\u505c\u7535\u8fd9\u79cd\u60c5\u51b5\uff0c\u8fd9\u79cd\u64cd\u4f5c\u90fd\u4f1a\u5148\u5199\u5165\u4e00\u4e2alog\uff0c\u5728\u6267\u884c\u3002 \u8fd9\u4e2alog \u5c31\u662f write-ahead log(WAL)</p> <p></p>"},{"location":"Chapter%203/#btree-optimizations","title":"BTree optimizations","text":"<p>optimization \u6709\u5f88\u591a\u79cd\u65b9\u5f0f - Instead of overwriting pages and maintaining a WAL for crash recovery, some databases (like LMDB) use a copy-on-write scheme. \u5199\u5165\u7684\u65f6\u5019\u5148 copy \u5230\u53e6\u4e00\u4e2a location, then new version of the parent pages is created, pointing to that new location. \u8fd9\u79cd\u65b9\u5f0f\u9002\u5408 snapshot isolation  - Save space by abbreviating the key. Packing more keys into a page allows the tree to have a higher branching factor, thus fewer levels (This variant is known as \\(B^{+}\\) tree) - Lay out the leaf pages in sequential order to speed up reads but it is difficult to maintain as the tree grows. LSM-trees on the other hand is easier - Additional pointers have been added to the tree. For example, each leaf page may have references to its sibling pages to the left and right, which allows scanning keys in order without jumping back to parent pages.</p>"},{"location":"Chapter%203/#comparing-btree-and-lsm-trees","title":"Comparing BTree and LSM-Trees","text":"<p>As a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads However, benchmarks are often inconclusive and sensitive to details of the workload. You need to test systems with your particular workload in order to make a valid com\u2010 parison. In this section we will briefly discuss a few things that are worth considering when measuring the performance of a storage engine.</p>"},{"location":"Chapter%203/#advantages-of-lsm-trees","title":"Advantages of LSM-trees","text":"<p>Every write to B-tree occur at least twice, 1 for WAL, 1 for tree page itself (perhaps more if page needs to split). </p> <p>Log-structured indexes also rewrite data multiple times due to repeated compaction and merging of SSTables. This effect\u2014one write to the database resulting in multiple writes to the disk over the course of the database\u2019s lifetime\u2014is known as write amplification.</p> <p>LSM tree \u901a\u5e38\u53ef\u4ee5\u7ecf\u53d7\u4f4f\u66f4\u9ad8\u7684\u5199\u5165\u9700\u6c42\uff0c\u56e0\u4e3a append only \u7684\u65b9\u5f0f\uff0c\u4f46\u8fd9\u4e5f\u53d6\u51b3 storage engine \u7684 configuration and workload </p> <p>This difference is particularly important on magnetic hard drives, where sequential writes are much faster than random writes.</p> <p>LSM tree \u56e0\u4e3a compaction \u4f1a\u8282\u7701\u66f4\u591a\u7684 space (b-tree \u6709\u53ef\u80fd\u6709\u7684page \u8fd8\u6709 free space \u800c segment \u5728 compact \u8fc7\u540e\u53ea\u5305\u62ec\u6700\u65b0\u7684\u6570\u636e\u4e86)</p> <p>On many SSDs, the firmware internally uses a log-structured algorithm to turn ran\u2010 dom writes into sequential writes on the underlying storage chips, so the impact of the storage engine\u2019s write pattern is less pronounced</p>"},{"location":"Chapter%203/#downsides-of-lsm-trees","title":"Downsides of LSM-trees","text":"<p>\u6210\u4e5f compaction, \u8d25\u4e5f compaction\u3002 </p> <p>Even though storage engines try to perform compaction incrementally and without affecting concurrent access, disks have limited resources, so it can easily happen that a request needs to wait while the disk finishes an expensive compaction operation.</p> <p>the impact on throughput and average response time is small but at higher percentiles can be high. (p99 or p99.9)</p> <p>When writing to an empty database, the full disk bandwidth can be used for the initial write, but the bigger the database gets, the more disk bandwidth is required for compaction.</p> <p>\u5f53\u4e00\u4e2a database \u53d8\u7684\u8d8a\u6765\u8d8a\u5927\u7684\u65f6\u5019\uff0c\u6709\u4e00\u90e8\u5206\u7684 disk bandwidth \u5c31\u9700\u8981\u7528\u6765 compaction\u4e86</p> <p>\u8fd8\u6709\u4e00\u79cd\u60c5\u51b5\u5c31\u662f incoming writes \u7684\u901f\u5ea6\u6bd4\u4f60 compaction \u7684\u901f\u5ea6\u8981\u5feb\uff0c\u90a3\u4e48unmerged segments \u5c31\u4f1a\u4e0d\u65ad\u53d8\u5927\uff0c\u6700\u540e run out of disk space, \u5e76\u4e14\u4f60\u8bfb\u7684\u901f\u5ea6\u4e5f\u4f1a\u53d8\u6162\uff0c\u56e0\u4e3a\u5982\u679c\u4e00\u4e2akey \u4e0d\u5728memory\uff0c\u9700\u8981\u770b\u66f4\u591a\u7684 segment\u3002</p> <p>An advantage of B-trees is that each key exists in exactly one place in the index, whereas a log-structured storage engine may have multiple copies of the same key in different segments. This aspect makes B-trees attractive in databases that want to offer strong transactional semantics: in many relational databases, transaction isola\u2010 tion is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree</p> <p>B-trees are very ingrained in the architecture of databases and provide consistently good performance for many workloads, so it\u2019s unlikely that they will go away anytime soon.</p> <p>\u603b\u7684\u6765\u8bf4\u7edd\u5927\u90e8\u4efd\u7684 DB \u8fd8\u662f\u5728\u7528 btree\uff0c \u5982\u679c\u662f high write application, \u53ef\u4ee5\u6839\u636e workload \u8fdb\u884c\u6d4b\u8bd5</p>"},{"location":"Chapter%203/#other-indexing-structures","title":"Other indexing structures","text":"<p>\u5176\u5b9e\u4e0a\u9762\u8fd9\u4e24\u4e2a\u5e94\u8be5\u662f storage structure \u800c\u4e0d\u662f indexing structure.. \u56e0\u4e3a\u4ed6\u4eec\u672c\u8d28\u4e0a\u90fd\u8fd8\u662f k-v indexing..</p> <p>primary key, secondary key. secondary key is crucial for performing joins efficiently.  for example </p> <p>A secondary index can be constrcuted from a k-v index. The main difference is that keys are not unique. i.e., there might be many rows (documents, nodes) with the same key. This can be solved in 2 ways:  - list of rows/document with current id</p> id row/doc 131 123, 456, 789 - making each key unique by appending a row id to it id row/doc 1-123 123 1-456 456 <p>Either way, both b-trees and log structured can be used as secondary indexes \u5c31\u662f\u5b58\u50a8\u65b9\u5f0f\u4e0d\u4e00\u6837\u561b?</p>"},{"location":"Chapter%203/#storing-values-within-the-index","title":"Storing values within the index","text":"<p>The key in an index is the thing that queries search for, but the value can be 1 of 2 things 1. Actual row (document, vertex)  2. Reference to row For second case, the place where rows are stored is known as heap file. The heap file approach is common because it avoids duplicating data when multiple secondary index are present: each index just reference a location and actual data is kept in one place \u8fd9\u6837\u5c31\u53ef\u4ee5 overwritten in place \u4e86\uff0c\u524d\u63d0\u662f value \u5c0f\u4e8e\u7b49\u4e8e\u73b0\u5728\u7684 value, \u4e0d\u7136\u9700\u8981\u65b0\u5f00\u4e00\u4e2a page \u7136\u540eupdate reference</p> <p>The situation is more complicated if the new value is larger, as it probably needs to be moved to a new location in the heap where there is enough space. In that case, either all indexes need to be updated to point at the new heap location of the record, or a forwarding pointer is left behind in the old heap location</p> <p>\u5728\u4e00\u4e9b\u60c5\u666f\u4e0b\uff0c\u8fd9\u591a\u4f59\u7684\u4e00\u6b21 lookup \u4f1a\u53d8\u7684\u5f88\u6602\u8d35\uff0c\u6240\u4ee5\u6709\u4e86 store the indexed row directly within an index. Known as clustered index</p> <p>For example, in MySQL\u2019s InnoDB storage engine, the primary key of a table is always a clustered index, and secondary indexes refer to the primary key (rather than a heap file location)</p> <p>\u8fd9\u5c31\u6709\u4e86 tradeoff</p> <p>As with any kind of duplication of data, clustered and covering indexes can speed up reads, but they require additional storage and can add overhead on writes.Databases also need to go to additional effort to enforce transactional guarantees, because appli\u2010 cations should not see inconsistencies due to the duplication.</p>"},{"location":"Chapter%203/#multi-column-indexes","title":"Multi column indexes","text":"<p>\u4e4b\u524d\u8bf4\u7684\u90fd\u662f\u4e00\u4e2akey map \u5230\u4e00\u4e2a value \u4e0a\u9762\uff0c\u5982\u679c\u9700\u8981\u591a\u4e2a column map \u5230\u4e00\u4e2a key \u4e0a\u9762\uff0c\u901a\u5e38\u7528\u7684\u65b9\u6cd5\u662f concatenated index, which simply combines several fields into one key by appending one column to another  for example, <code>(lastname, firstname)</code> would be concatenated key on phonebook when you lookup phone number. \u8fd9\u6837\u4f60\u53ef\u4ee5\u627e\u5230\u6240\u6709 lastname \u6216\u8005 lastname + first name \u5f00\u5934\u7684\u4eba\u7684\u7535\u8bdd\u53f7\u7801\u4e86</p> <p>multi-dimensional indexes \u662f\u4e00\u79cd\u66f4\u901a\u7528\u7684 multi column index. \u6bd4\u5982\u4e00\u4e2a map search website \u7684 database contain latitude and longitude of each restaurant. When user search for a restaurant, the website needs to search for all restaurants within a rectangular map area that user is currently viewing. This requires 2d range query like the following: <pre><code>SELECT * FROM restaurants \nWHERE latitude &gt; 51.4946 AND latitude &lt; 51.5079\n    AND longitude &gt; -0.1162 AND longitude &lt; -0.1004;\n</code></pre></p> <p>A standard B-tree or LSM-tree index is not able to answer that kind of query efficiently</p> <p>\u56e0\u4e3a\u4ed6\u4eec\u90fd\u662f\u4e00\u4e2a\u7ef4\u5ea6\u7684 key index. \u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u662f\u628a 2d location translate into single number using a space-filling curve, then use regular b-tree index. \u66f4 common \u7684\u65b9\u5f0f\u662f\u7528 geospatial indexes such as R-trees. </p> <p>multi dimensional indexes \u4e0d\u4e00\u5b9a\u53ea\u5728\u5730\u7406\u67e5\u8be2\u6709\u5e2e\u52a9\uff0c\u6bd4\u5982\u6211\u4eec\u4e5f\u53ef\u4ee5\u628a\u989c\u8272 model \u6210\u4e00\u4e2a 3d indexes (red, green, blue) \u7528\u6765\u67e5\u8be2\u4e00\u4e2a\u7b26\u5408\u6761\u4ef6\u989c\u8272\u533a\u95f4\u5185\u7684\u7269\u54c1 (ecommerce, inventory database \u90fd\u53ef\u4ee5\u7528\u5230) \u6216\u8005\u4f60\u53ef\u4ee5\u5bf9\u4e00\u4e2a weather db \u7684 <code>(date, temperature)</code> \u5efa\u4e00\u4e2a 2d index \u7136\u540e efficiently search for all the observations during the year 2013 where the temperature was between 25 and 30. </p>"},{"location":"Chapter%203/#full-text-search-and-fuzzy-indexes","title":"Full text search and fuzzy indexes","text":"<p>\u76ee\u524d\u4e3a\u6b62\u63d0\u5230\u7684 index \u90fd\u5fc5\u987b exactly match \u624d\u884c\uff0c\u4e3a\u4e86\u8fbe\u6210 fuzzy search (similar key) \u5c31\u9700\u8981\u7528\u5230 edit distance \u65b9\u6cd5\u4e86\uff0cLucene \u5728\u5b83 in memory \u91cc\u9762\u7528\u7684\u662f trie data structure, \u7136\u540e\u7528 Levenshtein automaton \u6765\u652f\u6301 efficient search for given edit distance</p> <p>Other fuzzy search techniques go in the direction of document classification and machine learning. See an information retrieval textbook for more detail [e.g., 40].</p> <p>TF-IDF \u4e4b\u7c7b\u7684</p>"},{"location":"Chapter%203/#keep-everything-in-memory","title":"Keep everything in memory","text":"<p>\u4e0a\u9762\u7684 data structures \u90fd\u6536\u5230\u4e86 disk \u7684\u9650\u5236\uff0c</p> <p>With both magnetic disks and SSDs, data on disk needs to be laid out carefully if you want good performance on reads and writes. However, we tolerate this awkwardness because disks have two significant advantages: they are durable (their contents are not lost if the power is turned off), and they have a lower cost per gigabyte than RAM.</p> <p>\u968f\u7740 RAM \u7684\u4ef7\u683c\u53d8\u4f4e\uff0c\u5f88\u591a\u7684 dataset \u5e76\u6ca1\u6709\u90a3\u4e48\u5927 (less than 10GB) \u6240\u4ee5\u6211\u4eec\u5b8c\u5168\u53ef\u4ee5\u628a\u6574\u4e2a DB \u653e\u5728 memory\u91cc\u9762\uff0c\u6240\u4ee5\u6709\u4e86 in memory database. Memcashed, Redis \u6709\u51e0\u79cd\u65b9\u5f0f\u8ba9 in memory db persistent. \u6bd4\u5982\u7ed9 RAM \u52a0\u4e0a battery\uff0cwriting log of changes to disk, write periodic snapshots to disk, replicating memory state to other machines</p> <p>\u6240\u4ee5\u5982\u679c in memory db crashed, \u4ed6\u4eec\u53ef\u4ee5\u4ece disk\uff0c\u6216\u8005 replica \u90a3\u8fb9\u6062\u590d\u539f\u6765\u7684\u6570\u636e</p> <p>Counterintuitively, the performance advantage of in-memory databases is not due to the fact that they don\u2019t need to read from disk. Even a disk-based storage engine may never need to read from disk if you have enough memory, because the operating system caches recently used disk blocks in memory anyway. Rather, they can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk</p> <p>\u4e3b\u8981\u8fd8\u662f disk structure \u9700\u8981 maintain \u6bcf\u4e2a page \u7684 inode\uff0cdisk block \u8fd9\u7c7b\u7684\u4fe1\u606f</p> <p>Recent research indicates that an in-memory database architecture could be extended to support datasets larger than the available memory, without bringing back the over\u2010 heads of a disk-centric architecture</p> <p>\u4e66\u91cc\u9762\u4e5f\u63d0\u5230\u4e86\u8fd9\u8ddf OS \u7684 virtual memory \u4ee5\u53ca swap file \u7c7b\u4f3c\u3002 \u53ea\u4e0d\u8fc7 DB \u5bf9\u4e8e\u81ea\u5df1\u7ba1\u7406\u7684\u6570\u636e\u66f4\u4e86\u89e3\uff0c\u6240\u4ee5\u80fd\u591f work at the granularity of individual records rather than entire memory pages.</p> <p>Further changes to storage engine design will probably be needed if non-volatile memory (NVM) technologies become more widely adopted</p> <p>\u5982\u679cNVM \u5927\u9762\u79ef\u4f7f\u7528\uff0c\u90a3\u4e48 DB \u7684 implementation \u80af\u5b9a\u4f1a\u6709\u5927\u7684\u53d8\u5316</p> <p>https://colin-scott.github.io/personal_website/research/interactive_latency.html</p>"},{"location":"Chapter%203/#transaction-processing-or-analytics","title":"Transaction processing or Analytics?","text":"<p>\u65e9\u671f\u7684 data processing \u90fd\u8ddf commercial transaction \u6709\u5173\uff1a making a sale, placing an order, paying employee's salary etc </p> <p>As DB expanded into areas that didn't involve money changing hands, the term transaction stuck. Referring to a group of reads and writes that form a logical unit.</p> <p>\u5c3d\u7ba1\u6570\u636e\u5e93\u5df2\u7ecf\u7528\u5728\u5f88\u591a\u4e0d\u540c\u7684\u6570\u636e\u4e0a\u9762\u4e86 (comments on blogs, actions in games, contacts in an address book) \u8fd9\u4e9b\u4e5f\u53ef\u4ee5\u7528 business transaction \u6765\u5b9e\u73b0\u3002app \u901a\u5e38\u901a\u8fc7 key \u627e\u5230\u76f8\u5173\u6570\u636e\uff0c\u6839\u636e\u7528\u6237\u7684input \u66f4\u65b0\u7ed3\u679c\u3002\u56e0\u4e3a\u8fd9\u4e9b\u5e94\u7528\u662f\u5b9e\u65f6\u7684\uff0c\u8fd9\u4e9b access pattern \u4e5f\u53eb\u505a online transaction processing (OLTP)</p> <p>\u8fd8\u6709\u4e00\u79cd\u975e\u5e38\u4e0d\u540c\u7684 access pattern, \u662f\u7528\u6765\u505a\u6570\u636e\u5206\u6790\u7684\uff0c\u4e5f\u53eb\u505a online analytic processing (OLAP) OLAP \u901a\u5e38\u9700\u8981 scan over a huge number of records and calculate aggregate statistics (such as count, sum, or avg) if your data is a table of sales transactions, then analytic queries might be - What was the total revenue of each of our stores in Jan? - How many more bananas than usual did we sell during our latest promotion?  - Which brand of baby food is most often purchased together with brand X diapers? \u8fd9\u7c7b\u7684 query \u901a\u5e38\u662f\u7531 business analysts \u5199\u7684\uff0c\u7528\u6765\u5e2e\u516c\u53f8\u7ba1\u7406\u5c42\u6765\u51b3\u7b56</p> <p>\u4e0b\u9762\u662f\u4e00\u5f20\u5bf9\u7167\u56fe </p> <p>\u65e9\u671f OLTP and OLAP \u90fd\u5728\u4e00\u4e2a DB \u4e0a\u9762\u8dd1\uff0c\u540e\u6765\u516c\u53f8\u628a OLAP \u5206\u51fa\u6765\u4e13\u95e8\u7528 data warehouse \u6765\u6267\u884c\u4e86\uff0c\u56e0\u4e3a\u8fd9\u6837\u4e0d\u5f71\u54cd OLTP \u7684 latency</p>"},{"location":"Chapter%203/#data-warehousing","title":"Data Warehousing","text":"<p>\u4e00\u4e2a\u4f01\u4e1a\u53ef\u80fd\u6709\u591a\u4e2a\u4e0d\u540c\u7684 transaction processing system\uff0csystem powering the customer facing site, checkout systems in physical stores, system for tracking inventory in warehouse, system for routing vehicles, managing suppliers, administering employees etc.  \u800c\u8fd9\u4e9b transaction system \u5fc5\u987bhighly available. \u6240\u4ee5 DB admin \u4e0d\u4f1a\u8ba9 business analysts run ad hoc analytic queries on OLTP database. \u56e0\u4e3a\u8fd9\u4e9b query \u901a\u5e38\u9700\u8981 scan \u6574\u4e2a table \u4ece\u800c\u5f71\u54cd\u5176\u4ed6 query\u7684latency Data warehouse \u5219\u662f\u4e13\u95e8\u7528\u6765\u7ed9 analysts \u5206\u6790\u6570\u636e\u7528\u7684\uff0cdata warehouse \u4e00\u822c\u628a\u5404\u79cd\u4e0d\u540c\u7684\u6570\u636e\u90fd\u6574\u5408\u5728\u4e00\u8d77\uff0c\u65b9\u4fbf\u522b\u4eba\u67e5\u8be2 \u628a\u5404\u79cd\u4e0d\u540c\u7684\u6570\u636e\u6574\u5408\u7684\u8fc7\u7a0b\u53eb\u505a Extract-Transform-Load (ETL). \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u540c schema \u7684\u6570\u636e\u4f1a\u88ab\u6574\u5408\u6210\u9002\u5408\u6570\u636e\u5206\u6790\u7684 schema  </p> <p>\u5c0f\u516c\u53f8\u53ef\u80fd\u4e0d\u9700\u8981 data warehouse, \u56e0\u4e3a\u4e00\u4e2a db \u751a\u81f3\u662f spreadsheet \u5c31\u53ef\u80fd\u89e3\u51b3\u95ee\u9898\u4e86</p>"},{"location":"Chapter%203/#divergence-between-oltp-db-and-data-warehouses","title":"Divergence between OLTP DB and data warehouses","text":"<p>Data warehouse \u7528\u7684 data model \u901a\u5e38\u662f relational\uff0c\u56e0\u4e3a SQL \u975e\u5e38\u9002\u5408 analytics query. \u800c\u4e14\u6709\u5f88\u591a\u56fe\u5f62\u5316\u7684\u5de5\u5177\u53ef\u4ee5\u751f\u6210 SQL queries, \u628a\u7ed3\u679c\u53ef\u89c6\u5316</p> <p>data warehouse \u8ddf relational OLTP \u5728\u8868\u9762\u4e0a\u770b\u8d77\u6765\u5f88\u50cf\uff0c\u4f46\u4ed6\u4eec\u5206\u522b\u5bf9\u4e0d\u540c\u7684 query \u505a\u4e86\u4e0d\u540c\u7684\u4f18\u5316\u3002database vendor \u901a\u5e38\u652f\u6301 transaction workload or analytics workload but not both</p> <p>Data warehouse vendors such as Teradata, Vertica, SAP HANA, and ParAccel typi\u2010 cally sell their systems under expensive commercial licenses. Amazon RedShift is a hosted version of ParAccel.</p> <p>More recently, a plethora of open source SQL-on- Hadoop projects have emerged; they are young but aiming to compete with commercial data warehouse systems. These include Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill [52, 53]. Some of them are based on ideas from Google\u2019s Dremel [54].</p> <p>Hive, Spark, Presto \u8fd9\u4e9b\u5de5\u5177\u597d\u50cf\u633a\u51fa\u540d\u7684\uff0c\u662f SQL on data warehouse</p>"},{"location":"Chapter%203/#stars-and-snowflake-schema","title":"Stars and Snowflake schema","text":"<p>Chapter 2 \u8bb2\u4e86\u5f88\u591a transanctional data model\u3002\u8fd9\u91cc analytics \u7684 data model \u5c31\u4e24\u79cd star schema and snowflake schema</p> <p>\u7528\u4e00\u4e2a\u96f6\u552e\u5e97\u4e3e\u4f8b\u5b50 \u5728 schema \u6700\u4e2d\u95f4\u7684\u53eb\u505a fact table, fact table \u7684\u6bcf\u4e00\u884c\u8868\u793a\u5728\u4e00\u4e2a\u65f6\u95f4\u53d1\u751f\u7684\u4e8b\u4ef6 (\u7528\u6237\u4e70\u4e86\u4e00\u4e2a\u5546\u54c1)  </p> <p>Usually, facts are captured as individual events, because this allows maximum flexibility of analysis later. However, this means that the fact table can become extremely large. A big enterprise like Apple, Walmart, or eBay may have tens of petabytes of transaction history in its data warehouse, most of which is in fact tables</p> <p>column \u5219\u662f\u5728\u90a3\u4e2a\u65f6\u95f4\u7684\u65f6\u5019\u5bf9\u5e94\u7684\u6570\u503c such as price at which the product was sold \u6216\u8005\u6307\u5411\u53e6\u4e00\u4e2a dimension table\uff0c \u6bd4\u5982 the name of the customer. \u8fd9\u4e9b\u6307\u9488\u5c31\u662f foreign key reference. \u901a\u5e38\u56de\u7b54 who, what, where, when, how, why \u8fd9\u4e9b\u95ee\u9898</p> <p>Even date and time are often represented using dimension tables</p> <p>\u8fd9\u6837\u66f4\u65b9\u4fbfencode \u8282\u65e5\u4e4b\u7c7b\u7684\u4fe1\u606f, \u6bd4\u5982\u56fe\u7247\u91cc\u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u7528\u4e00\u4e2a boolean \u8868\u793a\u5f53\u5929\u662f\u5426\u662f\u8282\u65e5</p> <p>star schema \u7684\u540d\u5b57\u5176\u5b9e\u5c31\u8ddf\u5b83\u957f\u76f8\u4e00\u6837\uff0csnowflake schema \u540d\u5b57\u4e5f\u662f\u8fd9\u4e48\u6765\u7684\uff0c\u53ea\u4e0d\u8fc7 snowflake schema \u628a\u6bcf\u4e2a dimension table further broken down into subdimensions.  \u6bd4\u5982\u628a\u6bcf\u4e00\u4e2a brand \u4ee5\u53ca product category \u518d\u7528\u4e00\u4e2a dimension table \u8868\u793a\u51fa\u6765\u3002</p> <p>Snowflake schemas are more normalized than star schemas, but star schemas are often preferred because they are simpler for analysts to work with</p>"},{"location":"Chapter%203/#column-oriented-storage","title":"Column Oriented Storage","text":"<p>If you have trillions of rows and petabytes of data in your fact tables, storing and querying them efficiently becomes a challenging problem.</p> <p>\u6709\u65f6\u5019 analytics query \u53ea\u9700\u8981\u51e0\u767e\u4e2a column \u4e2d\u7684\u51e0\u4e2a\uff0c \u6bd4\u5982 example 3.1 <pre><code>SELECT\n    dim_date.weekday, dim_product.category,\n    SUM(fact_sales.quantity) AS quantity_sold \n\nFROM fact_sales\n    JOIN dim_date ON fact_sales.date_key = dim_date.date_key\n    JOIN dim_product ON fact_sales.product_sk = dim_product.product_sk \n\nWHERE\n    dim_date.year = 2013 AND\n    dim_product.category IN ('Fresh fruit', 'Candy') \n\nGROUP BY\n    dim_date.weekday, dim_product.category;\n</code></pre> \u8fd9\u91cc\u53ea\u9700\u8981 <code>date_key</code>, <code>product_sk</code>, <code>quantity</code> \u8fd9\u51e0\u4e2a column\uff0c\u90a3\u4e48\u5982\u4f55\u6709\u6548\u7684\u6267\u884c\u8fd9\u4e2aquery\u5462\uff1f \u5927\u591a\u6570 OLTP \u7684\u7cfb\u7edf\u90fd\u662f\u7528 row oriented structure \u6765\u5b58\u6570\u636e\u7684\uff0c\u6240\u4ee5\u6bcf\u6b21\u4f60 query\u7684\u65f6\u5019\u5c3d\u7ba1\u53ea\u9700\u8981\u5176\u4e2d\u4e00\u90e8\u5206\u7684 column\uff0c\u4ed6\u8fd8\u662f\u4f1a\u628a\u6bcf\u4e00\u4e2arow\u7684\u6240\u6709 column \u4ecedisk \u8bfb\u8fdb memory. \u8fd9\u5c31\u5bfc\u81f4query \u53d8\u6162 column oriented storage \u5c31\u53ef\u4ee5\u5f88\u597d\u7684\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ed6\u4e0d\u662f\u6309 row \u6765\u5b58\u800c\u662f\u6309\u7167 column \u6765\u5b58\uff0c  \u4ed6\u4eec\u4e5f\u662f\u6309\u7167row\u7684\u987a\u5e8f\u6392\u5217\u597d\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u91cd\u7ec4\u6210\u539f\u6765\u7684row</p> <p>Thus, if you need to reassemble an entire row, you can take the 23rd entry from each of the individual column files and put them together to form the 23rd row of the table.</p>"},{"location":"Chapter%203/#column-compression","title":"Column compression","text":"<p>column storage \u7684\u91cd\u590d\u6b21\u6570\u5f88\u9ad8\u6240\u4ee5\u5f88\u9002\u5408 compression,  \u6709\u5f88\u591a\u79cd compression \u65b9\u6cd5\uff0c\u8fd9\u91cc\u4ecb\u7ecd\u4e86 bitmap encoding  </p> <p>Bitmap indexes such as these are very well suited for the kinds of queries that are common in a data warehouse. For example:     WHERE product_sk IN (30, 68, 69):     Load the three bitmaps for product_sk = 30, product_sk = 68, and product_sk = 69, and calculate the bitwise OR of the three bitmaps, which can be done very efficiently.</p> <p>\u8fd9\u91cc bitwise OR \u662f\u4e00\u4e2a CPU instruction \u6240\u4ee5\u975e\u5e38\u5feb \u67e5\u8be2\u4e24\u4e2a\u4e0d\u540c\u7684 column \u4e5f\u53ef\u4ee5\u7528 bitwise AND \u6765\u8fdb\u884c\u67e5\u8be2</p> <p>WHERE product_sk = 31 AND store_sk = 3:     Load the bitmaps for product_sk = 31 and store_sk = 3, and calculate the bit\u2010 wise AND. This works because the columns contain the rows in the same order, so the kth bit in one column\u2019s bitmap corresponds to the same row as the kth bit in another column\u2019s bitmap.</p> <p>\u8fd8\u6709\u4e00\u4e9b\u66f4\u7ec6\u8282\u7684\u4f18\u5316\uff0c\u4e0d\u8fc7\u6211\u89c9\u5f97\u53ea\u662f\u5bf9\u4e8e column storage \u5f00\u53d1\u4eba\u5458\u6bd4\u8f83\u76f8\u5173</p> <p>Besides reducing the volume of data that needs to be loaded from disk, column- oriented storage layouts are also good for making efficient use of CPU cycles. For example, the query engine can take a chunk of compressed column data that fits comfortably in the CPU\u2019s L1 cache and iterate through it in a tight loop (that is, with no function calls).</p> <p>CPU \u5728 L1 cache \u53ef\u4ee5\u5bf9\u538b\u7f29\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4e5f\u53eb vector processing</p> <p>Operators, such as the bitwise AND and OR described previously, can be designed to operate on such chunks of compressed column data directly. This technique is known as vectorized processing</p>"},{"location":"Chapter%203/#sorted-order-in-column-storage","title":"Sorted order in column storage","text":"<p>\u6211\u4eec\u8fd8\u53ef\u4ee5\u6309\u7167\u9700\u6c42\u5bf9 column \u91cd\u65b0\u8fdb\u884c\u6392\u5e8f\uff0c\u5f53\u7136\u91cd\u65b0\u6392\u5e8f\u4e5f\u8981\u628a\u5bf9\u5e94\u7684\u6240\u6709 column \u90fd\u6309\u7167\u8fd9\u4e2a\u8981\u6c42\u6392\u5e8f\uff0c\u8fd9\u6837\u91cd\u65b0\u7ec4\u88c5\u7684 row \u4e5f\u662f\u5bf9\u7684</p> <p>Note that it wouldn\u2019t make sense to sort each column independently, because then we would no longer know which items in the columns belong to the same row. We can only reconstruct a row because we know that the kth item in one column belongs to the same row as the kth item in another column.</p> <p>A second column can determine the sort order of any rows that have the same value in the first column. For example, if date_key is the first sort key in Figure 3-10, it might make sense for product_sk to be the second sort key so that all sales for the same product on the same day are grouped together in storage. That will help queries that need to group or filter sales by product within a certain date range.</p>"},{"location":"Chapter%203/#writing-to-column-oriented-storage","title":"Writing to column oriented storage","text":"<p>column storage \u901a\u5e38\u4e0d\u9700\u8981 modify inplace, \u6240\u4ee5 LSM tree \u521a\u5408\u9002\u4e00\u4e9b</p> <p>Fortunately, we have already seen a good solution earlier in this chapter: LSM-trees. All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn\u2019t matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written to new files in bulk. This is essen\u2010 tially what Vertica does</p>"},{"location":"Chapter%203/#aggregation-data-cubes-and-materialized-views","title":"Aggregation: Data Cubes and Materialized Views","text":"<p>data warehouse \u9664\u4e86 column storage \u4ee5\u5916\u8fd8\u6709\u4e00\u70b9\u503c\u5f97\u4e00\u63d0, materialized aggregates  data warehouse \u7684 query \u901a\u5e38\u6d89\u53ca\u6574\u5408\u6570\u636e\uff0c\u6bd4\u5982 <code>COUNT</code>, <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code> \u8fd9\u7c7b\u7684\u3002 \u5982\u679c\u6bcf\u6b21\u8fd9\u79cd query \u90fd\u8981\u91cd\u65b0\u8ba1\u7b97\u4e00\u904d\u5c31\u5f88\u6d6a\u8d39\uff0c\u6240\u4ee5\u5c31\u53ef\u4ee5\u63d0\u524d\u8ba1\u7b97\u597d\u8fd9\u7c7b\u7684\u6570\u636e\u5e76\u4e14\u4fdd\u5b58\u8d77\u6765\uff0c\u8fd9\u79cd\u63d0\u524d\u8ba1\u7b97\u597d\u7684 aggregate \u6570\u636e\u7684\u4e5f\u53eb\u505a materialized view. \u5f53 materialized view \u5e95\u5c42\u7684\u6570\u636e\u88ab\u66f4\u65b0\u4e86\uff0c\u4ed6\u4eec\u4e5f\u8981\u88ab\u66f4\u65b0\uff0c\u56e0\u4e3a\u4ed6\u4eec\u53ea\u662f denormalized copy of the data. </p> <p>A common special case of a materialized view is known as a data cube or OLAP cube [64]. It is a grid of aggregates grouped by different dimensions. Figure 3-12 shows an example.</p> <p></p> <p>\u8fd9\u6837\u53ef\u4ee5\u5feb\u901f\u5f97\u5230\u4e00\u4e2a\u4ea7\u54c1\u5728\u4e00\u6bb5\u65f6\u95f4\u5185\u5356\u4e86\u591a\u5c11\uff0c\u6216\u8005\u4e00\u5929\u5185\u6240\u6709\u4ea7\u54c1\u5356\u4e86\u591a\u5c11 \u901a\u5e38 fact table \u80af\u5b9a\u591a\u4e8e\u4e24\u4e2a dimension\uff0c\u6bd4\u5982 date, product, store, promotion, customer 5 dimension \u7684 data cube \u6bd4\u8f83\u96be\u60f3\u8c61\uff0c\u4e0d\u8fc7\u9053\u7406\u662f\u4e00\u6837\u7684\uff0ceach cell contains the sales for a particular date-product-store-promotion-customer combination  matrix operation \u90fd\u662f\u7c7b\u4f3c\u7684\u9053\u7406 data cube \u53ea\u4e0d\u8fc7\u7528\u6765\u4f18\u5316\u4e00\u4e9bquery \u800c\u5df2\uff0c\u5927\u90e8\u5206 data warehouse \u8fd8\u662f\u7528 raw data \u7684\u65b9\u5f0f\u6765\u5b58\u50a8\u6570\u636e</p>"},{"location":"Chapter%203/#summary","title":"Summary","text":"<p>\u8fd9\u4e00\u7ae0\u8bb2\u4e86 storage engine\uff0cwhat happens when you store data in database? what does database do when you query the data again later? (storage and index)</p> <p>\u4f5c\u8005\u628a storage engine \u5206\u4e862\u7c7b OLTP and OLAP</p> <ul> <li> <p>OLTP systems are typically user-facing, which means that they may see a huge volume of requests. In order to handle the load, applications usually only touch a small number of records in each query. The application requests records using some kind of key, and the storage engine uses an index to find the data for the requested key. Disk seek time is often the bottleneck here.</p> </li> <li> <p>Data warehouses and similar analytic systems are less well known, because they are primarily used by business analysts, not by end users. They handle a much lower volume of queries than OLTP systems, but each query is typically very demanding, requiring many millions of records to be scanned in a short time. Disk bandwidth (not seek time) is often the bottleneck here, and column- oriented storage is an increasingly popular solution for this kind of workload.</p> </li> </ul> <p>\u5728 OLTP \u91cc\u9762\uff0c\u53c8\u7ec6\u5206\u6210\u4e86\u4e24\u7c7b - log structure (append only) - update in place, which treat disk as a set of fixed size pages. Btree is the bigest example</p> <p>\u9664\u4e86\u8fd9\u4e24\u79cd structure, \u4ed6\u8fd8\u63d0\u5230\u4e86\u5176\u4ed6\u7684 indexing structure \u4ee5\u53ca in memory database</p> <p>\u5728\u8bb2 OLAP \u7684\u65f6\u5019, \u56e0\u4e3a workload \u8ddf OLTP \u6781\u4e0d\u4e00\u6837\uff0c\u6240\u4ee5 column storage \u6709\u4e86\u72ec\u7279\u7684\u4f18\u52bf</p> <p>This background illustrated why analytic workloads are so different from OLTP: when your queries require sequentially scanning across a large number of rows, indexes are much less relevant. Instead it becomes important to encode data very compactly, to minimize the amount of data that the query needs to read from disk.</p> <p>\u6709\u4e86\u8fd9\u4e9b\u57fa\u7840\uff0c\u5f00\u53d1 app \u7684\u65f6\u5019\u9009\u62e9database \u5c31\u6709\u4e86\u5f88\u591a\u7684\u53c2\u8003\u5e2e\u52a9</p> <p>there are many compaction algorithm available (such as zstd) \u5bf9zstd \u7684\u53d1\u660e\u4eba\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u542c\u8fd9\u4e00\u671f podcast, \u8fd8\u662f\u5f88\u6709\u610f\u601d\u7684</p>"},{"location":"Chapter%204/","title":"Chapter 4","text":"<p>Encoding and Evolution  Application \u4f1a\u56e0\u4e3a\u65b0\u7684\u9700\u6c42\u800c\u4e0d\u65ad\u6539\u53d8\uff0c\u8fd9\u4e5f\u5728Chapter 1#Evolvability Making change easy \u91cc\u9762\u63d0\u5230\u8fc7\u4e86\uff0c\u90a3\u5c31\u662f\u6211\u4eec\u9700\u8981\u8ba9\u6211\u4eec\u7684 system easy to adapt change </p> <p>\u901a\u5e38\u60c5\u51b5\u4e0b\uff0capplication \u529f\u80fd\u7684\u53d8\u5316\u4e5f\u610f\u5473\u7740\u5bf9\u5e94\u7684 data \u53d1\u751f\u6539\u53d8\uff0cnew fields being added, record type needs to be captured, or exisiting data needs to be presented in a new way</p> <p>Chapter 2 \u91cc\u9762\u63d0\u5230\u7684 relational data \u901a\u5e38 assume \u6240\u6709\u7684 data \u4f1a\u6839\u636e\u5df2\u7ecf\u5b9a\u4e49\u597d\u7684 schema \u6765\u64cd\u4f5c\uff0c\u5c3d\u7ba1 schema \u53ef\u4ee5\u66f4\u65b0\uff0c\u4f46\u4e00\u4e2a\u65f6\u95f4\u70b9\u53ea\u80fd\u6709\u4e00\u4e2a schema \u6709\u6548</p> <p>\u4e0e\u4e4b\u76f8\u5bf9\u7684\u662f schema on read (\"schemaless\") database. \u8fd9\u79cddatabase \u53ef\u4ee5\u540c\u65f6\u5b58\u8001\u7684\u548c\u65b0\u7684 data formats\uff0c\u8fd9\u65f6\u5019\u5982\u679c\u6709 schema change\uff0c\u90a3\u4e48 application \u5c42\u5c31\u9700\u8981\u505a\u51fa\u76f8\u5e94\u7684\u6539\u53d8\uff0c\u6bd4\u5982\u4f60\u65b0\u52a0\u4e86\u4e00\u4e2a field\uff0c \u4f60application\u5c31\u8981\u5224\u65ad\u8fd9\u4e2a field \u662f\u5426\u4e3a\u7a7a \u4f46\u5f53\u4e00\u4e2a application \u53d8\u5f97\u590d\u6742\u6216\u8005\u5f88\u5e9e\u5927\u7684\u65f6\u5019\u8fd9\u79cd\u65b9\u6cd5\u901a\u5e38\u4e0d\u884c - With server-side app you may want to perform a rolling upgrade, deploy new version to few nodes at a time to ensure it can run smoothly. - With client-side applications, \u4f60\u5c31\u53ea\u80fd\u7948\u7977\u7528\u6237\u53ca\u65f6\u5b89\u88c5\u4f60\u7684\u66f4\u65b0\u4e86</p> <p>\u8fd9\u4e5f\u610f\u5473\u7740\u65b0/\u8001\u7684\u4ee3\u7801\uff0c\u65b0/\u8001\u7684 data formats \u53ef\u80fd\u540c\u65f6\u5b58\u5728\uff0c\u6240\u4ee5\u4e3a\u4e86\u6211\u4eec\u7684 system \u80fd\u6b63\u5e38\u8fd0\u8f6c\uff0c\u6211\u4eec\u9700\u8981\u4e24\u4e2a\u65b9\u5411\u7684 compactibility - Backward compatibility. Newer code can read data that was written by older code. - Forward compatibility. Older code can read data that was written by newer code.</p> <p>backward compatibility \u901a\u5e38\u4e0d\u96be\u5b9e\u73b0\uff0c\u4f5c\u4e3a\u66f4\u65b0\u4ee3\u7801\u7684\u4eba\uff0c\u4f60\u77e5\u9053\u4e4b\u524d\u7684 data format \u6240\u4ee5\u53ea\u9700\u8981\u6dfb\u52a0\u76f8\u5e94\u7684\u5224\u65ad\u5c31\u53ef\u4ee5\u4e86 forward compatibility \u5219\u9700\u8981older code to ignore additions made by a newer version of the code</p> <p>\u8fd9\u4e00\u7ae0\u4f1a\u770b\u51e0\u79cd\u4e0d\u540c\u7684 data format encoding (JSON, XML, Protocol Buffers, Thrift, and Avro) \u4ee5\u53ca\u5b83\u4eec\u5982\u4f55 handle schema changes \u5e76\u4e14\u4fdd\u8bc1 old/new data and code coexist in the system</p> <p>\u7136\u540e\u4f1a\u8ba8\u8bba\u8fd9\u4e9b formats used for data storage and communication: web services, Representational State Transfer (REST), and remote procedure calls (RPC), as well as message passing system (message queues and actors)</p>"},{"location":"Chapter%204/#formats-for-encoding-data","title":"Formats for Encoding Data","text":"<p>Programs \u901a\u5e38\u8ddf 2\u79cd\u6570\u636e\u6253\u4ea4\u9053 - In memory, data kept in objects, arrays, maps, trees, and these data structure are optimized for CPU access (pointers) - When write data to file or over network, you have to encode it as some kind of self-contained sequence of bytes (e.g. JSON document). Since a pointer wouldn't make sense to any other process, this sequence of bytes looks quite different from the data structures used in memory</p> <p>\u6240\u4ee5\u5728\u8fd9\u4e24\u79cd representation \u5c31\u9700\u8981\u4e00\u79cd\u8f6c\u6362  (translation), \u4ece in-memory \u5230 byte sequence is called encoding (also called serialization or marshalling), \u53cd\u8fc7\u6765\u53eb\u505a decoding (parsing, deserialization, unmarshalling)</p> <p>\u56e0\u4e3a serialization \u8fd9\u4e2a\u8bcd\u5728 transaction isolation level \u91cc\u9762\u8fd8\u4f1a\u7528\u5230\uff0c\u8fd9\u672c\u4e66\u5c31\u7528 encoding \u8fd9\u4e2a\u8bcd\u4e86\uff0c\u5c3d\u7ba1\u73b0\u5b9e\u666e\u904d\u7528 serialization \u8fd9\u4e2a\u8bcd</p> <p>\u4e0b\u9762\u8fc7\u4e00\u4e0b\u4e0d\u540c\u7684 encoding formats</p>"},{"location":"Chapter%204/#language-specific-foramts","title":"Language Specific Foramts","text":"<p>\u6bcf\u4e2a\u8bed\u8a00\u90fd\u6709\u81ea\u5df1\u7684 encoding \u65b9\u5f0f</p> <p>Java has java.io.Serializable [1], Ruby has Marshal [2], Python has pickle [3], and so on. Many third-party libraries also exist, such as Kryo for Java [4].</p> <p>\u4f46\u4ed6\u4eec\u6709\u5f88\u591a\u9650\u5236 - Tied to a particular programming language. Not possible to integrate your system to other organizations - Security problem because decoding process need to construct object from bytes and attacker can instantiate arbitrary class when passing byte sequence to your system - Hard to versioning data - Efficiency is bad (Java is famous for bad performance)</p>"},{"location":"Chapter%204/#json-xml-and-binary-variants","title":"JSON, XML, and Binary variants","text":"<p>JSON \u548c XML \u53ef\u80fd\u662f\u5f88\u666e\u904d\u7684 encoding \u683c\u5f0f\u4e86\uff0cwidely known, widely supported, and widely disliked. XML \u88ab\u8ba4\u4e3a too verbose. JSON \u662f\u56e0\u4e3a build-in support by web browsers CSV \u4e5f\u662f\u53e6\u4e00\u4e2a popular format \u5c3d\u7ba1\u6ca1\u90a3\u4e48\u5f3a\u5927</p> <p>JSON, XML, CSV \u662f\u4ee5\u6587\u5b57\u5f62\u5f0f\u5b58\u50a8\u7684\uff0c\u6240\u4ee5\u4e5f\u662f \u201chuman readable\" format \u4f46\u4ed6\u4eec\u8fd8\u662f\u6709\u4e0d\u5c11\u95ee\u9898 - Ambiguity around encoding of numbers. In XML and CSV, 0. you cannot distinguish between a number and a string that happens to consist of digits (except by referring to an external schema). JSON \u80fd\u533a\u5206\uff0c\u4f46\u4e0d\u533a\u5206 integer and float point numbers which has precision issue (JSON \u65e0\u6cd5\u8fa8\u8bc6\u5927\u4e8e \\(2^{53}\\)\u7684\u6570\uff0cThe JSON returned by Twitter\u2019s API includes tweet IDs twice, once as a JSON number and once as a decimal string, to work around the fact that the numbers are not correctly parsed by JavaScript applications ) - JSON, XML have good support for Unicode (human readable text) but not binary strings. Binary strings are useful (such as picture) so people get around by encoding the binary data as text using Base64. This works but increases the data size by 33% - There is optional schema support for both XML [11] and JSON [12]. These schema languages are quite powerful, and thus quite complicated to learn and implement. - CSV does not have any schema, so it is up to the application to define the mean\u2010 ing of each row and column. If an application change adds a new row or column, you have to handle that change manually. CSV is also a quite vague format (what happens if a value contains a comma or a newline character?).</p> <p>\u5373\u4f7f\u6709\u8fd9\u4e9b\u4e0d\u8db3\uff0cJSON, XML, CSV \u5728\u5f88\u591a\u573a\u666f\u4e0b\u8db3\u591f\u7528\u4e86</p>"},{"location":"Chapter%204/#binary-encoding","title":"Binary encoding","text":"<p>\u5982\u679c\u53ea\u5728\u5185\u90e8\u4f7f\u7528 JSON \u6216\u8005 XML\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u7528 binary \u7684\u65b9\u5f0f(BSON, BJSON, UBJSON, BISON, WBXML, Fast Infoset) \u6765\u5b58\u50a8\u548c\u4f20\u8f93\uff0c\u56e0\u4e3a\u6570\u636e\u8d8a\u5927 binary \u8282\u7701\u7684\u7a7a\u95f4\u5c31\u8d8a\u591a \u4e66\u91cc\u8981\u7528\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\u6765\u770b binary encoding <pre><code>{\n    \"userName\": \"Martin\",\n    \"favoriteNumber\": 1337,\n    \"interests\": [\"daydreaming\", \"hacking\"]\n}\n</code></pre></p> <p></p> <ol> <li>0x83 \u75280/1 \u8868\u793a\u51fa\u6765\u662f <code>1000 0011</code> \u4e5f\u5c31\u662ftop 4 bit indicate it is an object with 3 fields. (In case you\u2019re wondering what hap\u2010 pens if an object has more than 15 fields, so that the number of fields doesn\u2019t fit in four bits, it then gets a different type indicator, and the number of fields is encoded in two or four bytes.)</li> <li><code>0xa8</code> \u662f <code>1010 1000</code> \u8868\u793a\u63a5\u4e0b\u6765\u7684\u6570\u636e\u662f\u4e00\u4e2a \u957f\u5ea6\u4e3a8(<code>1000</code>) \u7684 string (<code>1010</code>) </li> <li>\u63a5\u4e0b\u6765\u7684 8 bytes \u5c31\u662f \u524d\u9762\u63d0\u5230\u7684 string \u7684\u6570\u636e\uff0c\u56e0\u4e3a\u5df2\u7ecf\u77e5\u9053\u4e86\u957f\u5ea6\uff0c\u8fd9\u91cc\u4e0d\u9700\u8981\u4efb\u4f55 marker \u544a\u8bc9\u6211\u4eecstring \u54ea\u91cc\u7ed3\u675f</li> <li><code>0xa6</code> \u8868\u793a\u4e00\u4e2a\u957f\u5ea6\u4e3a 6 \u7684 string </li> </ol> <p>The binary encoding is 66 bytes long, which is only a little less than the 81 bytes taken by the textual JSON encoding (with whitespace removed). All the binary encodings of JSON are similar in this regard. It\u2019s not clear whether such a small space reduction (and perhaps a speedup in parsing) is worth the loss of human-readability.</p> <p>\u4e0b\u9762\u4e3e\u7684\u4f8b\u5b50\u662f\u5982\u4f55\u628a\u8fd9\u4e2a 66 bytes \u7684data \u7528\u4ec5 32 bytes \u8868\u793a\u51fa\u6765</p>"},{"location":"Chapter%204/#thrift-and-protocol-buffers","title":"Thrift and Protocol Buffers","text":"<p>Apache Thrift [15] and Protocol Buffers (protobuf) [16] are binary encoding libraries that are based on the same principle. Protocol Buffers was originally developed at Google, Thrift was originally developed at Facebook, and both were made open source in 2007\u201308 [17].</p> <p>Both Thrift and Protocol Buffers require a schema for any data that is encoded. To encode the data in Example 4-1 in Thrift, you would describe the schema in the Thrift interface definition language (IDL) like this:</p> <pre><code>struct Person {\n    1: required string  userName,\n    2: optional i64     favoriteNumber,\n    3: optional list&lt;string&gt; interest\n}\n</code></pre> <p><pre><code>    message Person {\n        required string user_name= 1;\n        optional int64  favorite_number = 2;\n        repeated string interests= 3;\n\n}\n</code></pre> Thrift and protobuf both has code generation tool that takes the schema definition and produces classes that implement the schema in various programming languages. </p> <p>Application code can call this generated code to encode or decode records of the schema  </p> <p>\u8ddf figure 4.1 \u4e00\u6837\uff0c\u6bcf\u4e00\u4e2a field \u6709\u4e00\u4e2a type annotation (whether it is a string, integer, list, etc) and length indication. Strings are encoded in UTF8  The big difference is that there is no field names. (userName, favoriteNumber, interest)  Instead, the encoded data contains field tags, which are numbers (1, 2, 3) Those are numbers in schema definition </p> <p>Thrift CompactProtocol encoding is semantically equivalent to BinaryProtocol, but it packs field type and tag number into a single byte and by using variable-length integers, (use 2 bytes rather than 8 bytes for number 1337) This way it can pack this message into 34 bytes  ProtoBuf is very similar to Thrift's CompactProtocol </p>"},{"location":"Chapter%204/#field-tags-and-schema-evolution","title":"Field tags and schema evolution","text":"<p>Schemas inevitably change over time. How do Thrift and ProtoBuf handle schema changes while keeping backward and forward compactibility?  Each field is identified by its tag number and annotated with a datatype. Field tag is important to the meaning of encoded data. You can change the name of a field in the schema, since the encoded data never refers to field names, but you cannot change a field tag, since it will make all existing encoded data invalid.</p> <p>You can adding new field by giving each new field a new tag number. If old code tries to read data written by new code, including a new field with a tag number it doesn't recognize, it can simply ignore it. </p> <p>The datatype annotation allows the parser to determine how many bytes it needs to skip. This maintains forward compatibility: old code can read records written by new code</p> <p>What about backward compatibility? As long as each field has a unique tag number, new code can always read old data, because tag number don't change and thus have same meaning</p> <p>The only detail is if you add a new field, you cannot make it required since old code cannot write new data field and new code will fail upon parsing</p> <p>Thus every field you add after the initial deployment of the schema must be optional or have a default value </p> <p>Removing a field is same, you can only remove a field that is optional and never use the same tag number again </p>"},{"location":"Chapter%204/#datatypes-and-schema-evolution","title":"Datatypes and schema evolution","text":"<p>What about changing the datatype of a field? (int to string, etc)  It is certainly possible but be careful of losing precision or data get truncated. For example, change a 32bit integer to 64 bit integer. New code can easily fill the rest with 0 but old code will use 32bit to try to hold 64 bit value. If the decoded value doesn't fit in 32 bits, it will be truncated.</p> <p>Protobuf uses repeated instead of array datatype. This has nice effect that it's okay to change an <code>optional</code> to <code>repeated</code>. New code reading old data sees a 0 or 1 elements and old code read new data see only the last element of the list</p> <p>Thrift has list type which is parameterized with the datatype of the list elements. This doesn't allow evolution from single valued to multivalued datatype but it has advantage of nested lists </p>"},{"location":"Chapter%204/#avro","title":"Avro","text":"<p>Apache Avro is another binary encoding format. Started as a subproject of Hadoop Avro also uses schema and has 2 schema languages  1. Avro IDL intended for human editing 2. Based on JSON that is more easily machine readable <pre><code>record Person {\n    string userName;\n    union {null, long} favoriteNumber = null;\n    array&lt;string&gt; interests;\n}\n</code></pre></p> <p>equivalent JSON  <pre><code>{\n    \"type\": \"record\",\n    \"name\": \"Person\",\n    \"field\": [\n        {\"name\": \"userName\", \"type\": \"string\"},\n        {\"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null},\n        {\"name\": \"interests\", \"type\": \"type\": {\"type\": \"array\", \"items\": \"string\"}},\n    ]\n}\n</code></pre></p> <p>no tag number in Avro's schema, but Avro binary encoding is just 32 bytes   Nothing to identify fields or their datatypes. The encoding simply consists of values concatenated together. Integer is encoded using a variable-length encoding</p> <p>The parse the binary data, you need to go through the fields in the order they appear in the schema. This means the binary data can only be decoded correctly if the code reading the data is using exact same schema as the code that wrote the data.  So how does Avro support schema evolution? </p>"},{"location":"Chapter%204/#writers-schema-and-reads-schema","title":"Writer's schema and read's schema","text":"<p>Avro encodes the data using whatever version it knows about. This is known as writer's schema</p> <p>When an application want to decode some data (read from file or database, receive from network etc), it is expecting the data to be in some schema, which is known as reader's schema The key idea with Avro is that writer's schema and reader's schema don't have to be the same. They only need to be compatible. </p> <p>Avro library resolves the differences by looking at the writer\u2019s schema and the reader\u2019s schema side by side and translating the data from the writer\u2019s schema into the reader\u2019s schema. The Avro specification [20] defines exactly how this resolution works, and it is illustrated in Figure 4-6.</p> <p> order doesn't matter because schema resolution matches up by the fields name  If writer schema has a field that is not showing in reader's schema, reader's schema will fill it with default value</p>"},{"location":"Chapter%204/#schema-evolution-rules","title":"Schema evolution rules","text":"<p>With Avro, forward compatibility means you can have newer version of the schema as writer and older version of the schema as reader. And reverse, backward compatibility means new version of the schema as reader and an old version as writer </p> <p>To maintain compatibility, you may only add or remove a field that has a default value. </p> <p>In Avro, if you want to allow a field to be null, you have to use a union type. For example, <code>union { null, long, string }</code> field indicates that field can be a number, or a string, or null. You can only use null as a default value if it is one branch of the union. This is verbose but help prevent some bugs</p> <p>So Avro doesn't have <code>optional</code> and <code>required</code> markers like ProtoBuf or Thrift </p> <p>There is an important question that we\u2019ve glossed over so far: how does the reader know the writer\u2019s schema with which a particular piece of data was encoded? We can\u2019t just include the entire schema with every record, because the schema would likely be much bigger than the encoded data, making all the space savings from the binary encoding futile.</p> <p>Depends on the context. given few examples Large file with lots of records</p> <p>A common use for Avro\u2014especially in the context of Hadoop\u2014is for storing a large file containing millions of records, all encoded with the same schema. (We will discuss this kind of situation in Chapter 10.) In this case, the writer of that file can just include the writer\u2019s schema once at the beginning of the file. Avro specifies a file format (object container files) to do this.</p> <p>Database with individually written records</p> <p>In a database, different records may be written at different points in time using different writer\u2019s schemas\u2014you cannot assume that all the records will have the same schema. The simplest solution is to include a version number at the beginning of every encoded record, and to keep a list of schema versions in your database.  A reader can fetch a record, extract the version number, and then fetch the writer\u2019s schema for that version number from the database. Using that writer\u2019s schema, it can decode the rest of the record.</p> <p>Sending records over a network connection</p> <p>When two processes are communicating over a bidirectional network connection, they can negotiate the schema version on connection setup and then use that schema for the lifetime of the connection.</p>"},{"location":"Chapter%204/#dynamically-generated-schemas","title":"Dynamically generated schemas","text":"<p>Avro \u6ca1\u6709 tag number \u662f\u4e00\u79cd\u4f18\u52bf\uff0c \u56e0\u4e3a\u4ed6\u53ef\u4ee5\u66f4\u5bb9\u6613\u5b9e\u73b0 dynamically generate schemas\u3002 \u6bd4\u5982\u4f60\u60f3\u628a\u4e00\u4e2a relational database \u7684\u5185\u5bb9\u7528binary \u683c\u5f0f dump \u5230\u4e00\u4e2a file \u91cc\u9762\uff0cAvro \u53ef\u4ee5\u6839\u636e relational schema generate Avro schema \u7136\u540e\u76f4\u63a5 dump \u5230\u4e00\u4e2a Avro object container file \u91cc\u9762\uff0c column name match field name in Avro (reference)</p> <p>\u4e0e\u4e4b\u76f8\u5bf9\u7684\uff0cthrift and protobuf \u5219\u9700\u8981\u624b\u52a8\u7ed9\u4e00\u4e2a field tag. \u6bcf\u4e00\u6b21 database schema changes, \u4e00\u4e2a admin \u5c31\u9700\u8981\u624b\u52a8\u66f4\u65b0 mapping from database column names to field tags. </p>"},{"location":"Chapter%204/#code-generation-and-dynamically-typed-languages","title":"Code generation and dynamically typed languages","text":"<p>Thrift and protobuf's code generation is helpful for statically typed languages but not much useful in dynamically typed languages </p> <p>Avro provides optional code generation for statically typed programming languages, but it can be used just as well without any code generation. If you have an object con\u2010 tainer file (which embeds the writer\u2019s schema), you can simply open it using the Avro library and look at the data in the same way as you could look at a JSON file. The file is self-describing since it includes all the necessary metadata.</p> <p>This property is especially useful in conjunction with dynamically typed data pro\u2010 cessing languages like Apache Pig [26]. In Pig, you can just open some Avro files, start analyzing them, and write derived datasets to output files in Avro format without even thinking about schemas.</p>"},{"location":"Chapter%204/#the-merits-of-schemas","title":"The Merits of Schemas","text":"<p>ProtoBuf, Thrift, and Avro all use schema for encoding. </p> <p>\u8fd9\u4e3b\u610f\u4e5f\u4e0d\u662f\u65b0\u53d1\u660e\u7684</p> <p>The ideas on which these encodings are based are by no means new. For example, they have a lot in common with ASN.1, a schema definition language that was first standardized in 1984 [27]. It was used to define various network protocols, and its binary encoding (DER) is still used to encode SSL certificates (X.509), for example [28]. ASN.1 supports schema evolution using tag numbers, similar to Protocol Buffers and Thrift [29] However, it\u2019s also very complex and badly documented, so ASN.1 is probably not a good choice for new applications.</p> <p>Compare to text based encoding, binary encoding have a number of nice properties: - They can be much more compact than the various \u201cbinary JSON\u201d variants, since they can omit field names from the encoded data.</p> <ul> <li> <p>The schema is a valuable form of documentation, and because the schema is required for decoding, you can be sure that it is up to date (whereas manually maintained documentation may easily diverge from reality).</p> </li> <li> <p>Keeping a database of schemas allows you to check forward and backward com\u2010 patibility of schema changes, before anything is deployed.</p> </li> <li> <p>For users of statically typed programming languages, the ability to generate code from the schema is useful, since it enables type checking at compile time.</p> </li> </ul> <p>In summary, schema evolution allows the same kind of flexibility as schemaless/ schema-on-read JSON databases provide</p>"},{"location":"Chapter%204/#modes-of-dataflow","title":"Modes of Dataflow","text":"<p>\u6bcf\u5f53\u4f60\u4ece\u4e00\u4e2a process \u7ed9\u53e6\u4e00\u4e2a\u4e0d share memory\u7684 process \u53d1\u6570\u636e\u7684\u65f6\u5019 (\u6bd4\u5982\u5199\u5165\u4e00\u4e2a\u6587\u4ef6\u6216\u8005\u7f51\u7edc\u4f20\u8f93)\uff0c\u4f60\u5c31\u9700\u8981 encode data as sequence of bytes, \u4e0a\u9762\u5df2\u7ecf\u8ba8\u8bba\u4e86\u4e0d\u540c\u7684encoding \u7684\u65b9\u5f0f\u4e86</p> <p>\u800c\u6570\u636e\u4ece\u4e00\u4e2a process flow \u5230\u53e6\u4e00\u4e2a process \u6709\u5f88\u591a\u4e0d\u540c\u7684\u65b9\u5f0f\uff0cbelow introduce most common ways of how data flow through one process to another - Via Database - Via service calls - Via async message passing</p>"},{"location":"Chapter%204/#dataflow-through-databases","title":"Dataflow Through Databases","text":"<p>In Database, the process that writes to database encodes the data, and process that reads from database decodes it. There may be just one process, in this case you can think of this as sending a message to your future self.</p> <p>Backward compatibility is clearly necessary here; otherwise your future self won't be able to decode what you previously wrote. \u5c31\u597d\u50cf\u62c9\u4e01\u6587\u4e00\u6837\uff0c\u5982\u679c\u6ca1\u6709\u5bf9\u7167\u4e66\u7c4d\uff0c\u5927\u90e8\u5206\u4eba\u5df2\u7ecf\u770b\u4e0d\u61c2\u4e86</p> <p>In general, it\u2019s common for several different processes to be accessing a database at the same time.</p> <p>\u4e0a\u9762\u63d0\u5230\u7684 rolling upgrade \u5c31\u4f1a\u8ba9\u4e0d\u540c\u7684 process access \u4e00\u4e2adatabase \u5e76\u4e14 newer code maybe write to database where older code reads it</p> <p>the desirable behavior is usually for the old code to keep the new field intact, even though it couldn\u2019t be interpreted.</p> <p>\u8001\u4ee3\u7801\u5bf9\u65b0\u7684 field \u4e0d\u8fdb\u884c\u6539\u52a8\uff0c\u800c\u4e14 encoding \u7684\u65f6\u5019\u8981\u5c0f\u5fc3\uff0c\u4e0d\u7136\u7684\u8bdd\u6709\u53ef\u80fd\u4e22\u5931\u6570\u636e </p>"},{"location":"Chapter%204/#different-values-written-at-different-times","title":"Different values written at different times","text":"<p>DB \u901a\u5e38\u63a5\u53d7\u4efb\u4f55\u65f6\u95f4\u7684\u5199\u5165\u8bf7\u6c42\uff0c\u8fd9\u610f\u5473\u7740\u4f60\u6570\u636e\u5e93\u91cc\u53ef\u80fd\u6709 5 ms \u6216\u8005 5 \u5e74\u524d\u7684\u6570\u636e\u3002\u5982\u679c\u4f60deploy new version of your application (server-side), \u4f60\u51e0\u5206\u949f\u4e4b\u5185\u53ef\u80fd\u5c31\u628a\u65b0\u7248\u672c\u90e8\u7f72\u4e86\uff0c\u4f46\u662fDB \u91cc\u9762\u7684\u6570\u636e\u8fd8\u5728\u90a3\u91cc\uff0c\u9664\u975e\u4e00\u4e2a5\u5e74\u524d\u7684 record \u521a\u597d\u88ab\u91cd\u5199\u8fc7\uff0c\u4e0d\u7136\u8fd8\u662f 5\u5e74\u524d\u7684 schema\u3002\u8fd9\u79cd\u73b0\u8c61\u4e5f\u53eb data outlives code</p> <p>Rewriting (migrating) data into a new schema is certainly possible, but it\u2019s an expen\u2010 sive thing to do on a large dataset, so most databases avoid it if possible. Most rela\u2010 tional databases allow simple schema changes, such as adding a new column with a null default value, without rewriting existing data. ... Schema evolution thus allows the entire database to appear as if it was encoded with a single schema, even though the underlying storage may contain records encoded with various historical versions of the schema.</p> <p>\u91cd\u5199\u6570\u636e\u5f53\u7136\u53ef\u884c\u4f46\u662f\u4ee3\u4ef7\u592a\u9ad8\uff0cschema evolution \u5219\u66f4\u5b9e\u7528\uff0c\u66f4\u73b0\u5b9e</p> <p>LinkedIn\u2019s document database Espresso uses Avro for storage, allowing it to use Avro\u2019s schema evolution rules</p> <p>\u539f\u6765 linkedin \u7528 Avro\uff0c\u6211\u8bf4 Martin \u600e\u4e48\u8fd9\u4e48\u559c\u6b22 Avro </p>"},{"location":"Chapter%204/#archival-storage","title":"Archival storage","text":"<p>Perhaps you take a snapshot of your database from time to time, say for backup pur\u2010 poses or for loading into a data warehouse</p> <p>\u65f6\u4e0d\u65f6\u7ed9 DB \u505a\u4e00\u6b21\u5907\u4efd\uff0c\u800c\u8fd9\u4e00\u4e2a\u65f6\u95f4\u70b9\u7684\u5907\u4efd\u4e5f\u610f\u5473\u7740\u5f53\u65f6\u7684 schema \u8ddf\u7740\u88ab\u5907\u4efd\u4e86\uff0c\u6240\u4ee5\u7528\u6700\u65b0\u7684 schema \u5907\u4efd\u66f4\u597d</p>"},{"location":"Chapter%204/#dataflow-through-services-rest-and-rpc","title":"Dataflow Through Services: REST and RPC","text":"<p>\u5f53\u4e00\u4e2a process \u8981\u901a\u8fc7\u7f51\u7edc\u4f20\u8f93\u6570\u636e\u7684\u65f6\u5019\uff0c\u4e5f\u6709\u5f88\u591a\u79cd\u65b9\u6cd5\u53ef\u4ee5\u9009\u62e9\u3002\u6700\u5e38\u89c1\u7684\u662f clients and servers. Server \u63d0\u4f9b\u4e00\u4e2a API \u7136\u540e client \u53ef\u4ee5\u7528\u8fd9\u4e2a API \u53d1\u9001\u8bf7\u6c42\uff0c\u8fd9\u4e2a server \u63d0\u4f9b\u7684 API \u4e5f\u53eb\u505a service </p> <p>web works this way: clients (web browsers) make requests to web servers, making <code>GET</code> requests to download HTML, CSS, JS, images etc. Because browsers, server agree on this standard, you could use any browser to access any website. </p> <p>Web browser is not only type of client. Mobile device or desktop computer can also make network request to a server. Client side JavaScript application running inside the browser can use XMLHttpRequest to become a HTTP client (this technique is known as Ajax) In this case, server response is typically not HTTP but encoding that is convenient for client side application to process (JSON for example) </p> <p>A server can itself be a client to another service. Which mean it make request to other service to get some data it needed. This is also called service oriented architecture (SOA) or microservices architecture</p> <p>service are similar to database: they allow clients to submit and query data. But service expose application specific API rather than query language that can query any data. </p> <p>This allows inputs and outputs predetermine by the business logic  Key goal of service-oriented/microservices architecture is to make application easier to change and maintain</p>"},{"location":"Chapter%204/#web-services","title":"web services","text":"<p>When HTTP is used as protocol for talking to a service, it is called web service. This is misleading because web services are used in many other context - A client application running on user's device (mobile, JavaScript web app using Ajax) making requests to a service over HTTP. These request go over public internet - One service make requests to another service own by the same organization. (often in same datacenter) As part of service-oriented architecture (software support those use case is sometimes called middleware) - One service making requests to a service owned by a different organization via internet. (data exchange between different organization backend system) This include public APIs provided by online services, such as credit card processing systems, or OAuth for shared access to user data. REST and SOAP are 2 popular approaches </p> <p>REST is not a protocol but a design philosophy that builds upon HTTP. It emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication, and content type negotiation. </p> <p>REST is gaining popularity compared to SOAP and is often associated with microservices. An API designed according to the principles of REST is called RESTful</p> <p>SOAP is an XML-based protocol for making network API requests. Although it is most commonly used over HTTP, it aims to be independent from HTTP. </p> <p>The API of SOAP web service is described using an XML based language called the Web Services Description Language (WSDL) WSDL enables code generation so that a client can access a remote service using local classes and method calls</p> <p>WSDL is not designed to be human-readable, SOAP messages are often too complex to construct manually. So it rely heavily on tool support. For users of programming languages that are not supported SOAP vendors, integration with SOAP is difficult \u8fd9\u4e5f\u662f\u4e3a\u4ec0\u4e48 JSON \u8fd9\u4e48\u6d41\u884c\u4e86\u5427\uff0c browser natively support and easy to construct by hand</p> <p>RESTful APIs tend to favor simpler approaches, typically involving less code genera\u2010 tion and automated tooling. A definition format such as OpenAPI, also known as Swagger [40], can be used to describe RESTful APIs and produce documentation.</p>"},{"location":"Chapter%204/#problems-with-remote-procedure-calls","title":"Problems with remote procedure calls","text":"<p>Remote procedure call \u6709\u5f88\u591a\u5c40\u9650\u6027\uff0cEnterprise Java Bean (EJB), Java's Remote Method Invocation \u5c40\u9650\u4e8e Java\uff0cDistributed Component Object Model (DCOM) \u5c40\u9650\u4e8e microsoft. RPC \u8bde\u751f\u4e8e 1970s\uff0c\u76ee\u7684\u5c31\u662f\u4e3a\u4e86\u901a\u8fc7\u7f51\u7edc\u80fd\u50cf\u5728\u672c\u5730\u4e00\u6837 call \u4e00\u4e2a function\uff0c\u4f46\u8fd9\u4e2a\u4e3b\u610f\u4ece\u6839\u672c\u4e0a\u5c31\u6709\u95ee\u9898\uff0c\u4e3b\u8981\u8fd8\u662f\u56e0\u4e3a\u7f51\u7edc\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u3002\u6574\u4e2a distributed system \u90a3\u4e48\u590d\u6742\u5c31\u662f\u56e0\u4e3a\u7f51\u7edc\u7684\u95ee\u9898\u2026\u2026 - local function either succeed for fail. RPC might fail due to network issue, machine being slow  - Local function either return result, throws exception, never returns. Network request might time out which you don't know what happened. Chapter 8 \u4f1a\u8be6\u7ec6\u8bb2 - Retry on network request needs to be idempotent (deduplication or result is always the same) Chapter 11 will talk more about this - Network is slower than a local function call.  - Local function call just use memory where network call needs to encode data into sequence of bytes  - Client and service might be in different language so RPC framework must translate datatypes from one language into another</p>"},{"location":"Chapter%204/#current-directions-for-rpc","title":"Current directions for RPC","text":"<p>RPC isn't going away. Thrift and Avro come with RPC support. gRPC is an RPC implementation using protobuf</p> <p>New generation of RPC framework use futures (promises) to encapsulate asynchronous action that may fail. gRPC supports streams where a call consists of not just one request and one response but series of request and response</p> <p>Some framework provide service discovery which allow client to find out which IP address and port number it can find a particular service. </p> <p>Binary encoding achieve better performance where RESTful is good for experimentation and debugging </p> <p>(you can simply make requests to it using a web browser or the command-line tool curl, without any code generation or software installation), it is supported by all main\u2010 stream programming languages and platforms, and there is a vast ecosystem of tools available (servers, caches, load balancers, proxies, firewalls, monitoring, debugging tools, testing tools, etc.).</p> <p>public API often use REST API where main focus of RPC is services owned by same organization (typically within same datacenter)</p>"},{"location":"Chapter%204/#data-encoding-and-evolution-for-rpc","title":"Data encoding and evolution for RPC","text":"<p>\u8ddf database \u4e00\u6837\uff0c RPC \u7684 client \u548c server \u4e5f\u9700\u8981deployed independently. \u4e0d\u8fc7 RPC \u76f8\u5bf9\u6765\u8bf4\u66f4\u5bb9\u6613\u4e00\u4e9b\uff0c\u5047\u8bbe server \u90fd\u5148 update\u4e86\uff0cclient \u5176\u6b21\uff0c\u90a3\u4e48\u6211\u4eec\u53ea\u9700\u8981\u5728 request \u7684\u65f6\u5019\u8003\u8651backward compatibility\uff0c forward compatibility on response</p> <p>RPC scheme are inherited from whatever encoding it is used - Thrift, gRPC, Avro can be evolved based on the compatibility rules - SOAP are specified with XML schemas  - RESTful API commonly use JSON or URI encoded request parameters for request. </p> <p>Service compatibility is made harder by the fact that RPC is often used for communi\u2010 cation across organizational boundaries, so the provider of a service often has no control over its clients and cannot force them to upgrade. Thus, compatibility needs to be maintained for a long time, perhaps indefinitely. If a compatibility-breaking change is required, the service provider often ends up maintaining multiple versions of the service API side by side.</p> <p>public API \u901a\u5e38\u662f\u9700\u8981\u591a\u4e2a\u7248\u672c\u540c\u65f6\u652f\u6301\u7684, \u56e0\u4e3a\u4ed6\u4eec\u6ca1\u6cd5\u63a7\u5236 client \u4ec0\u4e48\u65f6\u5019\u66f4\u65b0 \u8bf4\u8fd9\u8ba9client \u53d1\u9001\u8bf7\u6c42\u7684\u65f6\u5019\u7ed9\u51fa version number </p> <p>There is no agreement on how API versioning should work (i.e., how a client can indicate which version of the API it wants to use [48]). For RESTful APIs, common approaches are to use a version number in the URL or in the HTTP Accept header. For services that use API keys to identify a particular client, another option is to store a client\u2019s requested API version on the server and to allow this version selection to be updated through a separate administrative interface</p>"},{"location":"Chapter%204/#message-passing-dataflow","title":"Message-Passing Dataflow","text":"<p>\u6211\u4eec\u5df2\u7ecf\u770b\u4e86\u4fe1\u606f\u662f\u5982\u4f55\u4ece\u4e00\u4e2a process flow \u5230\u53e6\u4e00\u4e2a process\u4e86\uff0c\u5176\u4e2d\u5305\u62ec REST and RPC (network that expect response as soon as possible) \u4ee5\u53ca database (one process write encoded data, another process reads it in future)</p> <p>this section will talk about asynchronous message-passing systems which are somewhere between RPC and databases</p> <p>They are similar to RPC where client sends a request called message  They are similar to database in that message is not sent via direct network connection, but via an intermediary called message broker (also called message queue or message oriented middleware) which stores the message temporarily </p> <p>Message broker \u5bf9\u4e8edirect RPC \u6709\u51e0\u4e2a\u597d\u5904 - Act as buffer if recipient is unavailable or overloaded  - It can auto redeliver messages to a process that has crashed (prevent message being lost) - Avoids sender needing to know the IP address and port number of the recipient (useful in cloud where VM come and go) - Allows one message send to several clients - logically decouples. the sender from recipient </p> <p>Message passing communication is usually one-way: sender don't expect receiving a reply from recipient. </p> <p>This communication pattern is asynchronous: the sender doesn\u2019t wait for the message to be delivered, but simply sends it and then forgets about it.</p>"},{"location":"Chapter%204/#message-brokers","title":"Message brokers","text":"<p>\u4ee5\u524d message broker \u90fd\u662f\u5546\u7528\u8f6f\u4ef6\u4e3b\u5bfc\u7684 TIBCO, IBM etc. \u6700\u8fd1\u5219\u662f\u5f00\u6e90\u66f4\u52a0\u6d41\u884c RabbitMQ, ActiveMQ, HornetQ, NETS, Kafka</p> <p>details vary but in general, message brokers are used as follows: 1 process sends a message to a named queue or topic, the broker ensures that the message is delivered to 1 or more consumers or subscribers to that queue or topic. There can be many producers and consumers on same topic </p> <p>Topic is one-way dataflow. however, consumer itself may publish message to another topic (you can chain them together) or to a reply queue for sender (request/response dataflow similar to RPC)</p>"},{"location":"Chapter%204/#distributed-actor-frameworks","title":"Distributed actor frameworks","text":"<p>The actor model is a programming model for concurrency in a single process. Rather than dealing with threads (which is associated with lock, deadlock, racing problem), logic is encapsulated in actors. Each actor represents 1 client or entity. It may have some local state and it communicates with other actors by sending and receiving async messages. Since actor process 1 message at a time, it doesn't need to worry about threads. </p> <p>In distributed actor frameworks, this programming model is used to scale an application across multiple nodes. Same mechanism is used, no matter whether the sender and recipient are on the same node or different nodes. </p> <p>A distributed actor framework essentially integrates a message broker and actor programming model into a single framework. </p> <p>3 popular distributed actor frameworks handle encoding as follows:  - Akka uses Java\u2019s built-in serialization by default, which does not provide forward or backward compatibility. However, you can replace it with something like Protocol Buffers, and thus gain the ability to do rolling upgrades - Orleans by default uses a custom data encoding format that does not support rolling upgrade deployments; to deploy a new version of your application, you need to set up a new cluster, move traffic from the old cluster to the new one, and shut down the old one [51, 52]. Like with Akka, custom serialization plug-ins can be used. - In Erlang OTP it is surprisingly hard to make changes to record schemas (despite the system having many features designed for high availability); rolling upgrades are possible but need to be planned carefully [53]. An experimental new maps datatype (a JSON-like structure, introduced in Erlang R17 in 2014) may make this easier in the future</p>"},{"location":"Chapter%204/#summary","title":"Summary","text":"<p>\u8fd9\u4e00\u7ae0\u6211\u4eec\u770b\u4e86\u5c06 data structures encoding into bytes(network or disk) \u7684\u4e0d\u540c\u65b9\u5f0f\u3002 encoding \u7684\u7ec6\u8282\u4e0d\u4ec5\u5f71\u54cd\u4e86\u4ed6\u4eec\u7684\u6548\u7387 (space util) \u800c\u4e14\u8fd8\u80fd\u5f71\u54cd\u4f60 application \u7684\u7ed3\u6784\u4ee5\u53ca deploy \u7684 options</p> <p>\u5f88\u591a service \u9700\u8981 rolling upgrade (new version deployed to few nodes at a time). Rolling upgrade \u7684 property \u5c31\u9700\u8981\u6211\u4eec\u652f\u6301 evolvability  - we must assume that different nodes are running the different versions of our application\u2019s code. data flowing around the system provide backward compatibility(new code can read old data) and forward compatibility (old code can read new data) \u5c31\u662f\u5982\u679c\u4f60\u7a7f\u8d8a\u5230\u672a\u6765\u4f9d\u7136\u80fd\u8bfb\u61c2\u90a3\u65f6\u5019\u7684\u6587\u5b57\uff0c\u8fd9\u5c31\u9700\u8981\u6587\u5b57\u57fa\u672c\u542b\u4e49\u4e0d\u80fd\u88ab\u5220</p> <p>\u6211\u4eec\u8ba8\u8bba\u4e86\u51e0\u79cd encoding formats \u4ee5\u53ca\u4ed6\u4eec\u7684 compatibility properties: - Programming languages - restricted to single language and bad at forward and backward compatibility - Text formats (JSON, XML, CSV) compatibility depends on how you use them and not efficient - Binary formats (Thrift, ProtoBuf, Avro) -&gt; efficient and clearly defined forward and backward compatibility semantics. Downside is data needs to be decoded before it is human readable. \u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86 data flow - Database  - RPC and REST API - Async message passing</p> <p>\u6709\u4e86\u8fd9\u4e9b\u5de5\u5177\uff0c\u6211\u4eec\u7684 application \u5c31\u53ef\u4ee5\u5b9e\u73b0 rolling upgrade (evolvability). \u5feb\u901f\u8fed\u4ee3</p>"},{"location":"Chapter%205/","title":"Chapter 5","text":""},{"location":"Chapter%205/#leaders-and-followers","title":"Leaders and Followers","text":"<p>Replication mean keep same copies on multiple machines that are connected with network</p> <p>As Mentioned in Part 2, the goals of replication are: - Reduced latency through geographically connected network - Another machine can still serve traffic while a machine is down - Balance the load to different machine when traffic spikes</p> <p>This chapter assume entire dataset can fit in a single machine. Chapter 6 will discuss about partitioning when a dataset is too big for a single machine</p> <p>All difficulty of replication comes from handling changes. single-leader, multi-leader and leaderless replication are different method/algorithms we could use </p> <p>Replication is an old topic and method hasn't changed much since 1970s. Distributed databases is more recent There are different trade offs and issues such as eventual consistency that will be discussed</p>"},{"location":"Chapter%205/#q-how-do-we-ensure-that-all-the-data-ends-up-on-all-the-replicas","title":"Q: How do we ensure that all the data ends up on all the replicas?","text":"<p>Each write needs to be processed by every replica otherwise it would not contain the same dataset anymore. So a common solution is leader-based replication (aka active/passive, master/slave replication)</p> <p></p> <p>How it works: 1.  leader process all writes and forward those write to followers. The media of forwarding writes is called replication log or change stream 2. Each replica takes the log from leader and applies the changes in same order to its local storage  3. Both leader and follower can serve read</p> <p>This mode of replication is a built-in feature of many relational databases, such as PostgreSQL (since version 9.0), MySQL, Oracle Data Guard [2], and SQL Server\u2019s AlwaysOn Availability Groups [3].</p> <p>Distributed Message queue/brokers use same method</p> <p>leader-based replication is not restricted to only databases: distributed message brokers such as Kafka [5] and RabbitMQ highly available queues [6] also use it. Some network filesystems and replicated block devices such as DRBD are similar.</p>"},{"location":"Chapter%205/#sync-vs-async","title":"Sync vs Async","text":"<p>This is a tradeoff when implementing leader based replication (configurable option in relational DB) Figure 5-2 shows communication with time flow from left to right   in this figure, follower 1 is synchronous, follower 2 is asynchronous</p> <p>advantage of synchronous updates is when leader fails, follower can stood up as leader. Disadvantage is when synchronous follower is not responding it will not complete the updates for leader (leader must block until the follower is responsive again)</p> <p>In practice is not possible to make all follower synchronous thus it is common to have 1 synchronous replica. This is also called semi-synchronous </p> <p>Asynchronous means even if all follower is fallen behind, leader can still process request which means if leader is down and not recoverable, previous processed writes are lost </p> <p>Weak durability on Asynchronous might sounds a bad trade off but it is widely used on geographically distributed data based.</p> <p>Research on Replication</p> <p>researchers have continued investigating replication methods that do not lose data but still provide good performance and availability. For example, chain replication [8, 9] is a variant of synchronous replication that has been successfully implemented in a few systems such as Microsoft Azure Storage [10, 11].</p>"},{"location":"Chapter%205/#setting-up-new-followers","title":"Setting Up New Followers","text":"<p>How do you ensure that new follower has an accurate copy of the leader's data?  1. Take a consistent snapshot of leader's database at some point in time. Most DB support this natively as they need a feature for backup 2. Copy snapshot to new follower 3. New follower connects to leader and request all data since the snapshot was taken. This requires the snapshot has a exact position in leader's replication log. This position has various names. PostgreSQL calls it log sequence number, MySQL calls it binlog coordinates 4. When follower has processed all changes since the snapshot, it is caught up </p>"},{"location":"Chapter%205/#handling-node-outages","title":"Handling node outages","text":"<p>Any node can go down, for example a node need to reboot for kernel security updates. Being able to reboot nodes without downtime is a big advantage (high availability)</p>"},{"location":"Chapter%205/#follower-failure","title":"Follower failure","text":"<p>Each follower will keep a log of data change received from leader. If a follower failed, it can just reconnect to leader and resume previous point of transaction that was processed </p>"},{"location":"Chapter%205/#leader-failure","title":"Leader failure","text":"<p>When leader fail, it needs failover which involves client reconfigure to send writes to new leader and follower connect to new leader  Failover can be manual to automatic. Automatic failover involve following steps: 1. Determine leader has failed (most system use timeout) 2. Choosing new leader (election from followers, appointed by controller node etc) this is consensus problem and will be discussed in detail in Chapter 9 3. Reconfigure the system to use the new leader (tell client to send request to new leader, and ensure if old leader comes back it stay as follower)</p> <p>Things can go wrong during failover: - With asynchronous replication, write might be lost if old leader never comes back. And if old leader does join, there will be conflict writes. Simple solution is to discard old leader's writes - Discard writes can be dangerous. Github incident when an out of date MySQL follower promoted to leader and primary key was using auto increment so there were primary keys used twice which result some private data to be disclosed to the wrong user - Two nodes both believe they are leader (called split brain). This is dangerous because write conflict can happen when 2 nodes are both accepting writes. For example, shopping cart is updated by mobile and desktop with split brain leaders then shopping cart will have different items. Solution is to shutdown 1 leader when 2 leaders are detected - What is the right timeout? </p> <p>These issues\u2014node failures; unreliable networks; and trade-offs around replica consistency, durability, availability, and latency\u2014are in fact fundamental problems in distributed systems. In Chapter 8 and Chapter 9 we will discuss them in greater depth.</p>"},{"location":"Chapter%205/#implementation-of-replication-logs","title":"Implementation of Replication Logs","text":""},{"location":"Chapter%205/#statement-based-replication","title":"Statement based replication","text":"<p>leader logs every write request and send that statement to its followers. For relational DB, it means every <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> statement is forwarded to followers </p> <p>but this can fail in various ways: - A statement calls nondeterministic function such as <code>NOW()</code> where clock is different on each machine or <code>RAND()</code> to generate a random number - They must execute in exact same order and can be limiting when multiple concurrently transaction is happening - Statements that have side effects (user-defined functions) may have different result unless these side effect are absolutely deterministic.</p>"},{"location":"Chapter%205/#write-ahead-log-shipping","title":"Write-ahead log shipping","text":"<p>In Chapter 3 we talked about WAL (doesn't matter whether its BTree or LSM tree)   We can use WAL to build a replica on another node (beside writing this log to disk, leader also sends it to its followers via network)</p> <p>This method of replication is used in PostgreSQL and Oracle</p> <p>Disadvantage of this approach is this is very low level: a WAL contains details of which bytes were changed in which disk blocks. So it is closely coupled to storage engine (BTree or LSMTree). It also means follower and leader needs to use same version of software. (This is big deal if you want to upgrade your database software without downtime)</p>"},{"location":"Chapter%205/#logical-row-based-log-replication","title":"Logical (row-based) log replication","text":"<p>Use a different log format which allows replication log decoupled from storage engine. This type of replication log is called logical log </p> <p>A logical log for relational DB is sequence of records describing writes at granularity of a row: - When insert, log contains all values of this row - When delete, log contains which row get deleted. (Typically primary key, if no primary key then all old value of the row is recorded) - When update, log contains information to uniquely identify which row/column value has updated</p> <p>MySQL\u2019s binlog (when configured to use row-based replication) uses this approach</p> <p>Since logical log is decoupled from storage engine internals, leader and follower can run on different versions of database software </p> <p>it is also easier for external application to parse. This is useful if you want send database content to external system such as data warehouse or building custom indexes and caches. This technique is called change data capture (Kafka can be used for this)</p>"},{"location":"Chapter%205/#trigger-based-replication","title":"Trigger based replication","text":"<p>Above implementation is all in database system (without involving application code) This is usually what we want There are circumstances where flexibility is needed. For example, you only want to replicate subset of data, or replicate from one kind of database to another or need conflict resolution logic. Then move replication up to application layer is a choice</p> <p>triggers and stored procedures is a feature that is available in many relational DB  Trigger lets you put custom application code that is automatically executed when data change occurs in database system (trigger can log this change into another table and external process can read this change to replicate to another system)</p> <p>Databus for Oracle [20] and Bucardo for Postgres [21] work like this</p>"},{"location":"Chapter%205/#problems-with-replication-log","title":"Problems with Replication Log","text":"<p>leader based replication is read-scaling architecture. we can add however many read replicas we want. But all writes go through leader and it realistically works with asynchronous replication (if all follower need synchronous updates, then single node failure will cause entire system stop working)</p> <p>Asynchronous means your read can be stale if the follower is fallen behind. This is temporary (when follower eventually caught up, reads from leader and follower will be same). Aka eventual consistency</p> <p>Eventually is vague (no limits how far a replica can fallen behind). Normally is within a second but if system is operating near capacity it might become several seconds or minutes </p> <p>we will talk about 3 examples of problems and discuss approaches to solve them</p>"},{"location":"Chapter%205/#reading-your-own-writes","title":"Reading your own writes","text":"<p>Many app needs user to view what they have submitted (reply in a thread, comment on a discussion etc) When data is submitted, write sent to leader and read can be done from follower. But Asynchronous replication will have a problem  we need read-after-write (aka read-your-writes) consistency in this case.  This consistency guarantee user will see their own updates (not necessary for other users)  how to implement? - Read from leader if user have modified their profile, otherwise from follower - track replication lag on followers and prevents reads on any follower has longer than 1 mins replication lag - Client can remember a timestamp for most recent write. Then system can ensure read only after that timestamp. This timestamp could be a logical timestamp (order of writes such as log sequence number)  - If your replicas distributed across multiple datacenters, then any request that needs to be served by the leader must be routed to the datacenter that contains the leader</p> <p>cross device read-after-write consistency. Desktop and mobile both want to see the same result after write happens.  - To achieve this, you need to route all user's device to the same datacenter. </p>"},{"location":"Chapter%205/#monotonic-reads","title":"Monotonic Reads","text":"<p>This is needed when asynchronous follower can cause moving backward in time This can happen when user makes several reads from different replicas.   Monotonic reads guarantee this does not happen.  How to achieve this? Make sure always read from same replica (different user can read from different replica)</p>"},{"location":"Chapter%205/#consistent-prefix-reads","title":"Consistent Prefix Reads","text":"<p>This is needed when violation of causality might happen.   consistent prefix reads prevent this anomaly. It says if a sequence of writes happens in certain order, read must also in the same order This is an anomaly in partitioned/shard databases. Different partitions operate independently, so there is no global ordering of writes </p> <p>user might see older part of a partition and new part of another partition</p> <p>One solution is to make sure any writes that are causally related goes to same partition Or explicitly track causal dependencies</p>"},{"location":"Chapter%205/#solutions-for-replication-lag","title":"Solutions for Replication lag","text":"<p>there are ways which an application can provide a stronger guarantee than underlying database (performing certain kinds of read on leader) However, dealing with this issue is complex and error prone.</p> <p>It would be better application developer didn't have to worry about replication issue and trust database \"do the right thing\". transactions exist for this reason. </p> <p>Single node transactions existed for a long time but many distributed databases systems have abandoned them. Chapter 7 and 9 will return to transactions</p>"},{"location":"Chapter%205/#multi-leader-replication","title":"Multi-Leader Replication","text":"<p>Multi leader replication allow more than 1 node to process writes. </p>"},{"location":"Chapter%205/#use-cases-for-multi-leader-replication","title":"Use Cases for Multi-Leader Replication","text":"<p>Multi leader is often in multiple data centers (so you can tolerate failure of entire datacenter, or in order to be closer to your users) </p> <p>Each datacenter have a leader. Within each datacenter, leader-follower replication is used; between datacenters, leader replicates its changes to the leader in other datacenters  Comparing single leader with multi leader</p> Metric Single leader Multi leader Performance significant latency as write must go to single datacenter where leader resides lower latency as every write can be processed in local datacenter and replicated asynchronously to other datacenter Tolerance of datacenter outage Needs another follower from different datacenter to caught up can continue operating independently Tolerance of network problem sensitive to inter-datacenter link because write needs to be done synchronously to another datacenter usually tolerate network problem better because asynchronous updates <p>Some databases support multi-leader configurations by default, but it is also often implemented with external tools, such as Tungsten Replicator for MySQL [26], BDR for PostgreSQL [27], and GoldenGate for Oracle [19].</p> <p>There are many small pitfalls in multi leader config. So it is often considered dangerous territory that should be avoided if possible [28].</p>"},{"location":"Chapter%205/#below-are-use-cases","title":"below are use cases","text":""},{"location":"Chapter%205/#clients-with-offline-operation","title":"Clients with offline operation","text":"<p>cross device application with offline operation (Calendar, notes, reminder etc)  In this case, client's local database acts as leader and there is asynchronous multi leader replication process (sync) between the replicas of your calendar on all of your device</p> <p>The replication lag maybe hours or even days. From architectural point of view, each device is a \"data center\". </p> <p>There are tools that aim to make this kind of multi-leader configuration easier. For example, CouchDB is designed for this mode of operation [29].</p>"},{"location":"Chapter%205/#collaborative-editing","title":"Collaborative editing","text":"<p>Google Docs is a good example. When one user made a change, it is applied immediately in their web browser or client app. And asynchronously replicated to the server and other users </p> <p>one way to prevent conflict is get lock at user's cursor. But usually conflict resolution is needed for this type of application </p>"},{"location":"Chapter%205/#handling-write-conflicts","title":"Handling Write Conflicts","text":"<p>When write conflicts occur, conflict resolution is needed. </p>"},{"location":"Chapter%205/#sync-vs-async-conflict-detection","title":"Sync vs Async conflict detection","text":"<p>If use Sync conflict detection you may go ahead using single leader because you need to wait for all leader to confirm there is no conflict detected</p>"},{"location":"Chapter%205/#conflict-avoidance","title":"Conflict avoidance","text":"<p>Application can make sure all writes for a particular record go through the same leader. For example, in an app where user can edit their own data, you can always route to same datacenter and use leader of that datacenter for read and writes (perhaps pick closest geo location for datacenter)</p> <p>but if user moved or the home datacenter has failed, you still have to face with conflict resolution</p>"},{"location":"Chapter%205/#converging-toward-a-consistent-state","title":"Converging toward a consistent state","text":"<p>Database must resolve conflict in a convergent way, which means all replicas must arrive at the same final value when all writes are processed. There are couple of ways to achieve this: - Give each write a unique ID(timestamp, UUID, hash of key and value) and pick write with highest ID as winner (this is known as last write wins) but this approach is prone to data loss - Give each replica a unique ID, let higher numbered replica take precedence over lower replica (this approach also implies data loss) - Somehow merge the values together -- concatenate or sort them in some order - Record the conflict in an explicit data structure that stores everything and let application code or user to resolve the conflict (git)</p>"},{"location":"Chapter%205/#custom-conflict-resolution-logic","title":"Custom conflict resolution logic","text":"<p>Most multi leader replication tool let you write conflict resolution logic using application code  On write  When DB system detects conflict in the log of replicated changes, it calls conflict handler  On read When conflict is detected, all the conflict writes are stored. When data is read next time, all versions of the data are returned to the application. The application may prompt user or auto resolve the conflict , and write back to database. CouchDB works this way</p> <p>There has been some interesting research into automatically resolving conflicts caused by concurrent data modifications. A few lines of research are worth mention\u2010 ing: \u2022 Conflict-free replicated datatypes (CRDTs) [32, 38] are a family of data structures for sets, maps, ordered lists, counters, etc. that can be concurrently edited by multiple users, and which automatically resolve conflicts in sensible ways. Some CRDTs have been implemented in Riak 2.0 [39, 40]. \u2022 Mergeable persistent data structures [41] track history explicitly, similarly to the Git version control system, and use a three-way merge function (whereas CRDTs use two-way merges). \u2022 Operational transformation [42] is the conflict resolution algorithm behind col\u2010 laborative editing applications such as Etherpad [30] and Google Docs [31]. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute a text document.</p>"},{"location":"Chapter%205/#multi-leader-replication-topologies","title":"Multi-Leader Replication Topologies","text":"<p>Replication topology describe the write path from one node to another. With more than 2 leaders, there are many topologies possible  </p> <p>Most general is all to all. MySQL use circular topology by default. Circular topology is one node received writes and forward those writes with its own writes together. Another topology is star where one node forwards writes to all other nodes (can be generalized to a tree)</p> <p>=&gt; this root node in star topology is like single leader. What happen if it fails?</p> <p>A problem with circular and star topologies is that if just one node fails, it can interrupt the flow of replication messages between other nodes, causing them to be unable to communicate until the node is fixed.</p> <p>all to all has issues if network link is not at the same speed, which result the write order arrives differently   This results to a problem of causality(writes appear in the wrong order) To fix this issue, a technique called version vectors is used (discussed later)</p> <p>However, conflict detection techniques are poorly implemented in many multi-leader replication systems. For example, at the time of writing, PostgreSQL BDR does not provide causal ordering of writes [27], and Tungsten Replicator for MySQL doesn\u2019t even try to detect conflicts [34].</p> <p>so company have in house version of conflict resolution </p>"},{"location":"Chapter%205/#leaderless-replication","title":"Leaderless Replication","text":"<p>Replication without a leader (i.e. any replica can take writes from client) is called leaderless. Some implementation let client directly send writes to multiple replicas and some implementation let client send write to a coordinator node. Coordinator does not enforce write order. This difference in design has profound consequences for the way database is used.</p>"},{"location":"Chapter%205/#writing-to-the-database-when-a-node-is-down","title":"Writing to the Database When a Node Is Down","text":"<p>Imagine you have a database system with 3 replicas, and one of them is down (system updates or whatever). In leader based approach you need to perform a fail over  On the other hand, leaderless configuration does not have a failover concept   Client can directly send write to all 3 replicas and if 2 respond with ok then we consider it succeeded. </p> <p>You may get stale read if the failed node comes back. To solve this, when performing read, client don't send read to 1 replica but send to several replicas in parallel. Version number is used to determine which value is newer </p>"},{"location":"Chapter%205/#how-does-replica-catch-up-the-writes","title":"How does replica catch up the writes?","text":"<p>2 mechanism used in Dynamo style datastores Read repair In above graph, user2345 can tell replica 3 has old value so it send newer data to it for update. this works well for values that are frequently read Anti-entropy process Let a background process to constantly look for difference between replicas and update if needed. This is different than replication log in leader based approach where write order is not guaranteed and may have significant delay for updates</p>"},{"location":"Chapter%205/#quorums","title":"Quorums","text":"<p>Generalize above example, if there are n replicas, every write must be processed by w number of replica in order to be consider successful, and we must query at least r nodes for each read. (In above example, \\(n = 3, w = 2, r = 2\\)) As long as \\(w + r &gt; n\\) , we expect to get up to date value upon read. </p> <p>Read and writes obey above rule are called quorum read and writes. In Dynamo style databases, n, w, r are configurable. A typical choice is to select a odd number for n and set \\(w = r = (n + 1) / 2\\) (round up) </p> <p>But you can change the configuration based on need. For example, few writes and many read may benefit from setting \\(w = n, r = 1\\). This makes read faster with trade off where 1 nodes fail will cause the system not accepting writes </p> <p>With \\(w + r &gt; n\\), we can tolerate unavailable nodes as follows: - if \\(w &lt; n\\), we can still process writes if some nodes are down - If \\(r &lt; n\\), we can still process read if some nodes are down  If fewer than required w and r are available, operation will return error</p>"},{"location":"Chapter%205/#limitations-of-quorum","title":"Limitations of Quorum","text":"<p>Quorum works because write set of nodes overlap with read set of nodes.  w and r are set to majority so at least 1 node overlap. But this is not necessary, other assignment are possible to allow flexibility</p> <p>you may set \\(w + r \\le n\\), this configuration allows you continue processing reads and writes even if large number of replica is down with tradeoff for getting stale read</p> <p>Even with \\(w + r &gt; n\\) you might still get stale value depend on implementation  - If a sloppy quorum is used, the w writes may end up on different nodes than the r reads - If write happens concurrently, it is not clear which happened first - If write happened concurrently with read, it is underdetermined if read returns old or new value - Even if everything is working correctly, there are edge cases in which you can get unlucky with the timing</p> <p>quorum with w and r parameter can adjust the probability of stale read but it's wise not to take them as absolute guarantees.  In particular you do not get reading your writes, monotonic reads, or consistent prefix reads</p> <p>Stronger guarantees generally require transactions or consensus.</p>"},{"location":"Chapter%205/#monitoring","title":"Monitoring","text":"<p>leader based replication can use replication lag to monitor (current write - previous write received) With leaderless, there is not write order. There are research in this area but has not become common practice. </p>"},{"location":"Chapter%205/#sloppy-quorums-and-hinted-handoff","title":"Sloppy Quorums and Hinted Handoff","text":"<p>Appropriate configuration for quorum can tolerate failure without failover, it can also tolerate individual node going slow. Because only w or r response is needed </p> <p>This makes leaderless replication appealing for application that needs high availability and low latency </p> <p>However, quorum mentioned above does not tolerate fault well. A network interruption can easily cut client from majority of the nodes</p> <p>Even those nodes are alive, but to the client that are cut off from them they appears to be dead. </p> <p>In a large cluster if client can connect to some database nodes (not in n set of nodes), database designer face a trade-off:  - is it better to just return errors? - Or should we accept writes anyway, and write them to some nodes that are reachable but aren\u2019t among the n nodes on which the value usually lives?</p> <p>Later known as sloppy quorum, w write and r reads are still required but they might not be in original n nodes. </p> <p>By analogy, if you lock yourself out of your house, you may knock on the neighbor\u2019s door and ask whether you may stay on their couch temporarily.</p> <p>Once network issue is fixed, any write handled by sloppy quorum nodes will be forward to original n nodes. This is called hinted handoff </p> <p>Sloppy quorum is particularly useful for increasing write availability. As long as w nodes are available for write. </p> <p>This quorum is not traditional quorum but a durability guarantee. (data store in w nodes somewhere else) There is no guarantee that a read of r nodes will see it until the hinted handoff has completed.</p> <p>Sloppy quorums are optional in all common Dynamo implementations. In Riak they are enabled by default, and in Cassandra and Voldemort they are disabled by default [46, 49, 50].</p>"},{"location":"Chapter%205/#example-of-leaderless-for-multi-data-center-operation","title":"Example of leaderless for multi data center operation","text":"<p>Cassandra and Voldemort implement their multi-datacenter support within the normal leaderless model: the number of replicas n includes nodes in all datacenters, and in the configuration you can specify how many of the n replicas you want to have in each datacenter. Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgment from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on the cross-datacenter link. The higher-latency writes to other datacenters are often configured to happen asynchronously, although there is some flexibility in the configuration [50, 51].</p> <p>Riak keeps all communication between clients and database nodes local to one data\u2010 center, so n describes the number of replicas within one datacenter. Cross-datacenter replication between database clusters happens asynchronously in the background, in a style that is similar to multi-leader replication [52].</p>"},{"location":"Chapter%205/#detecting-concurrent-writes","title":"Detecting concurrent writes","text":"<p>The root problem is the same as multi leader, write can arrive out of order.   client B <code>set x = B</code> but <code>get x = A</code>  How to resolve this issue?</p>"},{"location":"Chapter%205/#last-write-wins","title":"Last write wins","text":"<p>Each replica only store most \"recent\" value. Recent is quoted because it may actually not be the most recent one</p> <p>We can attach timestamp on each write and discard earlier timestamp ones. This technique is called last write wins, is the only supported conflict resolution method in Cassandra [53], optional for Riak</p> <p>LWW tradeoff with durability </p> <p>if there are several concurrent writes to the same key, even if they were all reported as successful to the client (because they were written to w replicas), only one of the writes will survive and the others will be silently discarded.</p> <p>not good for banking </p>"},{"location":"Chapter%205/#happens-before-relationship","title":"Happens before relationship","text":"<p> this example shows happens before relationship where B's operation build on top of A. We also say that B is causally dependent on A. (thus anomaly is called causality violation) figure 5-12 doesn't have causal dependency</p> <p>So whenever you have 2 operations, there are 3 possibility 1. A happens before B 2. B happens before A 3. A and B happen concurrently When there is causal dependency, later operation override earlier one. For concurrent operation, conflict need to be resolved.</p> <p>For defining concurrency, exact time doesn\u2019t matter: we simply call two operations concurrent if they are both unaware of each other, regardless of the physical time at which they occurred.</p> <p>same with single core CPU, each process doesn't know each other. Doesn't matter they sending sys call at the exact same time or not</p>"},{"location":"Chapter%205/#capture-happens-before-relationship","title":"Capture happens before relationship","text":"<p>Start with single replica, then generalize to multiple replicas </p>"},{"location":"Chapter%205/#merging","title":"Merging","text":"<p>In this shopping cart example, we can just take union of all values. However, if you want to allow people to remove from the cart, then union will cause things reappear in their shopping cart  To prevent this, add a deletion marker such as tombstone</p> <p>As merging siblings in application code is complex and error-prone, there are some efforts to design data structures that can perform this merging automatically, as dis\u2010 cussed in \u201cAutomatic Conflict Resolution\u201d on page 174. For example, Riak\u2019s datatype support uses a family of data structures called CRDTs [38, 39, 55] that can automatically merge siblings in sensible ways, including preserving deletions.</p>"},{"location":"Chapter%205/#version-vectors","title":"Version vectors","text":"<p>Above algo works for single replica, how do we do this with multiple replicas? Instead of single version number, we have version number per replica as well as per key. Each replica increase version number when processing and also track version from other replicas</p> <p>This collection of version numbers from all replicas are called version vector like version number, version vector is sent to client when values are read. </p>"},{"location":"Chapter%205/#summary","title":"Summary","text":"<p>This chapter look at replication and issue with it. There are 3 main approach to replication: 1. Single leader (all write goes to one node) 2. Multi leader (each write to one of several leaders) 3. Leaderless (write send to several node in parallel)</p> <p>Replication has following goal: - High availability - Disconnected operation (still working when there is network interruption) - Latency (replica that is closer to user) - Scalability (balance the load with replica)</p> <p>We discussed few consistency models  Read-after-write user should always see data that they submitted themselves Monotonic reads Time shouldn't go back (After user see a newer value, older one should come back) Consistent prefix read User should see data as the order of write (in causal sense, seeing question before answer) </p>"},{"location":"Chapter%206/","title":"Chapter 6","text":"<p>Chapter 5 discussed about replication without partitioning.(same dataset on each machine) This chapter will talk about break very large dataset into partitions, aka sharding</p> <p>Partition are defined where each record or document belongs to exactly one partition</p> <p>As result, each partition is a small database itself</p> <p>Scalability is the main reason for partitioning a dataset (different partition can be placed in a different machine/node for serving traffic), so query load is spread across different CPU and disk</p> <p>Queries that can be done in a single partition will require only single machine to finish the query. So you can scale by adding more machine.  Complex query where multiple partition can also benefit by executing the query in parallel (although it gets significantly harder)</p> <p>We will look into different approach of partitioning and how indexing can effect your partition </p>"},{"location":"Chapter%206/#partitioning-and-replication","title":"Partitioning and Replication","text":"<p>Partitioning usually go with replication together. (doesn't make sense if you don't replicate a partition because you will have single point of failure)</p> <p>A node may store more than 1 partition.  </p> <p>When leader-based replication scheme is used, everything from Chapter 5 still applies here</p>"},{"location":"Chapter%206/#partitioning-of-key-value-data","title":"Partitioning of Key-Value Data","text":"<p>So we would like to split large dataset into partitions, </p>"},{"location":"Chapter%206/#how-do-we-decide-which-records-to-store-on-which-nodes","title":"how do we decide which records to store on which nodes?","text":"<p>Our goal is to query load evenly across nodes. So 10 nodes should handle 10 times as much data and 10 times as read and write throughput of a single node</p> <p>If the partition is unfair, some partition will have more data or queries than others (known as skewed) This makes the partition less effective. In extreme, all load could end up on one partition, so 9 out of 10 nodes are idle. The partition with higher load is called hot spot</p> <p>One way to approach this is sort key by alphabetical order. Like encyclopedia. </p>"},{"location":"Chapter%206/#partitioning-by-key-range","title":"Partitioning by Key Range","text":"<p>If you also know which partition is assigned to which node, then you can make your request directly to the appropriate node (or, in the case of the encyclopedia, pick the correct book off the shelf).</p> <p>Simply have each volume begin with 2 letter will not work since data is distributed unevenly in this case.</p> <p>volume 1 contains words starting with A and B, but volume 12 contains words starting with T, U, V, X, Y, and Z.</p> <p>The partition boundaries need to adapt to the data. Within each partition, we can keep key in sorted order (LSM tree or BTree all has this property)</p> <p>key range partition makes range scan very easy. (like a network of sensors) However, downside of key range partitioning is that certain access patterns can lead to hot spots. </p> <p>For example, all writes from same day goes to same partition due to the nature of timestamp. To avoid this problem, you could add prefix with the sensor name for each timestamp. Now when fetch for a range of value on a certain day, you can send to different partition (sensor name) with a date query </p>"},{"location":"Chapter%206/#partitioning-by-hash-of-key","title":"Partitioning by Hash of Key","text":"<p>Many distributed DB use hash function to calculate partition of a given key For example, Cassandra and MongoDB use MD5. Once you have hash function picked, you can assign each partition a range of hashes (compare to range of keys)   However, partition by hash we lose the property of efficient range queries. (we lost the sorted order) In MongoDB, when hash-based sharding is turned on, range query has to be sent to all partitions. </p> <p>Cassandra used compound primary key to avoid some of the issue where first part of the key is hashed to determine the partition and rest are sorted (for example, <code>(user_id, update_timestamp)</code>)</p>"},{"location":"Chapter%206/#skewed-workloads-and-relieving-hot-spots","title":"Skewed Workloads and Relieving Hot Spots","text":"<p>Hashing a key cannot avoid issue where all reads and writes are for same key. (Celebrity on twitter or other social media sites) example: 3% of Twitter's server are  dedicated to Justin Bieber</p> <p>(where the key is perhaps the user ID of the celebrity, or the ID of the action that people are commenting on). Hashing the key doesn\u2019t help, as the hash of two identical IDs is still the same.</p> <p>Most data systems cannot auto compensate for this skewed workload. Application has to deal with this individually. For example, add a random number to the beginning or end of the key. (Just two-digit random number would split the writes to the key evenly across 100 different keys, allow those keys to be distributed to different partitions) </p> <p>This works but read have to do additional work, as they have to read all 100 partitions and combine it. So additional bookkeeping is required (which key are splited etc)</p>"},{"location":"Chapter%206/#partitioning-of-and-secondary-index","title":"Partitioning of and Secondary Index","text":"<p>When determine which partition to go for a given primary key, previous example works well. But things get complicated when secondary index are involved because secondary index doesn't identify the record uniquely (find all actions by user 123, all articles containing word <code>hogwash</code>, all cars color is <code>red</code>, etc)</p> <p>There are 2 main approaches to partitioning a database with secondary indexes</p> <ul> <li>Document-based partitioning</li> <li>Term-based partitioning</li> </ul>"},{"location":"Chapter%206/#partitioning-secondary-indexes-by-document","title":"Partitioning Secondary Indexes by Document","text":"<p>Imagine operating a website for selling used cars. Each listing has unique ID (document ID. In order to search for car by <code>color</code> and <code>make</code>, you added secondary index. So partition is done by document ID </p> <p>Reading from document-partitioned index requires scatter/gather which means read query needs to send to all partition since you don't know if this partition contains this secondary index or not and combine the results you get back</p> <p>scatter/gather has long tail latency but widely used. </p> <p>MongoDB, Riak [15], Cassandra [16], Elasticsearch [17], SolrCloud [18], and VoltDB [19] all use document-partitioned secondary indexes.</p>"},{"location":"Chapter%206/#partitioning-secondary-index-by-term","title":"Partitioning Secondary Index by Term","text":"<p>Rather each partition has its own secondary index (local index), we can construct a global index that covers data in all partitions </p> <p>You can't just store global index in a single node so global index are also partitioned. </p> <p>red cars from all partitions appear under color:red in the index, but the index is partitioned so that colors starting with the letters a to r appear in partition 0 and colors starting with s to z appear in partition 1. The index on the make of car is partitioned similarly (with the partition boundary being between f and h).</p> <p></p> <p>This is called term-partitioned because the term we're looking for determines the partition of the index. term here would be <code>color:red</code> (term comes from full-text indexes)</p> <p>exmaple<code>shakespear:[1,2,3,6,8]</code> As before, we could partition by key(term itself) or hash  global index has better read efficiency compare to local index since it doesn't need scatter/gather but lower write efficiency. Because write to a single document may now effect multiple partitions </p> <p>In practice, updates to global secondary indexes are often asynchronous (that is, if you read the index shortly after a write, the change you just made may not yet be reflected in the index). For example, Amazon DynamoDB states that its global secon\u2010 dary indexes are updated within a fraction of a second in normal circumstances, but may experience longer propagation delays in cases of faults in the infrastructure [20].</p>"},{"location":"Chapter%206/#rebalancing-partitions","title":"Rebalancing Partitions","text":"<p>Things changes over time in DB,  - Query throughput increase, more CPU - Dataset size increase, need to add more RAM and disk  - Swap machines All these changes might cause move data from one node to another This process of moving data from one node to another is called rebalancing Rebalancing is expected to meet some minimum requirements: - After rebalance, load should be shared evenly for nodes in the cluster - While rebalancing, database should still serve traffic - Only data that is necessary to move (reduce unnecessary copy thus reduce disk and network load)</p>"},{"location":"Chapter%206/#strategies-for-rebalancing","title":"Strategies for Rebalancing","text":""},{"location":"Chapter%206/#hash-mod-n","title":"hash mod N","text":"<p>First, not to hash mod by N, why?</p> <p>The problem with the mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another.</p> <p>For example, say hash(key) = 123456. If you initially have 10 nodes, that key starts out on node 6 (because 123456 mod 10 = 6). When you grow to 11 nodes, the key needs to move to node 3 (123456 mod 11 = 3), and when you grow to 12 nodes, it needs to move to node 0 (123456 mod 12 = 0). Such frequent moves make rebalancing excessively expensive.</p> <p>We need an approach that doesn\u2019t move data around more than necessary.</p>"},{"location":"Chapter%206/#fixed-number-of-partitions","title":"Fixed number of partitions","text":"<p>Basically new node will steal some partition from current nodes and reverse if a node fails </p>"},{"location":"Chapter%206/#dynamic-partitioning","title":"Dynamic partitioning","text":"<p>A fixed number of partition can easily got boundaries wrong and end up with all of the data in one partition. Reconfiguring the partition boundaries manually is very tedious</p> <p>Dynamic partitioning is similar to BTrees split and merge:</p> <p>When a partition grows to exceed a configured size (on HBase, the default is 10 GB), it is split into two partitions so that approximately half of the data ends up on each side of the split</p> <p>Dynamic partitioning is not only suitable for key range\u2013partitioned data, but can equally well be used with hash-partitioned data. MongoDB since version 2.4 supports both key-range and hash partitioning, and it splits partitions dynamically in either case.</p> <p>Advantage: Adapt to dataset size when increased, i.e. when total data volume increase, number of partition increase  </p>"},{"location":"Chapter%206/#partitioning-proportionally-to-nodes","title":"Partitioning proportionally to nodes","text":"<p>Compare with dynamic partitioning where partitions increase when dataset size increase, decrease when dataset size decrease, fixed number of partition the size of each partition is proportional to the size of dataset. The number of partition is independent of the number of nodes in both case.</p> <p>A third option is to make the number of partitions proportional to the number of nodes in other words, have fixed number of partition per node. </p> <p>so each partition grows when their dataset size grow, but when new node join the cluster it will randomly choose partition to split and take ownership from half of those partition.  more like consistent hashing shown below  https://tom-e-white.com/2007/11/consistent-hashing.html</p>"},{"location":"Chapter%206/#operations-automatic-or-manual-rebalancing","title":"Operations: Automatic or Manual Rebalancing","text":"<p>So is automatic partition rebalancing better or manual rebalancing is better? </p> <p>Fully automated rebalancing can be convenient, because there is less operational work to do for normal maintenance. However, it can be unpredictable. Rebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. If it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.</p> <p>Because of rerouting, it can hurt systems performance </p> <p>For that reason, it can be a good thing to have a human in the loop for rebalancing. It\u2019s slower than a fully automatic process, but it can help prevent operational surprises.</p>"},{"location":"Chapter%206/#request-routing","title":"Request Routing","text":"<p>Now we know how to partition our data onto multiple machines, but how do we route our client to correct partition when request comes in? </p> <p>As partition are rebalanced, assignment of partition to nodes changes. So when client ask \"I want to read or write to key <code>foo</code>\", which IP address and port number we give to this client to connect to? </p> <p>This is an instance of general problem called service discovery, which isn't just limit to DB but other type of server as well (load balancer). </p> <p>Many companies have written their own in- house service discovery tools, and many of these have been released as open source [30].</p> <p>There are few different approaches: 1. Allow clients to contact any node. (round-robin load balancer) if node contains the key then it takes the request, otherwise forward to the appropriate node 2. Send all request to routing tier first, then forward to correct node 3. Client know which node contains which partition and directly connect to the node upon request</p> <p> But how does it (node, routing tier, or client) know when configuration changes?  Because it needs all node agree which partition resides on which node, so this is another consensus problem</p> <p>Many distributed data systems rely on a separate coordination service such as ZooKeeper. ZooKeeper will be the source of truth for mapping of partitions to nodes.</p> <p>Client or routing tier can subscribe to this information in ZooKeeper. Whenever there is a partition changes ownership, ZooKeeper notifies routing tier to updates its routing info </p> <p>For example, LinkedIn\u2019s Espresso uses Helix [31] for cluster management (which in turn relies on ZooKeeper), implementing a routing tier as shown in Figure 6-8. HBase, SolrCloud, and Kafka also use ZooKeeper to track partition assignment. MongoDB has a similar architecture, but it relies on its own config server implementation and mongos daemons as the routing tier.</p> <p>etcd is another open source that does similar things (mostly used in k8s/docker world)</p>"},{"location":"Chapter%206/#parallel-query-execution","title":"Parallel Query Execution","text":"<p>For simple queries such as read or write single key, scatter/gather works pretty well. But in OLAP world, massively parallel processing(MPP) relational database often have queries contains several join, filtering, grouping, and aggregation operations. MPP query optimizer breaks this complex query into a number of execution stages and partitions, which can be executed in parallel on different nodes (reference 33 is about MapReduce, Spark etc)</p> <p>Fast parallel execution of data warehouse queries is a specialized topic, and given the business importance of analytics, it receives a lot of commercial interest. We will discuss some techniques for parallel query execution in Chapter 10. </p> <p>Snowflake went IPO </p>"},{"location":"Chapter%206/#summary","title":"Summary","text":"<p>We explored different ways of partitioning in this chapter. The goal of partitioning is to spread data evenly across different machine for scale.</p> <p>This requires to choose partition schemes and rebalance partition when nodes join/leaves the cluster</p> <p>2 main approach was discussed - Key range partitioning (key are sorted like encyclopedia) - Hash partitioning (hash function is applied to each key, each partition own a range of hashes)</p> <p>Hybrid approaches are possible (compound key) </p> <p>We discussed when partition involved with secondary indexes where it also need to be partitioned </p> <p>2 methods are used: - Document partitioned indexes (local index) - Term partitioned indexes (global index) </p> <p>Last but not least, routing queries to correct partition is discussed. </p>"},{"location":"Chapter%207/","title":"Chapter 7","text":"<p>In reality of database system, many things can go wrong.  - DB software or hardware may fail at any time - The application that uses DB may crash at any time  - Network might get interruption at anytime - Client may overwrite each other's changes - Race condition can cause surprising bugs</p> <p>transactions have been the mechanism of choice for simplifying these issues. </p>"},{"location":"Chapter%207/#what-is-transaction","title":"What is transaction?","text":"<p>A transaction is a way for an application to group several reads and writes together into a logical unit: either the entire transaction succeeds (commit) or it fails (abort, rollback). If it fails, the application can safely retry. </p>"},{"location":"Chapter%207/#purpose-of-transaction","title":"Purpose of transaction","text":"<p>simplify programming model for accessing database. Application can ignore concurrency issues because database has safety guarantees</p> <p>This chapter will talk about different race conditions and isolation levels such as read committed, snapshot isolation, and serializability</p> <p>This chapter applies to both single node and distributed databases.  Chapter 8 will focus on distributed problems only</p>"},{"location":"Chapter%207/#concept-of-transactions","title":"Concept of Transactions","text":"<p>Almost all transactions supported by today's databases follow style that was introduced in 1975 by IBM System R. </p>"},{"location":"Chapter%207/#the-meaning-of-acid","title":"The meaning of ACID","text":"<p>The safety guarantees provided by transactions can be described acronym ACID, which stands for Atomicity, Consistency, Isolation, and Durability.</p> <p>In practice, one database's implementation of ACID is not same as another's implementation. Devil is in the details. ACID has become a marketing term.</p>"},{"location":"Chapter%207/#atomicity","title":"Atomicity","text":"<p>Atomic refers to something that cannot be broken down into smaller parts.  In multi-threaded programming, if one thread executes an atomic operation, that means there is no way that another thread could see the half-finished result of the operation.</p> <p>The system could only see before or after the operation (not something in between)</p> <p>But in the context of ACID, atomicity is not about concurrency. It does not describe what happens if several processes try to access the same data at the same time, because it is covered under the letter I for isolation</p> <p>Atomicity basically is when client has multiple writes and one of them failed (disk become full, process crash, network interruption etc) then this transaction must be aborted or undo writes that has been made.</p> <p>The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. Perhaps abortability would have been a better term than atomicity, but we will stick with atomicity since that\u2019s the usual word.</p> <p>abortability might be a better term</p>"},{"location":"Chapter%207/#consistency","title":"Consistency","text":"<p>In the context of ACID, consistency refers to an application-specific notion of the database being in a \"good state\"</p> <p>Basically this consistency is something application defines, for example, in an accounting system, credits and debits across all accounts must always be balanced. </p> <p>so the letter C doesn't really belong in ACID</p>"},{"location":"Chapter%207/#isolation","title":"Isolation","text":"<p>This is related to concurrency Most databases are accessed by several clients at the same time, and when they access same database records, you need isolation level to help you prevent some issues that might occur</p> <p></p> <p>Isolation in the context of ACID means that concurrently executed transactions are isolated from each other. In textbook, isolation means serializability which means that each transaction can pretend that it is the only transaction running on the entire database. The database ensures that when the transactions have committed, the result is the same as if they had run serially </p> <p>In practice, serializable isolation is rarely used, because it has performance penalty. </p>"},{"location":"Chapter%207/#durability","title":"Durability","text":"<p>In single node, Durability is the promise that once transaction has been committed successfully, it is written to disk or WAL</p> <p>In a replicated database, durability may mean the data is copied to some number of nodes.</p> <p>Nothing is perfect, even after write to disk, there are still many problems that could occur - In replicated system, recent writes may be lost when the leader becomes unavailable - Disk firmware can have bugs, just like any other kind of software - Data on disk can gradually become corrupted without being detected - If SSD is disconnected from power, it can start losing data within a few weeks depending on the temperature</p> <p>In practice, there is no one technique that can provide absolute guarantees. There are only various risk-reduction techniques, including writing to disk, replicating to remote machines, and backups\u2014and they can and should be used together. As always, it\u2019s wise to take any theoretical \u201cguarantees\u201d with a healthy grain of salt.</p>"},{"location":"Chapter%207/#single-object-and-multi-object-operations","title":"Single-Object and Multi-Object Operations","text":"<p>To recap, atomicity and isolation is what database should do if there are multiple write within the same transaction: Atomicity  Abort if one of the writes failed.  Isolation   If one transaction makes several writes, then another transaction should see either all or none of those writes, but not in between</p> <p>These definitions assume that you want to modify several objects at once. Such multi-object transactions are often needed.</p> <p>For example, <pre><code>SELECT COUNT(*) FROM emails WHERE recipient_id = 2 AND unread_flag = true\n</code></pre></p> <p></p> <p>similarly, atomicity is needed when write failed  </p>"},{"location":"Chapter%207/#single-object-writes","title":"Single Object writes","text":"<p>Atomicity and isolation also applies when single object/row is involved </p> <p>Imagine writing a 20KB JSON document to a database - If network get interrupted after 10KB have been sent, does the database store that fragment of JSON? - If power fails during middle of overwriting the previous value on disk, do you end up with old and new value spliced together? - If another client reads while there is a write in progress, will it see partially updated value (dirty read)?</p> <p>Atomicity can be implemented using a log (WAL) for crash recovery and isolation can be implemented using a lock on each object </p> <p>There are atomic operations that is used by databases, read-modify-write cycle like figure 7-1 and similar one is compare-and-set operation, which allows a write to happen only if the value has not been concurrently changed by someone else </p> <p>These single-object operations are useful because they can prevent lost updates. But they are not transactions. (more like atomic operation)</p>"},{"location":"Chapter%207/#the-need-for-multi-object-transactions","title":"The need for multi-object transactions","text":"<p>Do we need multi-object transactions at all? Would it be possible to implement any application with only a k-v data model and single-object operations?</p> <p>There are cases where writes to several different objects need to be coordinated:  - In a relational data model, foreign key reference to a row in another table - In document data model, when denormalized information needs to be updated, we need to update several documents in one go. Transactions are very useful in this situation to prevent denormalized data from going out of sync. - Same as denormalized data, databases with secondary indexes also need to be updated every time you change a value. </p>"},{"location":"Chapter%207/#handling-errors-and-aborts","title":"Handling errors and aborts","text":"<p>ACID database are build based on this philosophy: </p> <p>if the database is in danger of violating its guarantee of atomicity, isolation, or durability, it would rather abandon the transaction entirely than allow it to remain half-finished</p> <p>Leaderless system does not follow this philosophy. It work on a \"best effort\" basis, which means \"if it runs into an error, it won't undo something it has already done\" so it's the application's responsibility to recover from errors.</p> <p>Errors will inevitably happen and software developer prefer to think happy path rather than the intricacies of error handling.</p> <p>ORM frameworks such as Django don't retry aborted transactions -- the error usually results in an exception bubbling up the stack. </p> <p>This is a shame because the whole point of aborts is to enable safe retries.</p> <p>There are still many subtleties when retrying an aborted transaction.  - If the transaction actually succeeded, but the network failed while server send acknowledge. Then it will cause transaction perform twice  - If there is overload, retrying will worsen the problem. limit the number of retries will help (use exponential backoff) - It is only worth retrying for transient error (permanent error retry is pointless) - If the transaction has side effects outside of database, those side effect may happen even if the transaction is aborted. For example, you don't want to resend an email if a transaction failed upon retry. This can be fixed by two phase commit </p>"},{"location":"Chapter%207/#weak-isolation-levels","title":"Weak isolation levels","text":"<p>When 2 transactions are modifying same data simultaneously, we need to take care of concurrency issues </p> <p>Concurrency bug are hard to find by testing because it depends on timing and it is difficult to reproduce. It is also difficult to reason about because in large application you don't know which other piece of code are accessing database are accessing the database</p> <p>For those reason, database use transaction isolation to hide concurrency issues from application developers</p> <p>serializable isolation means that the database guarantees that transactions have the same effect as if they ran serially </p> <p>In practice, serializable isolation has performance cost, therefore it is common for DB systems to use weaker levels of isolation, which protect against some concurrency issues, but not all. </p> <p>This section will informally introduce those weak level of isolation</p>"},{"location":"Chapter%207/#read-committed","title":"Read committed","text":"<p>This is the most basic level of isolation level. It makes 2 guarantees  1. When read from database, you only see data that has been committed (no dirty reads) 2. When writing to database, you will only write data that has been committed (no dirty writes)</p>"},{"location":"Chapter%207/#no-dirty-reads","title":"No dirty reads","text":"<p>Dirty read basically mean if a transaction can see uncommitted data from another transaction, then it is called a dirty read</p> <p>If a database is running on read committed isolation level, then it must prevent dirty reads. This means other transaction's write can only been seen after it is committed  </p>"},{"location":"Chapter%207/#no-dirty-writes","title":"No dirty writes","text":"<p>What happens if two transactions concurrently try to update the same object in a database? We don't know which order the writes will happen, but we normally assume that the later write overwrites the earlier write.</p>"},{"location":"Chapter%207/#what-is-dirty-write","title":"What is dirty write?","text":"<p>If 2 transactions are writing to same object, and one of write from a transaction that is not committed gets overwritten by another transaction's write. This is called dirty write</p> <p>Transactions running at read committed isolation level must prevent dirty writes, usually by delaying the second write until the first write's transaction has committed or aborted.</p> <p>dirty writes can lead to several concurrency problems: - listing on sale needs to be updated to reflect the buyer and sales invoice needs to be sent to the buyer.   - read committed doesn't prevent the race condition between two counter increments in figure 7-1 because second write happens after first transactions has committed. So it's not a dirty write. </p>"},{"location":"Chapter%207/#implementing-read-committed","title":"Implementing read committed","text":"<p>Most commonly, databases prevent dirty writes by using row-level locks. It must hold that lock for any given object; if another transaction wants to write to the same object, it must wait until current transaction is committed or aborted before it can acquire the lock and continue </p> <p>However this approach does not work well in practice because one long-running write transaction can force many read-only transactions to wait until the long-running transaction has completed. For this reason, most databases prevent dirty reads by simply return old value if new write has not been committed yet </p>"},{"location":"Chapter%207/#snapshot-isolation-and-repeatable-read","title":"Snapshot Isolation and Repeatable Read","text":"<p>Read committed still can have concurrency bug  </p> <p>where total balance went from 1000 to 900 due to read before transaction and another part of read is done after a update transaction is committed</p> <p>This $100 vanished anomaly is called a nonrepeatable read or read skew </p> <p>This case if fine if Alice refresh her web page, but there are circumstances where this is not acceptable Backups Taking a backup requires making a copy of the entire database, which may take hours on a large database. During backup, writes will continue to be made to the database. Thus we could end up with some part of the backup containing an older version and other parts containing a newer version</p> <p>Analytic queries and integrity checks When running queries that scans over large parts of the database (which is common in analytic workload). These queries are likely to return nonsensical results if they observe parts of the database at different points in time</p> <p>Snapshot isolation ref is the most common solution to this problem. The idea is that each transaction reads from consistent snapshot (which means this transaction can see all the data that was committed in the database at the start of this transaction) of the database</p> <p>When a transaction can see a consistent snapshot of the database, frozen at a particular point in time, it is much easier to understand. </p>"},{"location":"Chapter%207/#implementing-snapshot-isolation","title":"Implementing snapshot isolation","text":"<p>Like read committed isolation, snapshot isolation use write locks to prevent dirty writes. However, reads do not require any locks. Basically a key principle of snapshot isolation is readers never block writers </p> <p>To implement snapshot isolation, databases use a generalization of the mechanism we saw for preventing dirty reads </p> <p>The database must keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. </p> <p>When multiple versions are maintained at the same time, this method is also called multi version concurrency control (MVCC)</p> <p>If a database only needed to provide read committed isolation, but not snapshot isolation, it would be sufficient to keep 2 versions of an object: - The committed version  - Overwritten but not yet committed version</p> <p>A typical approach for MVCC is to use a separate snapshot for each query </p>"},{"location":"Chapter%207/#visibility-rules-for-observing-a-consistent-snapshot","title":"Visibility rules for observing a consistent snapshot","text":"<p>Database can present a snapshot of database to application by following: 1. At start of each transaction, the database makes a list of all the other transactions that are in progress  2. Any writes made by aborted transactions are ignored. 3. Any writes made by transactions with a later txn ID are ignored, regardless of whether subsequently commit. 4. All other writes are visible to the application queries </p>"},{"location":"Chapter%207/#indexes-and-snapshot-isolation","title":"Indexes and snapshot isolation","text":"<p>How do indexes work in multi-version database? 1 option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction.</p> <p>Another approach is used in CouchDB, Datomic, and LMDB. Although they also use B-trees (see \u201cB-Trees\u201d on page 79), they use an append-only/copy-on-write variant that does not overwrite pages of the tree when they are updated, but instead creates a new copy of each modified page. Parent pages, up to the root of the tree, are copied and updated to point to the new versions of their child pages. Any pages that are not affected by a write do not need to be copied, and remain immutable [33, 34, 35].</p>"},{"location":"Chapter%207/#repeatable-read-and-naming-confusion","title":"Repeatable read and naming confusion","text":"<p>Snapshot isolation is a useful isolation level, especially for read-only transactions. However, many databases that implement it call it by different names. In Oracle it is called serializable, and in PostgreSQL and MySQL it is called repeatable read [23].</p> <p>It comes from System R </p> <p>The reason for this naming confusion is that the SQL standard doesn\u2019t have the concept of snapshot isolation, because the standard is based on System R\u2019s 1975 definition of isolation levels [2] and snapshot isolation hadn\u2019t yet been invented then. Instead, it defines repeatable read, which looks superficially similar to snapshot isola\u2010 tion.</p>"},{"location":"Chapter%207/#preventing-lost-updates","title":"Preventing Lost Updates","text":"<p>The read committed and snapshot isolation level talked so far primarily concern with guarantees about read operation when concurrent write happens. There are several other interesting kinds of conflict that can occur between concurrently writing transactions. The best known is lost update problem  </p> <p>When read-modify-write cycle happens, then following scenario might have lost update </p> <ul> <li>Incrementing a counter or updating an account balance (requires reading the current value, calculating the new value, and writing back the updated value)  </li> <li>Making a local change to a complex value, e.g., adding an element to a list within a JSON document (requires parsing the document, making the change, and writing back the modified document)</li> <li>Two users editing a wiki page at the same time, where each user saves their changes by sending the entire page contents to the server, overwriting whatever is currently in the database</li> </ul> <p>A variety of solution have been developed </p>"},{"location":"Chapter%207/#atomic-write-operations","title":"Atomic write operations","text":"<p>Atomic update remove the need to implement read-modify-write cycles in application code. </p> <p>for example <pre><code>UPDATE counters SET value = value + 1 WHERE key = 'foo';\n</code></pre> are concurrent-safe in most relational DBs</p> <p>Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transactions can read it until the update has been applied.</p> <p>However, ORM framework doesn't always translate atomic update to SQL very well </p>"},{"location":"Chapter%207/#explicit-locking","title":"Explicit locking","text":"<p>Another option is to let application explicitly lock the object it is trying to modify. Then the application can perform a read-modify-write cycle. If any other transaction tries to concurrently read the same object, it is forced to wait until the first read-modify-write cycle has completed  For example, multiple player want to move a figure  <pre><code>BEGIN TRANSACTION;\n\nSELECT * FROM figures\n    WHERE name = 'robot' AND game_id = 222\n    FOR UPDATE; -- 1\n\nUPDATE figures SET position = 'c4' WHERE id = 1234;\n\nCOMMIT;\n</code></pre></p> <p>The FOR UPDATE clause indicates that the database should take a lock on all rows   returned by this query.</p> <p>This works but requires careful thinking in application code where to lock because it is easy to forget to add necessary code. </p>"},{"location":"Chapter%207/#automatically-detecting-lost-updates","title":"Automatically detecting lost updates","text":"<p>Previous 2 method are ways of preventing lost updates by forcing the read-modify-write cycles to happen sequentially. </p> <p>An alternative is to allow them execute in parallel, and if transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.</p> <p>This approach allows databases can perform its check in parallel with snapshot isolation</p> <p>Indeed, PostgreSQL\u2019s repeatable read, Oracle\u2019s serializable, and SQL Server\u2019s snapshot isolation levels automatically detect when a lost update has occurred and abort the offending transaction. However, MySQL/ InnoDB\u2019s repeatable read does not detect lost updates [23].</p> <p>Martin didn't mention how to implement this </p>"},{"location":"Chapter%207/#compare-and-set","title":"Compare-and-set","text":"<p>In databases that don't provide transactions, you sometimes find an atomic compare-and-set operation(similar to CPU atomic operation?). The purpose of this operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it. </p> <p>If the current value does not match what you previously read, read-modify-write cycle must be retried. <pre><code>    -- This may or may not be safe, depending on the database implementation\n\nUPDATE wiki_pages SET content = 'new content' \n  WHERE id = 1234 AND content = 'old content';\n</code></pre></p> <p>However, if the database allows the WHERE clause to read from an old snapshot, this statement may not prevent lost updates, because the condition may be true even though another concurrent write is occurring.</p>"},{"location":"Chapter%207/#conflict-resolution-and-replication","title":"Conflict resolution and replication","text":"<p>When transaction involves in replicated(distributed) database, things get to another level: because data reside in multiple nodes, and data can potentially be modified concurrently on different nodes, some additional steps need to be taken to prevent lost updates.</p> <p>locks and compare-and-set assume there is single node\uff08single up to date copy of the data. Databases with multi-leader or leaderless replication usually allow several writes to happen, so there is no guarantee of single up-to-date copy of the data. </p> <p>A common approach discussed previously was to allow multiple version exist at the same time (allow concurrent writes to create several conflicting versions of a value) and use application code or special data structure to resolve and merge these versions after the fact.</p> <p>Atomic operations can work well in replicated context, especially if they are commutative (different order doesn't effect the result), for example, add an element to a set, incrementing a counter etc.</p> <p>On the other hand, the last write wins (LWW) conflict resolution method is prone to lost updates, as discussed in \u201cLast write wins (discarding concurrent writes)\u201d on page 186. Unfortunately, LWW is the default in many replicated databases.</p>"},{"location":"Chapter%207/#write-skew-and-phantoms","title":"Write skew and Phantoms","text":"<p>dirty writes and lost updates are two kinds of race condition we went through so far. There are other conflicts </p> <p>For example, if an application is managing doctor oncall shifts at a hospital. The hospital usually have several doctors oncall at the same time. (Must have at least one doctor on call). </p> <p>Doctors can give up their shifts, provided there is 1 doctor is on call.  </p> <p>This is known as write skew. It is neither dirty write nor lost update because 2 transaction are updating 2 different rows </p> <p>we can think of write skew as a generalization of lost update problem. </p> <p>Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</p> <p>dirty write or lost update is subset (only update single item from multiple transactions) of write skew</p> <p>If you can\u2019t use a serializable isolation level, the second-best option in this case is probably to explicitly lock the rows that the transaction depends on. In the doc\u2010 tors example, you could write something like the following:</p> <pre><code>BEGIN TRANSACTION;\n\n    SELECT * FROM doctors  \n    WHERE on_call = true  \n    AND shift_id = 1234 FOR UPDATE; --lock all rows\n\n    UPDATE doctors  \n    SET on_call = false WHERE name = 'Alice' AND shift_id = 1234;\n\nCOMMIT;\n</code></pre>"},{"location":"Chapter%207/#more-examples-of-write-skew","title":"More examples of write skew","text":"<ul> <li>Meeting room booking system</li> <li>Multiplayer game (move different figures on the board at the same time)</li> <li>Claiming a username</li> <li>Preventing double-spending</li> </ul>"},{"location":"Chapter%207/#phantoms-causing-write-skew","title":"Phantoms causing write skew","text":"<p>You can see the pattern now,  1. SELECT query checks some requirement is satisfied or not 2. Depending on the result of first query, application code decides how to continue 3. Makes a write to DB and commit the transaction</p> <p>This effect, where a write in one transaction changes the result of a search query in another transaction is called a phantom </p> <p>snapshot isolation prevent phantom read but not write skew</p>"},{"location":"Chapter%207/#serializability","title":"Serializability","text":"<p>This is not a new problem\u2014it has been like this since the 1970s, when weak isolation levels were first introduced [2]. All along, the answer from researchers has been sim\u2010 ple: use serializable isolation!</p> <p>Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency.</p> <p>But if serializable isolation is so much better than the mess of weak isolation levels, then why isn\u2019t everyone using it?</p> <p>3 options - Literally executing transactions in a serial order - Two-phase locking - Optimistic concurrency control techniques such as serializable snapshot isolation</p> <p>This chapter talks those in single node. Chapter 9 will talk about those in distributed setting </p>"},{"location":"Chapter%207/#actual-serial-execution","title":"Actual Serial Execution","text":"<p>Basically execute only one transaction at a time, in serial order, on a single thread.</p> <p>Even though this seems like an obvious idea, database designers only fairly recently\u2014 around 2007\u2014decided that a single-threaded loop for executing transactions was feasible [45].</p> <p>why?  - RAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory - Database designers realized that OLTP transactions are usually short and only make a small number of reads and writes</p> <p>The approach of executing transactions serially is implemented in VoltDB/H-Store, Redis, and Datomic [46, 47, 48]</p> <p>A system designed for single-threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead of locking. However, its throughput is limited to that of a single CPU core.</p> <p>You can increase the throughput to multiple CPU by partitioning the dataset </p>"},{"location":"Chapter%207/#summary-of-serial-execution","title":"Summary of serial execution","text":"<ul> <li>Every execution of txn must be small and fast</li> <li>active dataset should fit in memory. </li> <li>Write throughput must be low enough to be handled by a single CPU core or else transactions need to be partitioned</li> </ul>"},{"location":"Chapter%207/#two-phase-locking-2pl","title":"Two-Phase Locking (2PL)","text":"<p>2PL was the only one widely used algorithm for around 30 years</p> <p>2PL is similar to regular lock but makes lock requirements much stronger. </p> <p>Several transactions are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as anyone wants to write, exclusive access is required </p> <ul> <li> <p>If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue. (This ensures that B can\u2019t change the object unexpectedly behind A\u2019s back.)</p> </li> <li> <p>If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue. (Reading an old version of the object, like in Figure 7-1, is not acceptable under 2PL.)</p> </li> </ul> <p>In 2PL, writers don\u2019t just block other writers; they also block readers and vice versa. Snapshot isolation has the mantra readers never block writers, and writers never block readers</p>"},{"location":"Chapter%207/#implementation-of-2pl","title":"implementation of 2PL","text":"<p>The blocking of readers and writers is implemented by a having a lock on each object in the database. The lock can either be in shared mode or in exclusive mode. The lock is used as follows:</p> <ul> <li> <p>If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, these transactions must wait.</p> </li> <li> <p>If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. No other transaction may hold the lock at the same time (either in shared or in exclusive mode), so if there is any existing lock on the object, the transaction must wait.</p> </li> <li> <p>If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.</p> </li> <li> <p>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name \u201ctwo-phase\u201d comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</p> </li> </ul>"},{"location":"Chapter%207/#prevent-phantom","title":"Prevent phantom","text":"<p>Phantom is when one transaction changing the result of another transaction's search query </p> <p>To solve this problem, we need a predicate lock [3]. It works similarly to the shared/exclusive lock described earlier, but rather than belonging to a particular object (e.g., one row in a table), it belongs to all objects that match some search condition, such as:</p> <pre><code>SELECT * FROM bookings \n    WHERE room_id = 123 \n    AND end_time &gt; '2018-01-01 12:00' \n    AND start_time &lt; '2018-01-01 13:00';\n</code></pre>"},{"location":"Chapter%207/#serializable-snapshot-isolation-ssi","title":"Serializable Snapshot Isolation (SSI)","text":"<p>This chapter has painted a bleak picture of concurrency control in databases.</p> <p>weaker isolation level have concurrency issues and serializable isolation has performance issue</p> <p>An algorithm called serializable snapshot isolation (SSI) is very promising. It provides full serializability, but has only a small performance penalty compared to snapshot isolation. SSI is fairly new: it was first described in 2008 [40] and is the subject of Michael Cahill\u2019s PhD thesis [51].</p> <p>PostgreSQL MVCC (snapshot isolation implementation)</p>"},{"location":"Chapter%207/#pessimistic-versus-optimistic-concurrency-control","title":"Pessimistic versus optimistic concurrency control","text":"<p>Two-phase locking is a so-called pessimistic concurrency control mechanism: it is based on the principle that if anything might possibly go wrong (as indicated by a lock held by another transaction), it\u2019s better to wait until the situation is safe again before doing anything. It is like mutual exclusion, which is used to protect data structures in multi-threaded programming.</p> <p>serial execution is pessimistic to the extreme </p> <p>By contrast, serializable snapshot isolation is an optimistic concurrency control technique. Instead of blocking, transaction continue anyway and hope everything turn out alright. </p> <p>When transaction wants to commit, the database checks whether anything bad happened (read skew, dirty writes, write skew). If anything goes bad, transaction is aborted and has to be retried. </p> <p>As the name suggests, SSI is based on snapshot isolation\u2014that is, all reads within a transaction are made from a consistent snapshot of the database. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transaction to abort.</p>"},{"location":"Chapter%207/#decisions-based-on-an-outdated-premise","title":"Decisions based on an outdated premise","text":"<p>Write skew could happen when a premise is outdated. (i.e. when transaction reads some data and examine the result of the query, and decide to take some action based on the result that it saw) </p> <p>When application makes a query (how many doctors are currently oncall?) the database doesn't know how the application logic uses the result of that query. so database needs to assume any change in the query result (the premise) that writes in that transaction may be invalid. </p> <p>How to detect outdated premise? There are 2 cases to consider:  - Detecting reads of a stale MVCC object version  - Detecting writes that affect prior reads</p>"},{"location":"Chapter%207/#detecting-stale-mvcc-reads","title":"Detecting stale MVCC reads","text":"<p>When a transaction reads from consistent snapshot in an MVCC database, it ignores writes that has not been committed yet (which is transaction 42 below) So transaction 43 see Alice is still oncall. However, by the time transaction 43 wants to commit, transaction 42 is already committed. Then transaction 43's premise is no longer true</p> <p>database basically tracks all premise made by all transactions and check when a transaction wants to commit, wether its premise is updated by a committed transaction or not </p> <p>Why wait until commiting? Why not just abort transaction 43 immediately when stale read is detected? If transaction 43 is read only transaction, it wouldn't need to be aborted (no way of write skew). </p> <p>At the time of transaction 43 make a read, database doesn't know whether this transaction is going to perform a write. </p> <p>Moreover, transaction 42 may abort so the read may turn out not to be stale after all.</p>"},{"location":"Chapter%207/#detecting-writes-that-affect-prior-reads","title":"Detecting writes that affect prior reads","text":"<p>In context of 2 phase locking, index-range locks allow the database to lock access to all rows matching some search query, such as <code>WHERE shift_id = 1234</code>. Similar technique is used except SSI locks don't block other transactions </p> <p> Database can use index entry 1234 to record the fact that transaction 42 and 43 read this data. </p> <p>When a transaction writes to the database, it must look in the indexes for any other transactions that have read the affected data. This process is similar to acquiring a write lock, but rather than blocking until the readers have committed, the lock simply notifies the transactions that the data they read may no longer be up to date.</p> <p>In the graph above, transaction 42 notifies 43 that its prior read is outdated and vice versa. Transaction 42 is first to commit and it is successful because 43 is not committed yet. </p> <p>When 43 wants to commit, conflict write from 42 is already committed so 43 must abort </p>"},{"location":"Chapter%207/#performance-of-serializable-snapshot-isolation","title":"Performance of serializable snapshot isolation","text":"<p>As always, many engineering details affect how well an algorithm works in practice. For example, one trade-off is the granularity at which transactions\u2019 reads and writes are tracked. If the database keeps track of each transaction\u2019s activity in great detail, it can be precise about which transactions need to abort, but the bookkeeping overhead can become significant. Less detailed tracking is faster, but may lead to more transactions being aborted than strictly necessary. In some cases, it\u2019s okay for a transaction to read information that was overwritten by another transaction: depending on what else happened, it\u2019s sometimes possible to prove that the result of the execution is nevertheless serializable. PostgreSQL uses this theory to reduce the number of unnecessary aborts [11, 41].</p> <p>Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn\u2019t need to block waiting for locks held by another transaction. In particular, read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.</p>"},{"location":"Chapter%207/#summary","title":"Summary","text":"<p>Transaction is an abstraction layer from database that allows application to ignore certain concurrency issues and hardware faults. </p> <p>A large class of errors is reduced down to a simple transaction aborts and application just need to retry</p> <p>simple read and write in single record can manage without transactions. But for more complex access patterns, transactions can reduce the number of potential error cases for application developer </p> <p>Without transactions, various error scenarios (process crashing, network interruption, power outages, disk full, unexpected concurrency, etc) mean data can become inconsistent (denormalized data can easily go out of sync)</p> <p>This chapter went deep into topic of concurrency control of database. It discussed several widely used isolation levels: - Read committed - snapshot isolation - Serializable </p> <p>It characterized those isolation levels by discussing different race conditions Dirty reads     One client reads another client\u2019s writes before they have been committed. The read committed isolation level and stronger levels prevent dirty reads. Dirty writes     One client overwrites data that another client has written, but not yet committed. Almost all transaction implementations prevent dirty writes. Read skew (bank example)     A client sees different parts of the database at different points in time. This issue is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with multi-version concurrency control (MVCC). Lost updates (counter example)     Two clients concurrently perform a read-modify-write cycle. One overwrites the other\u2019s write without incorporating its changes, so data is lost. Some implemetations of snapshot isolation prevent this anomaly automatically, while others require a manual lock (SELECT FOR UPDATE).  Write skew (doctor example)     A transaction reads something, makes a decision based on the value it saw, and writes the decision to the database. However, by the time the write is made, the premise of the decision is no longer true. Only serializable isolation prevents this anomaly.  Phantom reads     A transaction reads objects that match some search condition. Another client makes a write that affects the results of that search. Snapshot isolation prevents straightforward phantom reads, but phantoms in the context of write skew require special treatment, such as index-range locks.</p> <p>Weak isolation level require application developer to handle errors manually. Only serializable isolation protects against all of these issues  3 approaches of implementing serializable transactions are discussed: Literally executing transactions in a serial order     If you can make each transaction very fast to execute, and the transaction throughput is low enough to process on a single CPU core, this is a simple and effective option. Two-phase locking     When acquiring write lock, it needs to wait for all readers to commit. When acquiring reader lock, it needs to wait for write lock (if there is any)      For decades this has been the standard way of implementing serializability, but many applications avoid using it because of its performance characteristics. Serializable snapshot isolation (SSI)     A fairly new algorithm that avoids most of the downsides of the previous approaches. It uses an optimistic approach, allowing transactions to proceed without blocking. When a transaction wants to commit, it is checked, and it is aborted if the execution was not serializable.</p> <p>The examples in this chapter used a relational data model. However, transactions are a valuable database feature, no matter which data model is used.</p> <p>This chapter discussed those in the context of single machine. Transactions in distributed databases open a new set of difficult challenges, which will be discussed in next 2 chapters</p>"},{"location":"Chapter%208/","title":"Chapter 8","text":"<p>This chapter deals with different kinds of issues in distributed system </p> <p>Single node is deterministic where same operation always produces the same result</p> <p>In distributed systems, there may be some part of the system that are broken which is known as partial failure. Partial failure are nondeterministic</p> <p>Because distributed system are connected by network, you may not even know whether something succeeded or not, as the time it takes for a message to travel across a network is also nondeterministic</p> <p>Large scale computing system has two kinds - High performance computing - Cloud computing which associated with multi-tenant datacenters This book focus on systems for implementing internet services which has following entities: - Application are online which means stopping the cluster is not acceptable - Nodes in cloud services are built from commodity hardware which means higher failure rates. - It is reasonable to assume that something is always broken.</p>"},{"location":"Chapter%208/#unreliable-networks","title":"Unreliable Networks","text":"<p>Distributed system is build on top of network (shared nothing architecture)  And network is inherently unstable. </p> <p>The internet and most internal networks in datacenters are asynchronous packet networks which means when one node sends a message, network gives no guarantees to when it arrive or it arrive at all. </p> <p>If you send a request and expect a response, many things could go wrong: 1. Your request may have been lost  2. Your request may be waiting in a queue 3. Remote node may have failed 4. Remote node may temporarily stopped responding (long garbage collection)  5. Remote node may processed the request but response get lost  6. Response may get delayed (network on receiver's machine is overloaded)</p> <p></p>"},{"location":"Chapter%208/#network-faults-in-practice","title":"Network Faults in Practice","text":"<p>during a software upgrade for a switch could trigger a network topology reconfiguration, during which network packets could be delayed for more than a minute [17]. Sharks might bite undersea cables and damage them [18]. Other surprising faults include a network interface that sometimes drops all inbound packets but sends outbound packets successfully [19]: just because a network link works in one direction doesn\u2019t guarantee it\u2019s also working in the opposite direction.</p> <p>[17] Mark Imbriaco: \u201cDowntime Last Saturday,\u201d github.com, December 26, 2012. [18] Will Oremus: \u201cThe Global Internet Is Being Attacked by Sharks, Google Confirms,\u201d slate.com, August 15, 2014.</p> <p>If the error handling of network faults is not defined and tested, arbitrarily bad things could happen: for example, the cluster could become deadlocked and permanently unable to serve requests, even when the network recovers [20], or it could even delete all of your data [21]. If software is put in an unanticipated situation, it may do arbi\u2010 trary unexpected things.</p>"},{"location":"Chapter%208/#detecting-faults","title":"Detecting Faults","text":"<ul> <li>Load balancer needs to stop sending requests to a dead node</li> <li>In distributed database with single leader replication, if leader fails, one of follower needs to be promoted to the new leader</li> </ul> <p>In some circumstances, we might get feedback to tell us something is not working - OS will refuse TCP connections by sending a <code>RST</code> or <code>FIN</code> packet in reply - If a node process crashed (or killed by admin), node's OS can run a script to notify other nodes about the crash so another node can take over quickly without having to wait for timeout to expire  - If we have access to interface of the network switches in our datacenter, we can query them to detect link failures at hardware level (this option is ruled out if we are connecting via the internet)  - If router is sure the IP address is unreachable, it may reply with ICMP Destination Unreachable packet</p> <p>Conversely, if something has gone wrong, you may get an error response at some level of the stack, but in general you have to assume that you will get no response at all. You can retry a few times (TCP retries transparently, but you may also retry at the application level), wait for a timeout to elapse, and eventually declare the node dead if you don\u2019t hear back within the timeout.</p>"},{"location":"Chapter%208/#timeouts-and-unbounded-delays","title":"Timeouts and Unbounded Delays","text":"<p>If timeout is only sure way of detecting a fault, then how long should the timeout be? </p> <p>Long timeout means long wait, short timeout may risk of incorrectly declaring a node is dead </p> <p>Prematurely declaring a node dead is problematic: if the node is actually alive and in the middle of performing some action (for example, sending an email), and another node takes over, the action may end up being performed twice.</p> <p>Imagine a fictitious system with a network that guaranteed a maximum delay for packets\u2014every packet is either delivered within some time d, or it is lost, but delivery never takes longer than d. Furthermore, assume that you can guarantee that a nonfailed node always handles a request within some time r. In this case, you could guarantee that every successful request receives a response within time 2d + r\u2014and if you don\u2019t receive a response within that time, you know that either the network or the remote node is not working. If this was true, 2d + r would be a reasonable timeout to use.</p> <p>Unfortunately, most systems we work with have neither of those guarantees: asynchronous networks have unbounded delays</p>"},{"location":"Chapter%208/#network-congestion-and-queueing","title":"Network congestion and queueing","text":"<p>Same as traffic congestion, packet delays varies with network due to queueing: - If several different nodes send packets simultaneously to same destination, the network switch must queue them up. If there is so much incoming data that switch queue fills up, the packet is dropped </p> <p></p> <ul> <li>When a packet reaches the destination machine, if all CPU cores are currently busy, the incoming request from the network is queued by the operating system until the application is ready to handle it.</li> <li>In virtualized environments, a running operating system is often paused for tens of milliseconds while another virtual machine uses a CPU core. During this time, the VM cannot consume any data from the network, so the incoming data is queued (buffered) by the virtual machine monitor [26], further increasing the variability of network delays.</li> <li>TCP performs flow control (also known as congestion avoidance or backpressure), in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node [27].</li> </ul>"},{"location":"Chapter%208/#tcp-vs-udp","title":"TCP vs UDP","text":"<p>Some latency-sensitive applications, such as videoconferencing and Voice over IP (VoIP), use UDP rather than TCP. It\u2019s a trade-off between reliability and variability of delays: as UDP does not perform flow control and does not retransmit lost packets, it avoids some of the reasons for variable network delays (although it is still susceptible to switch queues and scheduling delays).   UDP is a good choice in situations where delayed data is worthless. For example, in a VoIP phone call, there probably isn\u2019t enough time to retransmit a lost packet before its data is due to be played over the loudspeakers. In this case, there\u2019s no point in retransmitting the packet\u2014the application must instead fill the missing packet\u2019s time slot with silence (causing a brief interruption in the sound) and move on in the stream. The retry happens at the human layer instead. (\u201cCould you repeat that please? The sound just cut out for a moment.\u201d)</p>"},{"location":"Chapter%208/#choice-timeout-experimentally","title":"Choice timeout experimentally","text":"<p>In public clouds and multi-tenant datacenters, resources are shared among many customers: the network links and switches, and even each machine\u2019s network interface and CPUs (when running on virtual machines), are shared. Batch workloads such as MapReduce (see Chapter 10) can easily saturate network links. As you have no control over or insight into other customers\u2019 usage of the shared resources, network delays can be highly variable if someone near you (a noisy neighbor) is using a lot of resources [28, 29].</p> <p>In such environment, we need to choose timeouts experimentally:  measure the distribution of network round-trip times over an extended period and over many machines. Then determine the expected variability of delays </p>"},{"location":"Chapter%208/#synchronous-vs-asynchronous-networks","title":"Synchronous vs Asynchronous Networks","text":"<p>Why can\u2019t we solve this at the hardware level and make the network reliable so that the software doesn\u2019t need to worry about it?</p> <p>To answer this question, it\u2019s interesting to compare datacenter networks to the traditional fixed-line telephone network (non-cellular, non-VoIP), which is extremely reliable: delayed audio frames and dropped calls are very rare. A phone call requires a constantly low end-to-end latency and enough bandwidth to transfer the audio sam\u2010 ples of your voice. Wouldn\u2019t it be nice to have similar reliability and predictability in computer networks?</p> <p>When you make a call over telephone network, it establishes a circuit: fixed, guaranteed amount of bandwidth is allocated for the call, along with the entire route between the two callers. This circuit remains in place until the call ends. </p> <p>This kind of network is synchronous </p>"},{"location":"Chapter%208/#can-we-not-simply-make-network-delays-predictable","title":"Can we not simply make network delays predictable?","text":"<p>Circuit in a telephone network is very different from TCP connection. It reserved fixed amount of bandwidth when circuit is established. Whereas TCP connection will try to transfer data in the shortest time possible and doesn't use any bandwidth when idle </p> <p>Ethernet and IP are packet-switched protocols. These protocols do not have the concept of circuit </p> <p>Why internet use packet switching? The answer is that they are optimized for bursty traffic. </p> <p>A circuit is good for an audio or video call, which needs to transfer a fairly constant number of bits per second for the duration of the call. On the other hand, requesting a web page, sending an email, or transferring a file doesn\u2019t have any particular bandwidth requirement\u2014we just want it to complete as quickly as possible.</p> <p>If we transfer a file over a circuit, we would have to guess the bandwidth allocation. If we guessed too low, the transfer time will be very long (network capacity is not fully utilized). If we guess too high, the circuit cannot be set up.</p> <p>using circuits for bursty data transfer wastes network capacity or unnecessarily slow. By contrast, TCP dynamically adapts the rate of data transfer to the available network capacity</p> <p>There have been some attempts to build hybrid networks that support both circuit switching and packet switching, such as ATM.iii InfiniBand has some similarities [35]: it implements end-to-end flow control at the link layer, which reduces the need for queueing in the network, although it can still suffer from delays due to link congestion [36]. With careful use of quality of service (QoS, prioritization and scheduling of packets) and admission control (rate-limiting senders), it is possible to emulate circuit switching on packet networks, or provide statistically bounded delay [25, 32].</p> <p>This sounds promising, but it is not deployed to public internet yet.</p> <p>However, such quality of service is currently not enabled in multi-tenant datacenters and public clouds, or when communicating via the internet.iv Currently deployed technology does not allow us to make any guarantees about delays or reliability of the network: we have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there\u2019s no \u201ccorrect\u201d value for timeouts\u2014they need to be determined experimentally.</p>"},{"location":"Chapter%208/#unreliable-clocks","title":"Unreliable Clocks","text":"<p>Clocks and time are important. Application depends on clocks in various ways 1. Timeout 2. 99th percentile response time 3. Query per sec of this service can handle in last 5 mins 4. How long did user spend on our site? 5. When was the article published? 6. At what date and time should the reminder email be sent? 7. When does the cache expire? 8. What is the timestamp on this error message in this log file? 1-4 measures duration whereas 5-8 measures points in time</p> <p>In distributed system, time become tricky because message can be delayed and we don't know how much later will the message arrive.</p> <p>This fact makes it difficult to determine the order in which things happened when multiple machines are involved.</p> <p>Each machine on the network has its own clock, which is a quartz crystal oscillator. This device is not perfect so each machine has its own notion of time.</p> <p>Just like human being, each person experience time differently than others. My 1 hour might be different than yours </p> <p>It is possible to synchronize clocks to some degree, the most commonly used mechanism is the Network Time Protocol (NTP), which allow computer to adjust its time by send request to a group of servers where it gets its time from a more accurate time source (GPS receiver)</p>"},{"location":"Chapter%208/#monotonic-vs-time-of-day-clocks","title":"Monotonic vs Time-of-Day Clocks","text":""},{"location":"Chapter%208/#time-of-day-clocks","title":"Time-of-day clocks","text":"<p>A time of day clock returns the current date and time according to some calendar (aka wall-clock time)  <code>clock_gettime(CLOCK_REALTIME)</code> on Linux and <code>System.currentTimeMillis()</code> in java return the number of seconds (or milliseconds) since the epoch midnight UTC on Jan 1 1970 </p> <p>Time-of-day clocks ideally should be same on another machine, however, if the local clock is far ahead of NTP server, it may force to reset and appear to jump back in time </p> <p>These jumps and ignore of leap seconds (due to slow down of earth rotation) make time-of-day clock unsuitable for measuring elapsed time </p>"},{"location":"Chapter%208/#monotonic-clocks","title":"Monotonic clocks","text":"<p>Compare to time-of-day clock, monotonic clock won't jump back in time. </p> <p>The names come from the fact that they are guaranteed to always move forward </p> <p>A monotonic clock is suitable for measuring duration (such as timeout or service's response time) <code>clock_gettime(CLOCK_MONOTONIC)</code> on linux and <code>System.nanoTime()</code> in Java are monotonic clocks. </p> <p>You can check the value of monotonic clock at 1 point in time and check again at later time. The difference between 2 values tells you how much time elapsed between 2 checks.</p> <p>Server with multiple CPU may have separate timer per CPU and not necessarily sync with other CPU. OS try to present a monotonic view of the clock but it is wise to take this guarantee of monotonicity with a pinch of salt </p> <p>In distributed system, using a monotonic clock for measuring elapsed time is usually fine.</p>"},{"location":"Chapter%208/#clock-synchronization-and-accuracy","title":"Clock Synchronization. and Accuracy","text":"<p>time-of-day clocks need to be set according to an NTP server or other external time source in order to be useful.</p> <p>But hardware clocks and NTP can be unreliable:  - The quartz clock in a computer can drift depending on the temperature of the machine.</p> <p>Google assumes a clock drift of 200 ppm (parts per million) for its servers [41], which is equivalent to 6 ms drift for a clock that is resynchronized with a server every 30 seconds, or 17 seconds drift for a clock that is resynchronized once a day. This drift limits the best possible accuracy you can achieve, even if everything is working correctly.</p> <ul> <li>When computer's clock sync with NTP server, application observing the time before and after this sync may see time go backward or suddenly jump forward</li> <li>If a node is accidentally firewalled off from NTP servers, the misconfiguration may go unnoticed for some time (sister team had a sev2 for this...)</li> <li>NTP synchronization is bounded by network delay. One experiment showed that a minimum error of 35 ms is achievable when synchronizing over the internet [42]</li> <li>Some NTP servers are wrong or misconfigured, reporting time that is off by hours. client usually query multiple servers and ignore the outliers. </li> <li>Leap seconds result in a minute that is 59 seconds or 61 seconds long which have crashed large system that are not designed with leap seconds in mind  [38] John Graham-Cumming: \u201cHow and why the leap second affected Cloudflare DNS,\u201d blog.cloudflare.com, January 1, 2017. [46] Nelson Minar: \u201cLeap Second Crashes Half the Internet,\u201d somebits.com, July 3, 2012.</li> <li>In VM, CPU is shared between VM, each VM is paused for tens of milliseconds while another VM is running. </li> <li>If you run software on devices you don't control, you cannot trust the clock at all. Some user deliberately set their hardware clock to an incorrect date and time, for example to circumvent timing limitations in games. </li> </ul> <p>It is possible to achieve very good clock accuracy if you have money. For example, the MiFID II draft European regulation for financial institutions requires all high-frequency trading funds to synchronize their clocks to within 100 microseconds of UTC [51] [51] \u201cMiFID II / MiFIR: Regulatory Technical and Implementing Standards \u2013 Annex I (Draft),\u201d European Securities and Markets Authority, Report ESMA/2015/1464, September 2015.</p> <p>Such accuracy can be achieved using GPS receivers, the Precision Time Protocol (PTP) 52</p>"},{"location":"Chapter%208/#replying-on-synchronized-clocks","title":"Replying on Synchronized Clocks","text":"<p>The problem with clocks is that they have a surprising number of pitfalls: A day may not have exactly 86,400 seconds, time-of-day clocks may move backward in time. Different node may have different time compare to another node</p> <p>Network might drop packets and software must design with assumption that the network will occasionally be faulty. Software must handle such faults gracefully. </p> <p>Same is true with clocks, robust software needs to be prepared to deal with incorrect clocks </p> <p>If you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines. Any node drifts too far from the others should be declare dead and remove from the cluster. </p>"},{"location":"Chapter%208/#timestamps-for-ordering-events","title":"Timestamps for ordering events","text":"<p>Consider ordering events across multiple nodes. For example, if two clients write to a distributed database, who got there first? Which write is the more recent one?</p> <p></p> <p>The clock synchronization is very good in this example (3ms) which is probably better than you can expect in practice </p> <p>Nevertheless, when node 2 receives these 2 events, it will incorrectly conclude that x=1 is the more recent value </p> <p>This conflict resolution strategy is called last write wins discussed in Chapter 5#Last write wins and it is widely used in multi-leader replication and leaderless databases such as Cassandra and Riak </p> <p>Some implementation generate timestamps on client rather than server, but does change the fundamental problems with LWW: - a node cannot overwrite values written by a node with faster clock until the clock skew has elapsed - LWW doesn't distinguish sequential or concurrent write. We need version vectors to prevent violations of causality  - It is possible for 2 nodes to generate same timestamp during write operation. We need additional value (large random number) to resolve this conflict. But this approach can also lead to violations of causality 53 \u201cCall Me Maybe: Cassandra,\u201d</p> <p>logical clocks, which are based on incrementing counters rather than an oscillating quartz crystal, are a safer alternative for ordering events </p> <p>[56] Leslie Lamport: \u201cTime, Clocks, and the Ordering of Events in a Distributed Sys\u2010 tem,\u201d Communications of the ACM, volume 21, number 7, pages 558\u2013565, July 1978. doi:10.1145/359545.359563</p> <p>[57] Sandeep Kulkarni, Murat Demirbas, Deepak Madeppa, et al.: \u201cLogical Physical Clocks and Consistent Snapshots in Globally Distributed Databases,\u201d State University of New York at Buffalo, Computer Science and Engineering Technical Report 2014-04, May 2014.</p>"},{"location":"Chapter%208/#clock-readings-have-a-confidence-interval","title":"Clock readings have a confidence interval","text":"<p>With an NTP server on the public internet, the best possible accuracy is probably to the tens of milliseconds, and the error may easily spike to over 100 ms when there is network congestion [57].</p> <p>It makes sense to think of clock reading as range of times, within a confidence interval </p> <p>For example, a system may be 95% confident that time now is between 10.3 and 10.5 seconds pass the minute</p> <p>The uncertainty bound can be calculated based on your time source. If we have a GPS receiver, the uncertainty bound is reported by the manufacturer. If we getting time from NTP server, then it is \\(quartz\\ drift\\ + NTP\\ uncertainty\\ + network\\ round\\ trip\\)</p> <p>Most system don't expose this uncertainty but Google's TrueTime API in Spanner. When you ask current time, you get back 2 values <code>[earliest, latest]</code></p> <p>Based on this value, the clock knows that actual current time is somewhere within this interval</p>"},{"location":"Chapter%208/#synchronized-clocks-for-global-snapshots","title":"Synchronized clocks for global snapshots","text":"<p>The most common implementation of snapshot isolation requires a monotonically increasing transaction ID. If a write with a greater transaction ID, this write is invisible to the snapshot transaction </p> <p>On a single node database, simple counter is sufficient for generating transaction ids. However, for distributed databases, a global monotonically increasing transaction id is needed.</p> <p>This is hard to generate because it needs coordination.  With lots of small, rapid transactions, creating transaction IDs in a distributed system becomes an untenable bottleneck.</p> <p>Can we use the timestamps from synchronized time-of-day clocks as transaction IDs?</p> <p>Spanner implements snapshot isolation across datacenters in this way [59, 60].</p> <p>It uses TrueTime API and if 2 confidence interval do not overlap, for example  if  $$ A = [A_{earliest,} A_{latest}], B = [B_{earliest}, B_{latest}] $$</p> <p>and </p> <p>$$</p> <p>A_{earliest} &lt; A_{latest} &lt; B_{earliest} &lt; B_{latest}</p> <p>$$</p> <p>then B definitely happened after A. Only when they overlap, we are unsure which order A and B happened.</p> <p>In order to ensure causality for transaction, Spanner deliberately wait for the length of the confidence interval before committing a read-write transaction. </p> <p>In order to keep this wait time as short as possible, Google deploys GPS receiver or atomic clock in each datacenter, allowing clocks to be synchronized to within about 7ms [41]</p>"},{"location":"Chapter%208/#process-pauses","title":"Process Pauses","text":"<p>Another example of clock can be dangerous is multi-leader database Think if there is single leader per partition. How does a node know that it is still leader(others hasn't declare it as dead) and it may safely accept writes? </p> <p>One option is for the leader to obtain lease from other nodes, which is similar to lock with timeouts </p> <p>Only one node can hold the lease at any one time (thus it knows it is the leader until the lease expires)</p> <p>In order to remain leader, the node must periodically renew the lease before it expires. If the node fails to renew the lease, another nodes can take over </p> <p>something like this can be implemented  <pre><code>while (true) {  \n    request = getIncomingRequest();\n    // Ensure that the lease always has at least 10 seconds remaining\n    if (lease.expiryTimeMillis - System.currentTimeMillis() &lt; 10000) { \n        lease = lease.renew();\n    }\n    if (lease.isValid()) { \n        process(request);\n    } \n}\n</code></pre></p> <p>But this is wrong. First, it relies on synchronized clocks set by another machine and compare to local clock </p> <p>If the clock are out of sync for more than few seconds, this code will start doing strange things</p> <p>Secondly, the code assumes the time between <code>System.currentTimeMillis()</code> and <code>process(request)</code> is very small. But this could fail if unexpected pause happened during execution of the program. </p> <p>Imagine program paused for 15 second in <code>lease.isValid()</code>. In this case, another node already taken over as leader but this node didn't know it get paused thus processing unsafe request </p> <p>There are many examples where thread can be paused for this long - \"Stop-the-world\" GC in Java sometimes been known to last for minutes. Although pauses can be reduced by changing allocation patterns and GC setting, we must assume the worst if we want to offer robust guarantees  - VM can be suspended and resumed. The length of the pause depends on the rate at which processes writing to memory - On end-user devices such as laptop, execution may also be suspended and resumed arbitrarily. e.g. when user close the lid of their laptop - OS or VM doing context-switches - If application performs synchronous disk access, a thread may be paused waiting for disk I/O to complete - If OS is configured to allow swapping to disk (paging) then when page fault occur thread is paused while this takes place  - A Unix process can be paused by sending <code>SIGSTOP</code> signal, for example by pressing Ctrl-Z in a shell. Even if your environment does not normally use <code>SIGSTOP</code>, it might be sent accidentally by an operation engineer.</p> <p>Similar to make multi-threaded code thread-safe on a single machine, we can't assume anything about timing. </p>"},{"location":"Chapter%208/#response-time-guarantees","title":"Response time guarantees","text":"<p>Those reasons for pausing can be eliminated if we try hard enough </p> <p>Computers that control aircraft, rockets, robots, cars, and other physical objects must respond quickly and predictably to their sensor inputs </p> <p>In these systems, there is a specified deadline by which the software must respond; if it doesn't meet the deadline, it may cause failure of entire system </p> <p>These are called real-time systems</p> <p>For example, you don't want your airbag to be delayed during a car crash due to an GC pause</p> <p>Providing real-time guarantees requires support from all levels of software system  real-time operating system guaranteed allocation of CPU time; library functions must document their worst-case execution times; dynamic memory allocation may be restricted or disallowed entirely </p> <p>All this require a large amount of additional work and \"real time\" is not same as \"high-performance\" because it has to prioritize timely responses above all else</p> <p>For most server-side data processing systems, real-time are simply not economical or appropriate. Consequently, these systems must suffer the pauses and clock instability</p>"},{"location":"Chapter%208/#knowledge-truth-and-lies","title":"Knowledge, Truth, and Lies","text":"<p>A node in network cannot know for sure it can only make guesses based on the messages it receives via the network </p> <p>A node can only find out what state another node is in by exchanging messages with it. If remote node doesn't respond, there is no way of knowing what state it is in</p> <p>Discussion of these system become philosophical: What do we know to be true or false in our system? </p> <p>How sure can we be of that knowledge, if the mechanisms for perception and measurement are unreliable? </p> <p>Should software systems obey the laws that we expect of the physical world, such as cause and effect? </p> <p>We don't need to go as far as meaning of life. In distributed system, we can state the assumptions we are making about the behavior (the system model) and design the actual system in such a way that it meets those assumptions </p> <p>In the rest of this chapter we will further explore the notions of knowledge and truth in distributed systems, which will help us think about the kinds of assumptions we can make and the guarantees we may want to provide. </p>"},{"location":"Chapter%208/#the-truth-is-defined-by-the-majority","title":"The Truth Is Defined by the Majority","text":"<p>Getting political...  A network with asymmetric fault where node can receive messages but not send any messages. (it can receive request from other nodes, but other nodes cannot hear its responses) </p> <p>This situation sounds like a horror movie scene. Node being dragged to graveyard and screaming \"I'm not dead!\" but nobody can hear it so funeral procession continues  If the node noticed other node are not acknowledging its message. It still cannot do anything about it  </p> <p>3rd scenario, A node experiences stop-the-world garbage collection pause. Every thread stopped for a minute and other node declare this node is dead and load it onto the hearse. Then GC finishes and node came out of the coffin in full health  </p> <p>GCed node doesn't even realize that 1 min has passed by and it was declared dead. </p> <p>anyway, the moral of these stories is that a node cannot trust its own judgment of a situation. </p> <p>A distributed system cannot exclusively rely on a single node because it might fail and leaving the system stuck and unable to recover. </p> <p>Many distributed algorithms rely on quorum which is voting among the nodes: decisions require some minimum number of votes from several nodes in order to reduce the dependence on 1 particular node. </p> <p>It's very similar to political voting. If a quorum of nodes declares another node dead, it must be considered dead. </p> <p>In most case, quorum is more than half of the nodes. A majority quorum allows system to continue to work even if individual nodes have failed.</p>"},{"location":"Chapter%208/#the-leader-and-the-lock","title":"The leader and the lock","text":"<p>Normally, a system requires only one of something. For example - One node is allowed to be the leader for a database partition.  - Only one transaction is allowed to hold the lock for an object, to prevent concurrently writing to it and corrupting it - Only one user is allowed to register a particular username, so it can be unique identifier for this user</p> <p>Implementing this in distributed system requires care, even if the node believes it is the \"only one\", it might not be true because quorum of nodes might declare it is dead</p> <p>If the node continues to act as the chosen one, it could cause problems in a system that is not carefully designed. </p> <p></p>"},{"location":"Chapter%208/#fencing-tokens","title":"Fencing tokens","text":"<p>When using a lock or lease to protect access to some resource, such as file storage, we need to ensure the \"chosen\" node cannot disrupt the rest of the system. A technique called fencing is commonly used. </p> <p>Every time the lock server grants a lock or lease, it also returns a fencing token which is a number that increases every time a lock is granted (e.g. incremented by the lock service) </p> <p>Then every time a client sends a write request to a storage service, it must include its current fencing token. If older token is attached, the storage system will know that it is not the latest one  </p> <p>ZooKeeper is used as lock service, it has transaction ID <code>zxid</code> that can be used as fencing token. </p> <p>This mechanism requires the resource itself to take an active role in checking if any writes has expired token </p> <p>Checking a token on the server side may seem like a downside but it is actually good thing: it is unwise for a service to assume clients will always be well behaved. (user change their clock setting to deliberately bypass game restrictions)</p>"},{"location":"Chapter%208/#byzantine-faults","title":"Byzantine Faults","text":"<p>If a node deliberately wanted to subvert the system's guarantees, it could easily do so by sending messages with a fake fencing token. </p> <p>This book assumes node are unreliable but honest (playing by the rules of the protocol)</p> <p>Distributed systems problems become much harder if nodes may \"lie\" (deliberately send bad responses). Such behavior is called Byzantine fault and the problem of reaching consensus in untrusted environment is known as Byzantine Generals Problem </p> <p>Leslie Lamport, Robert Shostak, and Marshall Pease: \u201cThe Byzantine Generals Problem,\u201d ACM Transactions on Programming Languages and Systems (TOPLAS), volume 4, number 3, pages 382\u2013401, July 1982.</p> <p>Byzantine Generals Problem is a generalization of Two Generals Problem which means 2 army generals need to agree on a battle plan by passing messages. And messages might get lost (like packet in a network). This problem will be discussed in Chapter 9</p> <p>The name Byzantine is derived in the sense of excessively complicated, bureaucratic, devious. Lamport wanted to choose a nationality that would not offend any readers</p> <p>A system is Byzantine fault-tolerant if it continues to operate correctly even if some of the nodes are malfunctioning and not obeying the protocol. This concern is relevant in certain circumstances. For example:</p> <ul> <li>In aerospace environments, the data in computer's memory or CPU register could become corrupted by radiation, leading it to respond to other nodes in arbitrarily unpredictable ways. System failure would be very expensive (aircraft crashing and killing everyone on board, or rocket colliding with the international space station), flight control systems must tolerate Byzantine faults [81] John Rushby: \u201cBus Architectures for Safety-Critical Embedded Systems,\u201d at 1st International Workshop on Embedded Software (EMSOFT), October 2001. [82] Jake Edge: \u201cELC: SpaceX Lessons Learned,\u201d lwn.net, March 6, 2013.</li> <li>In a system with multiple participating organizations, some participants may attempt to cheat or defraud others. For example, peer-to-peer network like Bitcoin and other blockchains can be considered to be a way of getting mutually untrusting parties to agree whether a transaction happened or not, without relying on a central authority </li> </ul> <p>This book assume there are no byzantine faults. In your datacenter, all nodes are controlled by your organization (so they can be trusted) and radiation levels are low enough. In most server-side data systems, the cost of deploying Byzantine fault-tolerant solutions makes them impractical</p> <p>web application do need to expect arbitrary and malicious behavior of clients that are under end-user control such as web browsers. This is why input validation, sanitization, and output escaping are so important: preventing SQL injection and cross site scripting  But Byzantine fault-tolerant doesn't apply here because server is the authority on deciding what client behavior is and isn't allowed. </p> <p>In peer-to-peer networks, where there is no such central authority, Byzantine fault tolerance is more relevant.</p> <p>A bug in the software could be regarded as a Byzantine fault, but if you deploy the same software to all nodes, then Byzantine fault-tolerant algorithm cannot save you.  Most Byzantine fault-tolerant algorithms require a supermajority of more than 2/3rd of the nodes to be functioning correctly. To use this approach against bugs, you would have to have 4 independent implementations of the same software and hope that a bug only appears in one of the four implementations. </p>"},{"location":"Chapter%208/#protection-against-weak-forms-of-lying","title":"Protection against weak forms of lying","text":"<ul> <li>Checksums to detect corrupted packets (TCP, UDP or at application level)</li> <li>Sanitize input from user. For example by checking input value is within a reasonable range to prevent denial of service through large memory allocations</li> <li>Configure multiple NTP server to make sure outlier is detected </li> </ul>"},{"location":"Chapter%208/#system-model-and-reality","title":"System Model and Reality","text":"<p>Chapter 9 will discuss solutions for consensus problem. Algorithms need to be written in a way that does not depend too heavily on the details of the hardware and software configuration. This results the need to formalize the kinds of faults we expect to happen in a system. In order to do this, we need to define a system model which is an abstraction that describes what things an algorithm may assume.</p> <p>Regard to time, 3 system models are in common use Synchronous model     Synchronous model assumes bounded network delay, bounded process pauses, and bounded clock error. The synchronous model is not a realistic model of most practical systems, because unbounded delays and pauses do occur </p> <p>Partially synchronous model     Partial synchrony means that the system behaves like synchronous system most of the time but sometimes exceeds the bounds for network delay, process pauses, and clock drift. This is a realistic model of many systems.  Asynchronous model     In this model, an algorithm is not allowed to make any timing assumptions. Some algorithms can be designed for the asynchronous model, but it is very restrictive</p> <p>For node failures, we have 3 common system models:  Crash stop faults     Node can fail only one way: crashing. Which means node stop responding and is gone forever</p> <p>Crash-recovery faults     This model assumes node may crash and perhaps start responding again after some unknown time. In this model, node are assumed to have stable storage and memory state is assumed to be lost  Byzantine (arbitrary) faults     Nodes may do absolutely anything, including trying to trick and deceive other nodes. </p> <p>First is the definition, then go for proof of correctness</p>"},{"location":"Chapter%208/#correctness-of-an-algorithm","title":"Correctness of an algorithm","text":"<p>We have to know its properties in order to define what it means for an algorithms to be correct. For example, the output of sorting algo has the property that any two distinct elements of the output list, the left is smaller than the element on the right </p> <p>Similarly, we want to define the properties of distributed algorithm in order to know what it means to be correct. We require the algorithm to have the following properties  Uniqueness     No two request for a fencing token return the same value Monotonic sequence     If request x returned token \\(t_{x}\\), and request \\(y\\) returned token \\(t_{y}\\), and \\(x\\) completed before \\(y\\) began, then \\(t_{x} &lt; t_{y}\\). Availability     A node that requests a fencing token and does not crash eventually receives a response</p> <p>An algorithm is correct in some system model if it always satisfies its properties in all situations. But is it realistic where all nodes crashed or network delay become infinitely long? </p>"},{"location":"Chapter%208/#safety-and-liveness","title":"Safety and liveness","text":"<p>To clarify the situation, it is worth distinguishing between 2 different kinds of properties: safety and liveness properties. </p> <p>In this example, uniqueness and monotonic sequence are safety properties, but availability is a liveness property</p> <p>Definitions of safety and liveness: - If a safety property is violated, we can point at a particular point in time at which it was broken. After a safety property has been violated, the violation cannot be undone--the damage is already done. (we can identify duplicate fencing token was returned)  - A liveness property works the other way round: it may not hold at some point in time, but there is always hope that it may be satisfied in the future (receiving a response)</p> <p>Advantage of distinguishing safety and liveness properties is that it helps us deal with difficult system models. For distributed algorithms, it is common to require that safety properties always hold. Namely, even if all nodes crashed, entire network fails, the algorithm must nevertheless ensure that it does not return a wrong result </p> <p>Liveness properties are allowed to make caveats: System can malfunction (partial node failure or network interruption) but with finite duration it eventually recovers </p>"},{"location":"Chapter%208/#mapping-system-models-to-the-real-world","title":"Mapping system models to the real world","text":"<p>Safety and liveness are useful for reasoning about the correctness on distributed algorithm But in practice those system model is simplified abstraction of reality</p> <p>For example, algorithms in the crash-recovery model generally assume that data in stable storage survives crashes. However, what happens if the data on disk is corrup\u2010 ted, or the data is wiped out due to hardware error or misconfiguration [91]? What happens if a server has a firmware bug and fails to recognize its hard drives on reboot, even though the drives are correctly attached to the server [92]?</p> <p>Proving algorithm correct does not mean its implementation on a real system will necessarily always behave correctly. But it is a good first step.</p>"},{"location":"Chapter%208/#summary","title":"Summary","text":"<p>This chapter discussed wide range of problems that can occur in distributed system: - Packet over the network may be lost or arbitrarily delayed. Similarly, reply may get lost or delayed. If you don't get reply, you don't know whether the request has been processed or not - A node's clock may significantly out of sync with other nodes. It may jump back or forward in time. Relying on those can be dangerous - A process may get paused for a very long time. Other nodes may declare this node dead and it comes back without knowing it is being declared dead Such partial failures can occur is the defining characteristic of distributed system.  Whenever software tries to do with multiple nodes. Some node may occasionally fail.</p> <p>In distributed system, we try to build tolerance to partial failure into the software, so that the system can still function as a whole</p> <p>To tolerate faults, first we need to detect them. Most systems don't even have an accurate mechanism of detecting whether a node has failed. Most distributed algorithms rely on timeouts to determine whether a remote node is dead or not. However, timeout doesn't distinguish network or node failure. </p> <p>Once fault is detected, making a system tolerate this is not easy. No global variable, no shared memory, no common knowledge or any other kind of shared state between machines. The only way information can flow from one node to another is through unreliable network. Major decisions cannot be safely made by a single node, so we need a quorum to agree.</p> <p>Moving from idealized mathematical perfection of a single computer to messy distributed system can be a bit of shock. So it is worth to do if you can achieve something in a single computer.</p> <p>Discussed in the introduction in part 2, scalability is not the only reason for wanting to use a distributed system. Fault tolerance and low latency are equally important goals. Those goal cannot be achieved in single machine</p> <p>Unreliability of networks, clocks, and processes in inevitable in nature. This chapter is all about problems. In next chapter, we will move to solutions, and discuss some algorithms that can cope with problems in distributed systems.</p> <p>side note, nuclear clock seems possible now but not really relevant to our case</p>"},{"location":"Chapter%209/","title":"Chapter 9","text":"<p>As chapter 8 notes, a lot of things can go wrong in distributed systems. This chapter we will talk about algorithms and protocols that tolerate fault in building distributed systems. </p> <p>We assume packets can be lost, reordered, duplicated, or arbitrarily delayed in network; clocks are not completely synchronized; and nodes can pause or crash at any time.</p> <p>The best way of building fault-tolerant systems is to find general purpose abstractions with guarantees and let application rely on those guarantees. Similar to transactions in Chapter 7#What is transaction?, by using a transaction, the application can pretend there is not crash on database server, nobody is concurrently accessing the database(isolation), and that storage devices are perfectly reliable(durability). Even though these problem do occur, abstraction hide those away from application so that it doesn't need to worry about them </p> <p>Following this chain of thought, we seek abstraction that can allow an application to ignore problems with distributed systems. Consensus is one of the most important abstractions in this context. That is, getting all node to agree on something. </p> <p>Once we have implementation of consensus, application can use it for various purpose. For example, say we have a database with a single leader. If the leader dies and we fail over to another node, the remaining database nodes can use consensus to elect a new leader. It's important all nodes agree who is the new leader. If 2 nodes believe they both are leader, the situation is called split brain and it often leads to data loss. Correct implementation of consensus help to avoid this problem</p> <p>First we need to explore the range of guarantees and abstractions that can be provided in a distributed system </p> <p>We need to understand the scope of what can and cannot be done. In some situation, it's possible for the system to tolerate fault and continue to work. In other situation, it is not possible. </p>"},{"location":"Chapter%209/#consistency-guarantees","title":"Consistency Guarantees","text":"<p>Replication lag leads to issues when you look at 2 database nodes at the same moment. Because you likely to see different data on two nodes. </p> <p>Most replicated databases provide at least eventual consistency which means after some period of time, all database nodes eventually return same value for a given request. A better name for eventual consistency is convergence, as we expect all replicas converge to same value</p> <p>This is a very weak guarantee, because it doesn't say anything about when the replicas converge. Until the time of convergence, reads could return anything. For example, if you immediately read after you write, there is not guarantee that you see the value you just wrote. Because the read may routed to another node which doesn't apply your write yet. </p> <p>Eventual consistency is hard for application developers because it is very different than single threaded program. If you assign a value to a variable and read it again, you don't expect to see previous value or null. </p> <p>When working with a database with weak guarantees, we need to constantly aware of its limitations and not assume much. Otherwise bugs will be hard to find by testing because application may work most of the time. The edge case of eventual consistency may occur only during network interruption or high concurrency </p> <p>Data systems with stronger guarantee usually have worse performance but they are appealing because they are easier to use correctly. Once we seen different consistency models, we will be in a better position to decide which one best fits our needs. </p> <p>There is similarity between distributed consistency models and transaction isolation levels. (repeatable reads, snapshot isolation, SSI) But while there is overlap, they are mostly independent: transaction isolation is avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults </p> <p>This chapter will cover - Strongest consistency model in common use, linearizability and its pros and cons - Issues of ordering events in distributed system, particularly around causality and total ordering - How to atomically commit a distributed transaction </p>"},{"location":"Chapter%209/#linearizability","title":"Linearizability","text":"<p>Wouldn't it be a lot of simpler if the database could give the illusion that there is only one replica? This is the idea behind linearizability (aka atomic consistency, strong consistency, immediate consistency, external consistency)  All operation are atomic (either succeed or fail, not in between and no one can see it in between). With this guarantee, even though there may be multiple replicas in reality, the application does not need to worry about them. </p> <p>In a linearizable system, as soon as one write finished, all client reads from database must be able to see the value just written. In other words, linearizability is a recency guarantee </p>"},{"location":"Chapter%209/#what-makes-a-system-linearizable","title":"What Makes a System Linearizable?","text":"<p>Multiple clients read same key <code>x</code>. In distributed systems literature, <code>x</code> is called register. In practice, it could be key in key-value store, or one row in relational database, or one document in document database. </p> <p> each bar is a request send by the client. Start is when the request was sent. End is when response was received by the client</p> <p>In this example, the register has 2 type of operations - read(x) =&gt; v  database return the value of register x - write(x) =&gt; v  database return ok or error</p> <p>What are possible responses that A and B might get for their read request? - before write completed. which is 0 - after write has completed. 1  - in between, either 0 or 1. These operations are concurrent with the write</p> <p>To make system linearizable, we need to add another constraint  If one client read returns 1, all subsequent reads must return the new value, even if the operation has not completed.  I had question on this regarding dirty read. But Martin explained in later example because linearizability is not isolation level in transactions</p> <p>This model doesn\u2019t assume any transaction isolation: another client may change a value at any time. Client B\u2019s read returned 1 before client A received its response from the database, saying that the write of the value 1 was successful. This is also okay: it doesn\u2019t mean the value was read before it was written, it just means the ok response from the database to client A was slightly delayed in the network.</p> <p>Because this is from client's perspective, ok response is just delay in the network instead of server actually return the value before write operation is comitted.</p> <p>To refine this timing diagram, we add a 3rd type of operation  - <code>cas(x, v_old, v_new) =&gt; r</code> means the client requested an atomic compare-and-set operation. If register x equals to <code>v_old</code> it should atomically set x to <code>v_new</code>. If x is not equal to <code>v_old</code>, it should return error </p> <p> example come from this reference [10]</p>"},{"location":"Chapter%209/#linearizability-vs-serializability","title":"Linearizability vs Serializability","text":"<p>They get easily confused as both words mean something like \"can be arranged in a sequential order\". Here is the difference Serializability is an isolation property of transactions which may contain multiple read and write  Linearizability is a recency guarantee on reads and writes of a register. It doesn't group operations together into transactions and does not prevent write skew</p> <p>Implementation of serializability based on two phase locking or actual serial execution are linearizable. But serializable snapshot isolation is not linearizable because the whole point of consistent snapshot is not include writes that are more recent than the snapshot. And thus reads from snapshot are not linearizable </p>"},{"location":"Chapter%209/#relying-on-linearizability","title":"Relying on Linearizability","text":"<p>There are few areas linearizability is important requirements</p>"},{"location":"Chapter%209/#locking-and-leader-election","title":"Locking and leader election","text":"<p>One way of electing leader is to use lock. No matter how this lock is implemented, it must be linearizable: all nodes must agree which node owns the lock; otherwise it is useless.</p> <p>Coordination service such as Zookeeper and etcd are often used to implement distributed locks and leader election. They use consensus algorithm to implement linearizable operations in a fault-tolerant way. There are still many subtle details to implementing locks correctly. Libraries like Apache Curator providing higher-level recipes on top of ZooKeeper. </p>"},{"location":"Chapter%209/#constraints-and-uniqueness-guarantees","title":"Constraints and uniqueness guarantees","text":"<p>Username or email address have uniqueness constraints. You need linearizability when you want to enforce this constraints. Similar to lock and compare-and-set where it will return error if username is already exist or lock is being hold by someone else </p>"},{"location":"Chapter%209/#cross-channel-timing-dependencies","title":"Cross-channel timing dependencies","text":"<p>Linearizability violation was only noticed when additional communication channel is available (Alice voice to Bob's ears)</p> <p>Similar situation can arise in computer system </p> <p>Above works fine if the file storage is linearizable. If it is not, message queue might be faster than replication inside file storage. In this case, when resizer fetch the image, it will get older version or nothing at all. </p> <p>Problem exists because there are two different communication channels. If you own the communication channel, use reading your own writes can be effective</p>"},{"location":"Chapter%209/#implementing-linearizable-system","title":"Implementing Linearizable System","text":"<p>Since linearizable means there appears to be single copy of data and all operation are atomic. Simplest answer is just to have single copy of the data. But fault-tolerant system need multiple copy of data. And from Chapter 5 there are 3 ways we could replicate data Single leader replication (potentially linearizable) When read from leader. or synchronously updated followers. it is potentially linearizable. However, not all single leader database is actually linearizable either by design (uses snapshot isolation) or due to concurrency bugs </p> <p>Using leaders for reads relies on the assumption you know for sure this is the leader (sometimes node believe it is the leader but it is not due to network issue or process pause) With asynchronous replication, failover may even lose committed writes, which violates both durability and linearizability  Consensus algorithms (linearizable) consensus protocols contain measures to prevent split brain and stale replicas. With these constraints, consensus algorithms can implement linearizable safely. This is how ZooKeeper and etcd work </p> <p>Multi-leader replication (Not linearizable) Multi leader database processes writes on multiple nodes and asynchronously replicated to other nodes. This will result in write conflict. Such conflict are result of the lack of single copy of the data.</p> <p>Leaderless replication (Possibly linearizable)</p>"},{"location":"Chapter%209/#linearizability-and-quorums","title":"Linearizability and quorums","text":"<p>it seems as though strict quorum reads and writes should be linearizable in a Dynamo-style model. However, when we have variable network delays, it is possible to have race conditions </p> <p> B's read is after A but get old value  Same as Alice and Bobs's situation  It is possible to make Dynamo-style quorums linearizable at the cost of performance. For example, reader performs read repair </p> <p>In summary, it is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.</p>"},{"location":"Chapter%209/#the-cost-of-linearizability","title":"The Cost of Linearizability","text":""},{"location":"Chapter%209/#cap-theorem","title":"CAP theorem","text":"<p>CAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. But putting this way is misleading because network partition is only one kind of fault where you can't do anything about it. They will happen whether you like it or not</p> <p>And its definition of availability does not match its usual meaning 40 so it is best to avoided for understand systems</p> <p>The CAP theorem as formally defined [30] is of very narrow scope: it only considers one consistency model (namely linearizability) and one kind of fault (network partitions, or nodes that are alive but disconnected from each other). It doesn\u2019t say anything about network delays, dead nodes, or other trade-offs. Thus, although CAP has been historically influential, it has little practical value for designing systems [9, 40].</p>"},{"location":"Chapter%209/#linearizability-and-network-delays","title":"Linearizability and network delays","text":"<p>Although linearizability is a useful guarantee, surprisingly few system are actually linearizable in practice. Even RAM on a modern multi-core CPU is not linearizable 43 If a thread running on one CPU core writes to memory address, and a thread on another CPU core reads the same address shortly afterwards, it is not guaranteed to read the value written by the first thread (unless a memory barrier or fence is used)</p> <p>seems like hardware face similar issue. (memory hierarchy?)</p> <p>The reason for this behavior is that every CPU core has its own memory cache (L1, L2, L3 cache) and store buffer. Memory access first goes to cache by default. And any changes are asynchronously written out to main memory. This feature is essential for good performance on modern CPUs. But now there are several copies of the data. (one in main memory, many in various caches), and these copies are asynchronously updated, so linearizability is lost. </p> <p>It make no sense to justify the multi-core memory consistency model: In one computer we assume reliable communication and we don't expect one CPU core to be able to continue operating normally if it is disconnected from the rest of the computer. The reason for dropping linearizability is performance not fault tolerance.</p> <p>The same is true for distributed databases (not because of fault tolerance but favor performance instead of linearizability)</p> <p>Can\u2019t we maybe find a more efficient implementation of linearizable storage? It seems the answer is no: Attiya and Welch [47] prove that if you want linearizability, the response time of read and write requests is at least proportional to the uncertainty of delays in the network.</p> <p>Faster algorithm does not exist, but weaker consistency model can be much faster. </p>"},{"location":"Chapter%209/#ordering-guarantees","title":"Ordering Guarantees","text":"<p>Linearizability behaves as if one copy of data exists and all operation appears to take effect atomically at one point in time. This definition implies that operations are executed in some well-defined order. </p> <p>Ordering has been a recurring theme in this book which suggests this might be an important fundamental idea. To recap - In Chapter 5, the main purpose of single leader replication is to determine the order of writes in the replication log. If there is not single leader, conflicts can occur.  - Serializability discussed in Chapter 7 is about transactions executed in some sequential order.  - The use of timestamp and clocks in distributed system discussed in Chapter 8 is another attempt to introduce order into disorderly world. </p>"},{"location":"Chapter%209/#ordering-and-causality","title":"Ordering and Causality","text":"<p>One of the reason ordering keeps coming up is that it helps preserve causality. Several examples are discussed - Chapter 5#Consistent Prefix Reads. We saw example of observer saw the answer to the question first. This violates the intuition of cause and effect (doesn't make sense the person giving answer before the question is asked. Assuming this person cannot see in the future) We say there is causal dependency between question and answer - Doctor example in write skew is also a causal dependency  - Alice and Bob is also an example of causality violation  </p>"},{"location":"Chapter%209/#causal-order-is-not-total-order","title":"Causal order is not total order","text":"<p>A total order allows any 2 elements to be compared. For example, natural numbers are totally ordered. However, mathematical sets are not totally ordered: is <code>{a,b}</code> greater than <code>{b,c}</code>? well, you can't really compare them because neither is a subset of the other. We say they are incomparable. Therefore mathematical sets are partially ordered. Some cases one set is greater than another (if one set contains all the element of another), but in other cases they are incomparable. </p> <p>Total order and partial order is reflected in database consistency models: Linearizability In a linearizable system, we have total order of operations. That is, any 2 operations we can always say which one happened first.  Causality Two events are ordered if they are causally related, but they are incomparable if they are concurrent. This means causality defines a partial order, not a total order. </p> <p>Therefore, according to this definition, there is no concurrent operations in linearizable datastore: There must be a single timeline along which all operations are totally ordered.  </p> <p>Concurrency would mean that the timeline branches and merges again and in this case, operations on different branches are incomparable   timeline branches and merges when two client editing the shopping cart   Version control systems such as Git and its version histories are very much like the graph of causal dependencies. </p>"},{"location":"Chapter%209/#linearizability-is-stronger-than-causal-consistency","title":"Linearizability is stronger than causal consistency","text":"<p>Linearizability implies causality but linearizability has cost of performance. Good news is that linearizability is not the only way of preserving causality. In fact, causal consistency is the best possible consistency model that does not slow down due to network delays and remains available when network failure happens.</p>"},{"location":"Chapter%209/#capturing-causal-dependencies","title":"Capturing causal dependencies","text":"<p>you need to know which operation happens before which other operation in order to maintain causality. </p> <p>In order to determine causal dependencies, we need some way of describing the \"knowledge\" of a node in the system. If a node had already seen the value X when it issued the write Y, then X and Y may be causally related. </p> <p>Version vectors can be generalized to determine happens before relationship 54 just like the shopping cart example, the version number from the prior operation is passed back to the database on a write. </p>"},{"location":"Chapter%209/#sequence-number-ordering","title":"Sequence Number Ordering","text":"<p>Actually keeping track of all causal dependencies can become impractical. In many applications, clients read lots of data before writing something, and then it is not clear whether the write is causally dependent on all or only some of those prior reads. Explicitly tracking all the data that has been read would mean a large overhead.</p> <p>There is a better way. We can use sequence numbers or timestamps to order events. A timestamp need not come from a time-of-day clock (or physical clock). Logical clock would suffice. Typically using counters that are incremented for every operation. Sequence numbers provide total order that is, we can always compare two sequence numbers to determine which is greater or happens later</p> <p>Sequence numbers can be generated in an order that is consistent with causality. In a database with single-leader replication, the replication log defines a total order of write operations that is consistent with causality. The leader can simply increment a counter for each operation and assign this monotonically increasing sequence number to each operation in the replication log. </p>"},{"location":"Chapter%209/#noncausal-sequence-number-generators","title":"Noncausal sequence number generators","text":"<p>For non-single leader databases. Various methods are used in practice: - Each node can generate its own independent set of sequence numbers. You want to reserve some bits in the binary representation of the sequence number to contain a unique node identifier. For example, if you have 2 nodes, one node can generate only odd numbers a the other only even numbers. - You can attach a timestamp from the time of the day clock to each operation. This fact is used in the last write wins conflict resolution method - You can preallocate blocks of sequence numbers. For example, node A might claim of block of sequence number from 1 to 1000, and node B might claim the block from 1001 to 2000. Then each node can independently assign sequence numbers from its block, and allocate a new block when its supply of sequence numbers begins to run low</p> <p>These 3 options all perform better and are more scalable than pushing all operations through a single leader that increments a counter. However, they all have a problem: the sequence numbers they generate are not consistent with causality</p> <p>The causality problems occur because these sequence number generators do not correctly capture the ordering of operations across different nodes:  - Each node may process a different number of operations per seconds. Thus, if one node generates even numbers and the other generate odd numbers, the counter for even numbers may lag behind the counter for odd numbers, or vice versa. You cannot accurately tell which one causally happened first if you have an odd-number operation and an even numbered operation  - Physical timestamps are subject to clock skew which inconsistent with causality.   - In the case of block allocator, one operation may be given a sequence number in the range from 1001 to 2000, and a causally later operation may be given a number in the range from 1 to 1000. Again, didn't capture causality</p>"},{"location":"Chapter%209/#lamport-timestamps","title":"Lamport timestamps","text":"<p>Lamport timestamp proposed in 1978 by Leslie Lamport is sequence numbers that is consistent with causality. 56 is one of the most cited paper in distributed systems </p> <p>The Lamport timestamp is simply a pair of <code>(counter, nodeID)</code>. Each node has a unique identifier and each node keeps a counter of the number of operations it has processed.  lamport timestamp bears no relationship to physical clock Two node might have same counter value, but by including the node ID in the timestamp, each timestamp is made unique </p> <p>Lamport timestamp provides total ordering: greater counter is the greater timestamp; if the counter values are the same, the one with the greater node ID is the greater timestamp</p> <p>The key idea about Lamport timestamps, which makes them consistent with causality, is this: every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.  </p>"},{"location":"Chapter%209/#timestamp-ordering-is-not-sufficient","title":"Timestamp ordering is not sufficient","text":"<p>Although lamport timestamp define total order that is consistent with causality, it is not sufficient to solve many common problems in distributed systems.</p> <p>For example, consider a system that needs to ensure that a username uniquely identifies a user account. If 2 users concurrently try to create an account with the same username, one of the two should succeed and the other should fail. </p> <p>Let's say we decide to pick earlier writer as winner. But this is determining the winner after the fact. Once you have collected all the username creation operation in the system, you can compare their timestamps. </p> <p>This is not sufficient when a node just received a request from a user to create a username because it needs to decide right now whether the request should succeed or fail. At that time, the node doesn't know there is other request that has same username. </p> <p>In order to ensure about no nodes create an username with lower timestamp, we would have to check with every other node to see what it is doing. If one node is failed or cannot be reached due to a network problem, this system grind to a halt. This is not what we want for a fault-tolerant system!</p> <p>The problem here is total order only emerges after we have collected all of the operations. </p> <p>To conclude: in order to implement uniqueness constraint for usernames, it's not sufficient to have a total ordering of operations. We need to know when the order is finalized. This idea of knowing when your total order is finalized is captured in the topic of total order broadcast</p>"},{"location":"Chapter%209/#total-order-broadcast","title":"Total Order Broadcast","text":"<p>The challenge is hot to scale the system if the throughput is greater than a single leader can handle and also how to handle failover if the leader fails. In distributed system literature, this problem is known as total order broadcast or atomic broadcast 57, 58</p> <p>Total order broadcast is protocol for exchanging messages between nodes. It requires 2 safety properties always be satisfied: Reliable delivery     No messages are lost: if a message is delivered to one node, it is delivered to all nodes. Totally ordered delivery     Messages are delivered to every node in the same order.</p> <p>A correct algorithm for total order broadcast must ensure that the reliability and ordering properties are always satisfied, even if a node or the network is faulty. Of course message cannot be delivered while the network is interrupted. Algorithm can keep retrying so that the message get through when the network is repaired.</p>"},{"location":"Chapter%209/#using-total-order-broadcast","title":"Using total order broadcast","text":"<p>Consensus services such as ZooKeeper and etcd actually implement total order broadcast. This shows there is strong connection between total order broadcast and consensus</p> <p>Total order broadcast is exactly what we need for database replication: if every write message get processed in the same order, all replicas will remain consistent with each other. This principle is known as state machine replication</p> <p>Similarly, total order broadcast can be used to implement serializable transactions </p> <p>An important aspect of total order broadcast is that the order is fixed at the time the messages are delivered. Node is not allowed to insert a message into previous broadcasted messages. </p> <p>Another way of looking at total order broadcast is that it is a way of creating a log (replication log, transaction log, or write-ahead log) broadcast is appending to the log all nodes can read the log and deliver the same messages in the same order</p> <p>Total order broadcast is useful for implementing a lock service that provides fencing tokens. Every request to acquire the lock is appended as a message to the log, and all messages are sequentially numbered in the order they appear in the log. The sequence number can then serve as a fencing token, because it is monotonically increasing. </p>"},{"location":"Chapter%209/#implementing-linearizable-storage-using-total-order-broadcast","title":"Implementing linearizable storage using total order broadcast","text":"<p>Total order broadcast guarantee to deliver messages reliably in a fixed order, but there is no guarantee when a message will be delivered. By contrast, linearizability is a recency guarantee: a read is guaranteed to see the latest value written</p> <p>If you have total order broadcast, you can build linearizable storage on top of it. Imagine for every possible username, you can have a linearizable register with an atomic compare-and-set operation. Every register initially has the value <code>null</code>. When user wants to create a username, you execute a compare-and-set operation on the register for that username, under the condition previous register value is <code>null</code>. If multiple users try to concurrently grab the same username, only one of the compare-and-set operations will succeed. </p> <p>You can implement such a linearizable compare-and-set operation as follows by using total order broadcast as an append-only log 62, 63 1. Append a message to the log, tentatively indicating the username you want to claim 2. Read the log, and wait for the message you appended to be delivered back to you 3. Check the log. If the first message for your desired username is your own message, then you are successful: you can commit the username claim and acknowledge it to the client. If the first message for your desired username is from another user, you aborted the operation</p> <p>Because log entries are delivered to all nodes in the same order. Choosing the first of the conflicting writes as the winner and abort later ones ensures uniqueness on all nodes. </p> <p>This procedure provides linearizable writes but not linearizable reads. To make reads linearizable, there are few options: 1. You can sequence reads through the log by appending a message, reading the log, and performing the actual read when the message is delivered back to you. The message's position in the log defines the point in time at which the read happens (Quorum reads in etcd work like this 16) 2. If the log allows you to fetch the position of latest log message in a linearizable way, you can query that position, wait for all entries up to that position to be delivered to you, and then perform the read (this is ZooKeeper's <code>sync()</code> operation) 3. You can make your read from a replica that is synchronously updated on writes and thus is sure to be up to date (this technique is used in chain replication)</p> <p>Basic idea behind this is still the log. I strongly recommend reading this article about the log</p>"},{"location":"Chapter%209/#implementing-total-order-broadcast-using-linearizable-storage","title":"Implementing total order broadcast using linearizable storage","text":"<p>Let's turn it around. Assume we have linearizable storage, and show how to build total order broadcast from it</p> <p>The easiest way is to assume we have a linearizable register that stores an integer and that has an atomic increment-and-get operation 28 or atomic compare-and-set operation will do the job</p> <p>The algorithm is simple: for every message you want to send through total order broadcast, you increment-and-get the linearizable integer, and attach the value you got from the register as sequence number to the message. Then send message to all nodes and recipients will deliver the messages consecutively by the sequence number </p> <p>How hard could it be to make a linearizable integer with an atomic increment-and-get operation? It would be simple if things never fail. But in case of network interruption, linearizable sequence number generators eventually end up with a consensus algorithm. This is no coincidence: it can be proved that a linearizable compare-and-set register and total order broadcast are both equivalent to consensus 28, 67 That is, if you solve one of these problems, you solve all of them</p> <p>This is just similar to applying algorithms. Main problem is not implementation but transform existing problem into something we already know how to solve tango reference is essentially journal...  Let's head to consensus</p>"},{"location":"Chapter%209/#distributed-transactions-consensus","title":"Distributed Transactions (consensus)","text":"<p>Consensus is one of the most important and fundamental problems in distributed computing. The goal is simply to get several nodes to agree on something. We might think this shouldn't be too hard but many broken system have been built in mistaken belief that this problem is easy to solve. </p> <p>The reason why this important topic shows up this late in this book is because it requires some prerequisite knowledge. Even in academic search community, the understanding of consensus gradually crystallized over the course of decades. </p> <p>With understanding of replication (Chapter 5), transactions (Chapter 7), system models (Chapter 8), linearizability, and total order broadcast, we are finally ready to tackle the consensus problem</p> <p>Here are some situations for nodes need to agree. Leader election     In a single leader replication database, all nodes need to agree on which node is the leader. In this case, consensus is important to avoid a bad failover (result in split brain which 2 nodes believe they are leader). If there were two leaders, they would both accept writes and their data would out of sync, leading to inconsistency and data loss Atomic commit     In a database that supports transactions over several nodes or partitions, we have the problem that a transaction may fail on some nodes but succeed on others. If we need atomicity in transaction (A in ACID), we need all nodes to agree on the outcome of the transaction: either they all abort/roll back or they all commit. This instance of consensus is known as the atomic commit problem</p>"},{"location":"Chapter%209/#the-impossibility-of-consensus","title":"The impossibility of Consensus","text":"<p>FLP result 68 -- named after authors Fischer, Lynch, and Paterson-- which proves that there is no algorithm that is always able to reach consensus. yet we are discussing algorithms for achieving consensus, what is going on here?</p> <p>Answer is that FLP result is proved in the asynchronous system model, a very restrictive model that assumes a deterministic algorithm that cannot use any clocks or timeouts. If the algorithm is allowed to use timeouts, or other way to identify suspected crashed nodes then consensus becomes solvable 67</p> <p>FLP is important for theoretical importance, distributed systems can achieve consensus in practice </p> <p>This section will examine atomic commit in detail. We will discuss two-phase commit algorithm, which is the most common way of solving atomic commit and implemented in various databases, messaging systems, and application servers. </p> <p>It turns out 2PC is kind of a consensus algorithm but not very good one 70, 71</p> <p>By learning 2PC, we will explore better consensus algorithm. Such as Zab (used in ZooKeeper) and Raft (used in etcd)</p>"},{"location":"Chapter%209/#two-phase-commit-2pc","title":"Two-Phase commit (2PC)","text":"<p>In Chapter 7 we define transaction atomicity to simplify semantics for application developer. For a transaction involved in multiple writes, either it commit (all writes made durable) or abort (all writes roll back)</p> <p>Atomicity prevents half-finished results and half-updated state in database. This is especially important for multi-object transactions and databases that maintain secondary indexes. </p> <p>In a single node machine, we can just check if <code>commit</code> record is written to WAL or not. What about multiple nodes are involved in a transaction? In this case, it is not sufficient to send a commit request to all nodes and let each node independently commit the transaction. Because of doing this it will easily get out of sync where some nodes succeed and others failed. Here is few example: - Some nodes may detect a constraint violation and abort is necessary. While other nodes are successfully able to commit - Some of the commit request might be lost in the network, eventually abort due timeout. While other commit request might get through - Some nodes may crash before the commit record is fully written and rolled back locally while others successfully commit</p> <p>A transaction commit must be irrevocable -- you are not allowed to change committed transactions. Thus we need two-phase commit to achieve atomicity</p>"},{"location":"Chapter%209/#introduction-to-two-phase-commit","title":"Introduction to two-phase commit","text":"<p>Two-phase commit is the algorithm for achieving atomic transaction commit across multiple nodes (i.e. ensure all nodes either commit or abort) </p> <p>The basic flow of 2PC is to split commit/abort process into two phases (hence the name) </p> <p>2PC uses a new component called coordinator (aka transaction manager). The coordinator or transaction manager is often implemented as a library within the same application process that is requesting the transaction </p> <p>A 2PC transaction begins reading and writing data on multiple database nodes as normal. These involved database nodes are called participants. When application is ready to commit, the coordinator begins phase 1: it sends a prepare request to each of the nodes. Asking them whether they are able to commit. Then transaction manager tracks the responses from the participants: - If all participants reply \"yes\", then coordinator/transaction manager sends out a commit request in phase 2, and commit actually takes place. - If any nodes replies \"no\", the coordinator/transaction manager sends abort request to all nodes in phase 2.</p> <p>Similar to wedding, when minister asks bride and groom individually whether each wants to marry the other. After receiving \"yes\" from both, minister pronounces the couple husband and wife. If any one says no, the ceremony is aborted.</p>"},{"location":"Chapter%209/#a-system-of-promises","title":"A system of promises","text":"<p>Why 2PC works? Wouldn't prepare and commit message can be lost as well?  To understand, we have to break down the process in more detail: 1. when application wants to begin a distributed transaction, it requests a transaction id from coordinator/transaction manager. This transaction id is globally unique 2. The application begins a single-node transaction on each of the participants, and attaches the globally unique transaction ID to single node transaction. All reads and writes are done in one of these single node transactions. If anything goes wrong, the coordinator/transaction manager or participants can abort. 3. When application is ready to commit, coordinator/transaction manager sends a prepare request to all participants, tagged with global transaction id. If any of requests fails or timeout, coordinator sends an abort request for that transaction id to all participants. 4. When participant receives prepare request, it makes sure that it can definitely commit the transaction under all circumstances. (writing all data to disk and checking for any conflicts or constraint violations) By replying \"yes\" to coordinator, the node promises this transaction can be committed no matter what upon request.  5. When coordinator/transaction manager received response from all participants, it makes definitive decision whether commit or abort. The coordinator must write this decision to its transaction log on disk so that it knows which way it decided in case it crashes. This is called the commit point 6. Once decision has been written to disk, the commit or abort request is sent to all participants. If this request fails or times out, the coordinator must retry forever until it succeeds. There is no more going back </p> <p>Thus, this protocol contains two crucial \"point of no return\"  1. When a participant votes \"yes\", it promises that it will definitely be able to commit later 2. Coordinator decides and this decision is irrevocable.  Single node atomic commit put those 2 into 1, writing the commit record to the transaction log </p>"},{"location":"Chapter%209/#coordinator-failure","title":"Coordinator failure","text":"<p>What happens if coordinator/transaction manager crashes? 1. it participant know coordinator crash before receive prepare request, they can freely abort. 2. If they have receive prepare request and voted yes. Then there is no choice but wait for coordinator to recover  This is why coordinator must write decision to a transaction log on disk before sending commit or abort requests. when the coordinator recovers, it determines the status of all in-doubt transactions by reading its transaction log. Any transactions that don't have a commit record in the coordinator's log are aborted. </p> <p>2PC is blocking atomic commit protocol. An algorithm called three-phase commit (3PC) has been proposed and it assumes network with bounded delay and nodes with bounded response times. But most practical system has unbounded network delay and process pauses. For this reason, 2PC continue to be used despite the known problem with coordinator failure</p>"},{"location":"Chapter%209/#distributed-transactions-in-practice","title":"Distributed Transactions in Practice","text":"<p>Distributed transaction such as the ones that implement 2PC has a mixed reputation. They provide important safety guarantee; on the other hand, they are criticized for causing operational problems, killing performance, and promising more than they can deliver 81, 82, 83, 84</p> <p>Some implementation of distributed transaction results heavy performance penalty. MySQL's distributed transactions are reported to be over 10 times slower than single-node transactions. Much of the performance cost from two-phase commit is due to additional disk forcing (fsync) and additional network round-trips</p> <p>Let define what is distributed transaction first Database internal distributed transactions     In distributed databases, distributed transactions means all nodes participating in the transaction are running the same database software Heterogeneous distributed transactions     In a heterogeneous transaction, the participants are two or more different technologies: for example, one is a message broker and one is relational DB. A distributed transaction across these systems must ensure atomic commit even if the systems may be entirely different under the hood</p> <p>Heterogeneous are more challenging compare to internal ones Let's look at atomic commit protocol that allows heterogeneous distributed transactions</p>"},{"location":"Chapter%209/#xa-transactions","title":"XA transactions","text":"<p>X/Open XA (short for eXtended Architecture) is a standard for implementing two phase commit across heterogeneous technologies 76. Proposed in 1991, and It is supported by many traditional relational databases (PostgreSQL, MySQL, DB2, SQL server, Oracle) and message brokers</p> <p>XA is not a network protocol. It is merely a C API for interfacing with a transaction coordinator. In Java EE applications, XA transactions are implemented using Java Transaction API </p> <p>If your network driver support XA, it will call XA API to find out whether an operation should be part of a distributed transaction. If so, it sends the necessary information to the database server. </p> <p>The transaction coordinator implements XA API. In practice the coordinator is a library that is loaded into the same process as the application issuing the transaction. It keeps track of the participants in a transaction, collects participants responses after prepare request (via a callback into the driver), and use a log on local disk to keep track of commit/abort decision for each transaction </p>"},{"location":"Chapter%209/#holding-locks-while-in-doubt","title":"Holding locks while in doubt","text":"<p>Why care about transaction being stuck in doubt? can't the rest of the system get on with its work and ignore the in-doubt transactions that will be cleaned up eventually?</p> <p>The problem is with locking. Database transactions usually take a row-level exclusive lock on any rows they modify to prevent dirty writes. In addition, with serializable isolation, database use 2PL with all rows read by the transaction </p> <p>The database cannot release those locks until the transaction commits or aborts. Therefore if coordinator crashed and it takes 20 mins for it to start up again. Those locks must held for 20 mins. If the coordinator's log is lost for some reason, those locks will be held forever. Or at least the situation is manually resolved by a human</p> <p>While those locks are hold, no other writes can happen to these rows. Depending on the databases, other transaction might not even read those row (2PL). Thus other transactions cannot simply continue with their business. </p>"},{"location":"Chapter%209/#fault-tolerant-consensus","title":"Fault-Tolerant Consensus","text":"<p>The consensus problem is formalized as follows: one or more nodes may propose values, and the consensus algorithm decides on one of those values. When people concurrently book the last sear on an airplane, each node handling a customer request may propose the ID of the customer and decision indicates which one of those customer got the seat.</p> <p>Formally, consensus algorithm must satisfy the following properties: Uniform agreement     No two nodes decide differently Integrity     Node node decides twice Validity     If a node decides values v, then v was proposed by some node Termination     Every node that does not crash eventually decides some value</p> <p>The uniform and integrity properties define the core idea of consensus: you cannot change your mind once you decided. </p> <p>the termination property formalizes the idea of fault tolerance. It essentially says that a consensus algorithm cannot sit around and do nothing forever. it must make progress. (Termination is a liveness property)</p> <p>This system model assumes when a node crashes it will never come back. Imagine earthquake on the datacenter and node is destroyed by a landslide. In this system model, any algorithm that has to wait for a node to recover does not satisfy the termination property. In particular, 2PC does not meet this requirement</p> <p>Termination property is subject to the assumption that fewer than half of the nodes are crashed or unreachable. If more than half has failed, the system is not going to make decisions</p>"},{"location":"Chapter%209/#consensus-algorithms-and-total-order-broadcast","title":"Consensus algorithms and total order broadcast","text":"<p>The best known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft, and Zab. This book won't go into details of the different algorithms. Unless we are implementing a consensus system ourselves (which is hard)</p> <p>High level idea is:</p> <p>Viewstamped Replication, Raft, and Zab implement total order broadcast directly, because that is more efficient than doing repeated rounds of one-value-at-a-time consensus. In the case of Paxos, this optimization is known as Multi-Paxos.</p>"},{"location":"Chapter%209/#epoch-numbering-and-quorums","title":"Epoch numbering and quorums","text":"<p>All of the consensus protocols discussed so far internally use a leader in some form or another, but they don't guarantee that leader is unique. They make a weaker guarantee: the protocols define an epoch number and guarantee that within each epoch, the leader is unique</p> <p>When current leader is thought to be dead, vote is started to elect a new leader. New election increase the epoch number. If there is a conflict between two different leaders, then leader with the higher epoch number prevails.</p> <p>Before leader allows to decide anything, it needs to check there is not leader with higher epoch number exist. In order to do this, it must collect votes from a quorum of nodes. For every decision, leader must send the proposed value to other nodes and wait for a quorum of nodes to respond in favor of the proposal. </p> <p>Thus we have 2 rounds of voting: 1 for choose leader, 1 for vote on leader's proposal. The key insight is that those two vote's quorum must overlap. If the vote on proposal does not reveal any higher epoch, the current leader can conclude that no leader with higher epoch number exist. </p> <p>This voting process is similar two phase commit. The major difference is 2PC's coordinator is not elected. Consensus algorithm can recover from a leader failure by election while 2PC have to wait until coordinator recovers</p>"},{"location":"Chapter%209/#limitations-of-consensus","title":"Limitations of consensus","text":"<p>Although consensus algorithm are huge breakthrough, they are not used everywhere. Because benefits come at a cost.</p> <p>The process by which nodes vote on proposals is synchronous replication. And database often configured to use asynchronous replication and choose to accept this risk of lost data for the sake of better performance</p> <p>Consensus systems always require a strict majority to operate. This mean you need minimum of 3 in order to tolerate 1 failure, 5 in order to tolerate 2 failure. </p> <p>Consensus systems generally rely on timeouts to detect failed nodes. In environments with highly variable network delays (geographically distributed systems) it can cause frequent leader election (node falsely believes the leader to have failed). And this has performance penalty </p>"},{"location":"Chapter%209/#membership-and-coordination","title":"Membership and Coordination","text":"<p>ZooKeeper or etcd are often described as \"distributed key-value stores\" or \"coordination and configuration services\". The API of such a service looks like a database: read and write the value of a key, and iterate over keys. Why all the effort of implementing a consensus algorithm? What makes them different from the rest of the databases?</p> <p>To understand this, it's helpful to explore how ZooKeeper or etcd is used. Application developer rarely need to use ZooKeeper or etcd directly. More likely you will end up relying on it indirectly via other project such as HBase, Hadoop YARN, OpenStack Nova, and Kafka. What are these project use ZooKeeper for? </p> <p>ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory (although still write to disk for durability). So it's not suited to store all application data. That small amount of data is replicated across all of the nodes using a fault-tolerant total order broadcast algorithm. And total order broadcast is the way database replicate their nodes (reliable message delivery and total order delivery property must be met)</p> <p>ZooKeeper is modeled after Google's Chubby lock service 98, implementing not only total order broadcast but also set of interesting features that is useful for distributed systems: Linearizable atomic operation      Using an atomic compare-and-set operation, you can implement a lock. The consensus protocol guarantees that the operation will be atomic and linearizable, even if node fails or network is interrupted at any point. A distributed lock is usually implemented as a lease (lock with timeout) Total ordering of operations     ZooKeeper and etcd also has fencing token to prevent conflicting clients to write corrupted data. Fencing token is monotonically increased every time a lease is acquired. (ZooKeeper using transaction id <code>zxid</code> to achieve this) Failure detection     Clients maintain a long-lived session on ZooKeeper servers, and the client and server periodically exchange heartbeats to check their health. If the connection is interrupted, the session remains active until the heartbeat cease for a duration. Any locks held by a session can be released automatically  Change notifications     Not only can client read locks and values created by other clients, but it can watch them for changes. A client can find out when another client joins the cluster, or if another client fails. By subscribing to notifications, client avoids to frequently poll and find changes </p> <p>The combination of these features makes systems like ZooKeeper useful for distributed coordination. </p> <p>Different use cases </p>"},{"location":"Chapter%209/#service-discovery","title":"Service discovery","text":"<p>ZooKeeper, etcd are often used for service discovery, that is, to find out which IP address you need to connect to in order to reach a particular service. In cloud environments, where it is common for VM come and go. you often don't know the IP addresses of you services. Instead, you can configure your services such that when they start up they register their network endpoints in a service registry. </p> <p>It's not clear whether service discovery need consensus. DNS existed for a long time. And read from DNS is absolutely not linearizable. It is not a problem for DNS read to be little stale. </p> <p>Although service discovery doesn't require consensus, leader election does. thus, if your consensus system already knows who the leader is, then it make sense to also use this information to help other services discover who the leader is.  Basically a side benefit from consensus system</p>"},{"location":"Chapter%209/#membership-service","title":"Membership service","text":"<p>ZooKeeper and others can be seen as part of long history of research into membership services, which is important for building highly reliable systems, e.g. air traffic control. 110</p> <p>A membership service determines which nodes are currently active and live members of a cluster. If you couple failure detection with consensus, nodes can come to an agreement about which nodes should be considered alive or not. </p> <p>It could still happen where node is actually alive but declared dead. This is nevertheless very useful for system to have agreement on which member is active. </p>"},{"location":"Chapter%209/#summary","title":"Summary","text":"<p>This chapter examined topics of consistency and consensus from several different angles. Consistency model linearizability are looked in depth. Its goal is to make replicated data appear as though there is only one copy. And all operation on it is atomic. Just like a single threaded program. But it has the cost of performance</p> <p>Causality is also discussed. It imposes an ordering of events on a system. Unlike linearizability (all operations are put in a single timeline), causality provides a weaker consistency model: some things can be concurrent, a good example would be version history on git where timeline branch and merge. </p> <p>Even if we capture causal ordering, we saw that some things cannot be implemented this way: If node is accepting a unique username, it needs to know concurrently whether there is other node have this username or not. This problem leads to consensus</p> <p>In order to achieve consensus, all nodes need to agree on what was decided, and this decision is irrevocable. With some digging, a range of problem turns out to be equivalent to each other. Such equivalent problem include: Linearizable compare-and-set registers</p> <p>Atomic transaction commit</p> <p>Total order broadcast</p> <p>Locks and leases</p> <p>Membership/coordination service</p> <p>Uniqueness constraint</p> <p>All of these are straight forward if you only have 1 node, or you willing to assign the decision making capability to single node. This is what happened to single-leader database: such databases are able to provide linearizable operations, uniqueness constraints, a totally ordered replication log </p> <p>If single leader failed, or network interruption makes the leader unreachable, such system becomes unable to make any progress. There are 3 ways of handling this problem 1. wait for leader to recover (XA transaction coordinator choose this option) 2. Manually fail over by getting human involved 3. Use an algorithm to automatically choose a new leader. Which requires a consensus algorithm</p> <p>Although single leader doesn't require consensus on write, but it need consensus to maintain leader and leadership changes. </p> <p>Tools like ZooKeeper or etcd provide an \"outsourced\" consensus, failure detection, and membership service that application can use. </p> <p>Not every system need consensus: leaderless and multi-leader replication systems typically do not need global consensus. And maybe that's ok. human can cope with data that has branching and merging version histories.</p> <p>This chapter referenced a large body of research on the theory of distributed systems. Such research are valuable for reason about what can and cannot be done. It is worth exploring the reference</p> <p>This is end of part 2, where replication Chapter 5, partitioning Chapter 6, transactions Chapter 7, distributed system failure models Chapter 8 and consistency and consensus Chapter 9. This will lay a firm foundation of theory. In part 3 will turn to more practical systems and discuss how to build powerful applications from heterogeneous building blocks.</p>"},{"location":"FAQ/","title":"FAQ","text":""},{"location":"FAQ/#questions","title":"Questions","text":""},{"location":"FAQ/#what-is-fanout","title":"What is fanout?","text":"<p>fanout is mentioned in Chapter 1, borrowed from Electrical engineering concept. In data system basically mean how many additional outgoing request you need to send in order to fulfill the incoming request</p>"},{"location":"FAQ/#sla","title":"SLA","text":"<p>Service level agreement. Vendor should promise their service is above this metric. Otherwise you could ask for your money back example: Amazon prime delivery SLA is 2 days</p>"},{"location":"FAQ/#why-does-variable-length-encoding-revert-the-order","title":"Why does variable length encoding revert the order?","text":""},{"location":"Part%202/","title":"Part 2","text":"<p>For a successful technology, reality must take precedence over public relations, for nature cannot be fooled.</p> <p>\u2014Richard Feynman</p> <p>\u8fd9\u53e5\u8bdd\u5230\u5e95\u662f\u4ec0\u4e48\u610f\u601d\uff1f </p> <p>\u8fd9\u672c\u4e66\u7684\u7b2c\u4e00\u90e8\u5206\u8ba8\u8bba\u7684\u90fd\u662f\u4e00\u53f0\u673a\u5668\u7684\u60c5\u51b5\uff0c\u73b0\u5728\u7b2c\u4e8c\u90e8\u5206\u8981\u66f4\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e00\u4e2a\u95ee\u9898\uff1a</p> <p>\u5982\u679c\u591a\u53f0\u673a\u5668\u90fd\u8981\u8fdb\u884c\u6570\u636e\u5b58\u50a8+retrieval, \u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f</p> <p>\u6211\u4eec\u6709\u5f88\u591a\u7406\u7531\u628a\u4e00\u4e2a\u6570\u636e\u5e93\u5b58\u5728\u591a\u53f0\u673a\u5668\u4e0a\u9762\uff1a Scalability  \u5f53\u4e00\u53f0\u673a\u5668\u652f\u6301\u4e0d\u4e86\u76ee\u524d\u7684\u6570\u636e\u91cf\u7684\u65f6\u5019\uff0c\u636e\u9700\u8981\u591a\u53f0\u673a\u5668\u6765\u5206\u62c5\u6570\u636e\u91cf Fault tolerance/high availability \u8fd9\u4e2a\u5e94\u8be5\u662f\u7edd\u5927\u591a\u6570\u7684\u76ee\u7684\uff0c\u6bd4\u5982\u4f60\u4e00\u53f0\u673a\u5668\u574f\u4e86\uff0c\u5176\u4ed6\u673a\u5668\u53ef\u4ee5\u66ff\u4ed6\u63a5\u53d7\u8bf7\u6c42 Latency \u901a\u8fc7\u4e00\u4e2a\u7269\u7406\u4e0a\u66f4\u8fd1\u7684\u8ddd\u79bb\u6765\u964d\u4f4e latency (\u5317\u4eac\u5230\u7ebd\u7ea6\u5149\u90fd\u9700\u8981\u8d70 \\(10,988.88 km \\div 299\u00a0792\u00a0.458 km / s\\) = 36.65496 ms)</p>"},{"location":"Part%202/#scaling-to-higher-load","title":"Scaling to Higher Load","text":"<p>\u5982\u679c\u53ea\u662f\u9700\u8981 scale to higher load. \u90a3\u4e48\u6211\u4eec\u4e70\u66f4\u597d\u7684\u673a\u5668\u5c31\u53ef\u4ee5\u4e86\uff0c\u4e5f\u53eb\u505a scaling up/vertical scaling</p> <p>\u66f4\u591a\u7684CPU\uff0c\u66f4\u5927\u7684 RAM\uff0c\u66f4\u591a\u4e2a\u786c\u76d8\u3002\u8fd9\u4e9b\u786c\u4ef6\u8d44\u6e90\u901a\u5e38\u90fd\u5728\u4e00\u4e2a OS \u4e0b\u9762\u7ba1\u7406\uff0c\u6240\u4ee5\u4e5f\u8fd8\u7b97\u505a single machine. \u4e5f\u53eb shared memory architecture  \u4f46\u8fd9\u79cd\u65b9\u5f0f\u7684\u7f3a\u70b9\u5728\u4e8e cost \u53ef\u4ee5\u4e0a\u957f\u5f97\u5f88\u5feb\uff08\u51e0\u4f55\u7ea7\uff1f\u53cd\u6b63\u4e0d\u662f\u7ebf\u6027\u589e\u52a0\u7684\uff09</p> <p>\u6211\u8bb0\u5f97 oracle \u5c31\u6709\u81ea\u5df1\u7684 hardware\uff0c\u800c\u4e14\u8c8c\u4f3c\u53ca\u5176\u8d35\u2026\u2026</p> <p>\u8fd8\u6709\u4e00\u79cd\u65b9\u5f0f\u662f share-disk architecture, \u5c31\u662f\u591a\u4e2a\u673a\u5668\u901a\u8fc7\u7f51\u7edc\u5171\u4eab array of disk\u3002 \u8fd9\u4e2ascaling\u7684\u65b9\u5f0f\u4e5f\u6709limit (contention and locking)</p>"},{"location":"Part%202/#shared-nothing-architectures","title":"Shared nothing architectures","text":"<p>Shared nothing architectures (\u4e5f\u53eb horizontal scaling or scaling out) \u662f\u66f4\u6d41\u884c\u7684\u65b9\u5f0f \u5728\u8fd9\u4e2a scaling method\u4e2d\uff0c\u6bcf\u4e00\u53f0\u673a\u5668\u6216\u8005\u662f virtual machine is called node\u3002\u6bcf\u4e00\u4e2anode \u90fd\u662f\u72ec\u7acb\u7684\uff0c\u90fd\u9700\u8981\u901a\u8fc7 network \u6765\u5b8c\u6210\u534f\u4f5c</p> <p>shared nothing system \u5e76\u4e0d\u9700\u8981\u7279\u6b8a\u7684\u786c\u4ef6\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5\u4e70\u4efb\u4f55\u9002\u7528\u7684\u673a\u5668 (\u51e0\u53f0 resberry pi \u5e94\u8be5\u4e5f\u662f\u53ef\u4ee5\u7684) \u4f60\u8fd8\u53ef\u4ee5\u628a\u6570\u636e\u5206\u5e03\u5230\u4e0d\u540c\u7684\u5730\u57df\uff0creduce latency for users </p> <p>\u6709\u4e86\u73b0\u5728\u7684\u4e91\u670d\u52a1\uff0c\u5c0f\u516c\u53f8\u4e00\u6837\u53ef\u4ee5\u5efa\u4e00\u4e2a multi-region \u7684\u5206\u5e03\u5f0f\u7cfb\u7edf</p> <p>\u8fd9\u672c\u4e66\u4e3b\u8981\u96c6\u4e2d\u8bb2 share nothing architectures, \u5e76\u4e0d\u662f\u56e0\u4e3a\u4ed6\u662f\u6700\u597d\u7684\u9009\u62e9\u800c\u662f\u56e0\u4e3a\u4ed6\u9700\u8981\u6700\u591a\u7684 care\uff0c\u9700\u8981\u5927\u91cf\u7684engineering effort</p> <p>\u5c3d\u7ba1 shared nothing \u6709\u5f88\u591a\u597d\u5904\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u6781\u5927\u7684\u590d\u6742\u5ea6\uff0c\u4e0b\u9762\u51e0\u7ae0\u4e3b\u8981\u8ba8\u8bba shared nothing architecture \u7684\u95ee\u9898</p>"},{"location":"Part%202/#replication-vs-partitioning","title":"Replication vs Partitioning","text":"<p>\u4e3b\u8981\u67092\u79cd\u628a\u6570\u636e\u5206\u5230\u591a\u4e2a\u673a\u5668\u4e0a\u9762 Replication \u628a\u540c\u6837\u7684\u6570\u636e\u590d\u5236\u5230\u591a\u4e2a\u673a\u5668\u4e0a chapter 5 \u8bb2\u8fd9\u4e00\u4e2a\u65b9\u6cd5 Partitioning \u628a\u4e00\u4e2a\u5927\u7684\u6570\u636e\u5e93\u5207\u5206\u6210\u591a\u4e2a subsets (\u4e5f\u53eb partitions/sharding) chapter 6 talks about partitions \u4e0d\u8fc7\u8fd9\u4e24\u4e2a\u4e00\u822c\u662f\u4e00\u8d77\u7528\u7684 </p> <p>chapter 7 \u8ba8\u8bba transactions (\u4e5f\u662f shared-nothing system \u4f1a\u9047\u5230\u7684\u5404\u79cd\u95ee\u9898\uff0c\u4ee5\u53ca\u89e3\u51b3\u65b9\u6848) \u6700\u540e chapter 8 and chapter 9 \u8bb2 distributed systems \u7684\u6839\u672c\u9650\u5236</p>"},{"location":"Part%203/","title":"Part 3","text":"<p>Part 1 and part 2 discuss about all background knowledge about distributed database system. Based on the assumption about system only use one particular type of database</p> <p>In practice, data systems often made up from different kinds of databases. In this last part of the book, we will discuss about integrate different data systems into one application architecture. In reality, integrating disparate systems is one of the most important things that needs to be done in nontrivial application </p> <p>Data system can derive into 2 categories System of records      System of record can be seen as source of truth. It holds authoritative version of your data. </p> <p>Derived data systems     Data in derived system is result of transforming or processing your data in some way</p> <p>Derived data can be seen as denormalized or redundant data. Enabling you to look at the data from different \"point of view\"</p> <p>Not all system make clear distinction between systems of record and derived data but it is good distinction to make. Because it clarifies data flow of your system. Being explicit about which part of the system have which input and which output and how they depend on each other </p> <p>Data systems (storage engines, query languages) are not inherently system of record or derived systems. Just like a flat head screw driver also works on cross nails. Database is just a tool, you define how it is used in your application. Either be a system of record or derived data</p> <p>Being clear about which data derived from other data is important for system architecture. This point will be continue throughout this part of the book</p> <p>Chapter 10 will review batch-oriented dataflow systems such as MapReduce and good tools and principles for building large scale data systems Chapter 11 will took those ideas and apply them in streaming, which allow us to do same kinds of things with lower delays  Chapter 12 concludes the book by exploring those ideas to build reliable, scalable, and maintainable application in the future</p>"}]}